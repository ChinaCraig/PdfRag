"""
æ¨¡å‹ç®¡ç†å™¨ - ç¡¬ä»¶è‡ªé€‚åº”ç‰ˆæœ¬
è´Ÿè´£å„ç§AIæ¨¡å‹çš„å»¶è¿ŸåŠ è½½ã€ç¼“å­˜ç®¡ç†å’Œèµ„æºä¼˜åŒ–
æ”¯æŒæ ¹æ®ç¡¬ä»¶é…ç½®åŠ¨æ€è°ƒæ•´æ¨¡å‹åŠ è½½ç­–ç•¥
"""

import os
import logging
import threading
import time
import gc
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor

# ç¬¬ä¸‰æ–¹åº“
from sentence_transformers import SentenceTransformer
from paddleocr import PaddleOCR

# é¡¹ç›®å†…éƒ¨æ¨¡å—
from utils.config_loader import config_loader
from utils.easyocr_manager import EasyOCRManager

logger = logging.getLogger(__name__)

class ModelManager:
    """
    æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨
    
    ç‰¹æ€§:
    - å»¶è¿ŸåŠ è½½ï¼šæ¨¡å‹ä»…åœ¨éœ€è¦æ—¶åŠ è½½
    - LRUç¼“å­˜ï¼šè‡ªåŠ¨ç®¡ç†æ¨¡å‹å†…å­˜ä½¿ç”¨
    - ç¡¬ä»¶è‡ªé€‚åº”ï¼šæ ¹æ®ç³»ç»Ÿæ€§èƒ½è°ƒæ•´åŠ è½½ç­–ç•¥
    - å¹¶å‘å®‰å…¨ï¼šæ”¯æŒå¤šçº¿ç¨‹ç¯å¢ƒ
    - æ‰¹é‡å¤„ç†ï¼šä¼˜åŒ–å‘é‡ç”Ÿæˆæ•ˆç‡
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨"""
        self.models = {}  # æ¨¡å‹ç¼“å­˜
        self.model_locks = {}  # æ¨¡å‹åŠ è½½é”
        self.model_usage = {}  # æ¨¡å‹ä½¿ç”¨æ¬¡æ•°
        self.last_access_time = {}  # æœ€åè®¿é—®æ—¶é—´
        self.loading_flags = {}  # åŠ è½½çŠ¶æ€æ ‡è®°
        
        # ç¼“å­˜é…ç½®
        self.max_models_in_memory = 2
        self.model_ttl = 1800  # 30åˆ†é’ŸTTL
        self.cleanup_thread = None
        
        # ç¡¬ä»¶è‡ªé€‚åº”è®¾ç½®
        self.adaptive_settings = {
            "enable_gpu": False,
            "enable_model_cache": True,
            "batch_size": 4,
            "conservative_mode": False,
            "preload_models": False,
            "device": "cpu"
        }
        
        # OCRå¼•æ“ç®¡ç†
        self.ocr_engines = {}
        self.current_ocr_engine = None
        self.default_ocr_engine = "easyocr"  # é»˜è®¤ä½¿ç”¨EasyOCR
        
        # å¯åŠ¨æ¸…ç†çº¿ç¨‹
        self._start_cleanup_thread()
        
        # åˆå§‹åŒ–OCRå¼•æ“
        self._init_ocr_engines()
        
        logger.info("æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ - EasyOCRé›†æˆç‰ˆæœ¬")
    
    def apply_hardware_config(self, hardware_config: Dict[str, Any]) -> None:
        """
        åº”ç”¨ç¡¬ä»¶é…ç½®åˆ°æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            hardware_config: ç¡¬ä»¶æ¨èé…ç½®
        """
        try:
            old_gpu_setting = self.adaptive_settings.get("enable_gpu", False)
            
            # æ›´æ–°è‡ªé€‚åº”è®¾ç½®
            self.adaptive_settings.update({
                "enable_gpu": hardware_config.get("gpu_acceleration", False),
                "batch_size": hardware_config.get("batch_size", 4),
                "enable_model_cache": hardware_config.get("model_cache_enabled", True),
                "conservative_mode": hardware_config.get("processing_mode") == "conservative",
                "preload_models": hardware_config.get("processing_mode") == "aggressive"
            })
            
            # è®¾ç½®è®¾å¤‡
            if self.adaptive_settings["enable_gpu"]:
                try:
                    import torch
                    if torch.cuda.is_available():
                        self.adaptive_settings["device"] = "cuda"
                        logger.info("âœ… GPUåŠ é€Ÿå·²å¯ç”¨")
                    else:
                        self.adaptive_settings["device"] = "cpu"
                        logger.warning("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
                except ImportError:
                    self.adaptive_settings["device"] = "cpu"
                    logger.warning("âš ï¸ PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨CPUæ¨¡å¼")
            else:
                self.adaptive_settings["device"] = "cpu"
            
            # è°ƒæ•´ç¼“å­˜è®¾ç½®
            processing_mode = hardware_config.get("processing_mode", "balanced")
            if processing_mode == "conservative":
                self.max_models_in_memory = 1
                self.model_ttl = 600  # 10åˆ†é’Ÿ
            elif processing_mode == "aggressive":
                self.max_models_in_memory = 3
                self.model_ttl = 3600  # 1å°æ—¶
            else:  # balanced
                self.max_models_in_memory = 2
                self.model_ttl = 1800  # 30åˆ†é’Ÿ
            
            # å¦‚æœGPUè®¾ç½®æ”¹å˜ï¼Œæ¸…ç©ºæ¨¡å‹ç¼“å­˜
            if old_gpu_setting != self.adaptive_settings["enable_gpu"]:
                logger.info("GPUè®¾ç½®å·²æ”¹å˜ï¼Œæ¸…ç©ºæ¨¡å‹ç¼“å­˜")
                self._clear_model_cache()
            
            # å°†ç¡¬ä»¶é…ç½®åº”ç”¨åˆ°OCRå¼•æ“
            for engine in self.ocr_engines.values():
                if hasattr(engine, 'apply_hardware_config'):
                    engine.apply_hardware_config(hardware_config)
            
            logger.info(f"ç¡¬ä»¶è‡ªé€‚åº”é…ç½®å·²åº”ç”¨: {self.adaptive_settings}")
            
        except Exception as e:
            logger.error(f"åº”ç”¨ç¡¬ä»¶é…ç½®å¤±è´¥: {e}")
    
    def _start_cleanup_thread(self) -> None:
        """å¯åŠ¨æ¨¡å‹æ¸…ç†çº¿ç¨‹"""
        if self.cleanup_thread and self.cleanup_thread.is_alive():
            return
        
        self.cleanup_thread = threading.Thread(
            target=self._cleanup_unused_models,
            name="ModelCleanup",
            daemon=True
        )
        self.cleanup_thread.start()
        logger.debug("æ¨¡å‹æ¸…ç†çº¿ç¨‹å·²å¯åŠ¨")
    
    def _cleanup_unused_models(self) -> None:
        """å®šæœŸæ¸…ç†æœªä½¿ç”¨çš„æ¨¡å‹"""
        while True:
            try:
                time.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                current_time = time.time()
                models_to_remove = []
                
                for model_type, last_access in self.last_access_time.items():
                    if current_time - last_access > self.model_ttl:
                        models_to_remove.append(model_type)
                
                for model_type in models_to_remove:
                    self._unload_model(model_type)
                    logger.info(f"è‡ªåŠ¨æ¸…ç†æœªä½¿ç”¨æ¨¡å‹: {model_type}")
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                if models_to_remove:
                    gc.collect()
                    
            except Exception as e:
                logger.error(f"æ¨¡å‹æ¸…ç†å¼‚å¸¸: {e}")
                time.sleep(60)  # å¼‚å¸¸æ—¶ç­‰å¾…1åˆ†é’Ÿå†ç»§ç»­
    
    def _unload_model(self, model_type: str) -> None:
        """å¸è½½æŒ‡å®šæ¨¡å‹"""
        try:
            if model_type in self.models:
                del self.models[model_type]
            if model_type in self.model_usage:
                del self.model_usage[model_type]
            if model_type in self.last_access_time:
                del self.last_access_time[model_type]
            logger.debug(f"æ¨¡å‹å·²å¸è½½: {model_type}")
        except Exception as e:
            logger.error(f"å¸è½½æ¨¡å‹å¤±è´¥: {model_type}, {e}")
    
    def _clear_model_cache(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰æ¨¡å‹ç¼“å­˜"""
        try:
            model_types = list(self.models.keys())
            for model_type in model_types:
                self._unload_model(model_type)
            gc.collect()
            logger.info("æ‰€æœ‰æ¨¡å‹ç¼“å­˜å·²æ¸…ç©º")
        except Exception as e:
            logger.error(f"æ¸…ç©ºæ¨¡å‹ç¼“å­˜å¤±è´¥: {e}")
    
    def _get_model_lock(self, model_type: str) -> threading.Lock:
        """è·å–æ¨¡å‹åŠ è½½é”"""
        if model_type not in self.model_locks:
            self.model_locks[model_type] = threading.Lock()
        return self.model_locks[model_type]
    
    def load_embedding_model(self, force_reload: bool = False) -> SentenceTransformer:
        """
        å»¶è¿ŸåŠ è½½åµŒå…¥æ¨¡å‹
        
        Args:
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½
            
        Returns:
            åµŒå…¥æ¨¡å‹å®ä¾‹
        """
        model_type = "embedding"
        
        # æ£€æŸ¥ç¼“å­˜
        if not force_reload and model_type in self.models:
            self._update_model_access(model_type)
            logger.debug(f"ä»ç¼“å­˜åŠ è½½åµŒå…¥æ¨¡å‹")
            return self.models[model_type]
        
        # è·å–åŠ è½½é”
        with self._get_model_lock(model_type):
            # åŒé‡æ£€æŸ¥
            if not force_reload and model_type in self.models:
                self._update_model_access(model_type)
                return self.models[model_type]
            
            # é˜²æ­¢é‡å¤åŠ è½½
            if model_type in self.loading_flags:
                logger.warning(f"æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­: {model_type}")
                return None
            
            self.loading_flags[model_type] = True
            
            try:
                # è·å–æ¨¡å‹é…ç½®
                model_config = config_loader.get_nested_value("model.embedding", {})
                model_name = model_config.get("model_name", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
                model_path = model_config.get("model_path", "./models/embedding")
                dimensions = model_config.get("dimensions", 384)
                
                # è®¾å¤‡é…ç½®
                device = self.adaptive_settings["device"]
                
                logger.info(f"ğŸ”¤ğŸ”¤ğŸ”¤ å¼€å§‹åŠ è½½åµŒå…¥æ¨¡å‹")
                logger.info(f"ğŸ”¤ æ¨¡å‹åç§°: {model_name}")
                logger.info(f"ğŸ”¤ æ¨¡å‹è·¯å¾„: {model_path}")
                logger.info(f"ğŸ”¤ å‘é‡ç»´åº¦: {dimensions}")
                logger.info(f"ğŸ”¤ ä½¿ç”¨è®¾å¤‡: {device}")
                
                # ä¿å®ˆæ¨¡å¼ä¸‹çš„åŠ è½½ç­–ç•¥
                if self.adaptive_settings["conservative_mode"]:
                    logger.info(f"ğŸ”¤ ä¿å®ˆæ¨¡å¼: ä½¿ç”¨è½»é‡åŒ–åŠ è½½ç­–ç•¥")
                else:
                    logger.info(f"ğŸ”¤ æ³¨æ„ï¼š768ç»´æ¨¡å‹é¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦5-10åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
                
                import sys
                sys.stdout.flush()
                sys.stderr.flush()
                
                if os.path.exists(os.path.join(model_path, "pytorch_model.bin")):
                    # ä»æœ¬åœ°è·¯å¾„åŠ è½½
                    logger.info(f"ğŸ”¤ ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {model_path}")
                    model = SentenceTransformer(model_path, device=device)
                else:
                    # ä»Hugging Faceä¸‹è½½
                    logger.info(f"ğŸ”¤ ä»Hugging Faceä¸‹è½½æ¨¡å‹: {model_name}")
                    model = SentenceTransformer(model_name, device=device)
                    
                    # ä¿å­˜åˆ°æœ¬åœ°
                    os.makedirs(model_path, exist_ok=True)
                    model.save(model_path)
                    logger.info(f"ğŸ”¤ æ¨¡å‹å·²ä¿å­˜åˆ°æœ¬åœ°: {model_path}")
                
                # éªŒè¯æ¨¡å‹ç»´åº¦
                test_text = ["æµ‹è¯•æ–‡æœ¬"]
                test_embedding = model.encode(test_text, convert_to_tensor=False)
                actual_dim = len(test_embedding[0]) if len(test_embedding) > 0 else 0
                
                logger.info(f"ğŸ”¤ æ¨¡å‹åŠ è½½å®Œæˆï¼Œå®é™…ç»´åº¦: {actual_dim}")
                
                if actual_dim != dimensions:
                    logger.warning(f"âš ï¸ æ¨¡å‹ç»´åº¦ä¸åŒ¹é…ï¼é…ç½®ç»´åº¦: {dimensions}, å®é™…ç»´åº¦: {actual_dim}")
                
                # ç¼“å­˜æ¨¡å‹
                if self.adaptive_settings["enable_model_cache"]:
                    self._ensure_cache_space()
                    self.models[model_type] = model
                    self._update_model_access(model_type)
                    logger.info(f"âœ… åµŒå…¥æ¨¡å‹å·²ç¼“å­˜")
                
                logger.info(f"âœ…âœ…âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ - {model_name} ({actual_dim}ç»´)")
                return model
                
            except Exception as e:
                logger.error(f"âŒâŒâŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
                raise
            finally:
                # æ¸…é™¤åŠ è½½æ ‡å¿—
                if model_type in self.loading_flags:
                    del self.loading_flags[model_type]
    
    def load_ocr_model(self, force_reload: bool = False) -> PaddleOCR:
        """
        å»¶è¿ŸåŠ è½½OCRæ¨¡å‹
        
        Args:
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½
            
        Returns:
            OCRæ¨¡å‹å®ä¾‹
        """
        model_type = "ocr"
        
        # æ£€æŸ¥ç¼“å­˜
        if not force_reload and model_type in self.models:
            self._update_model_access(model_type)
            logger.debug(f"ä»ç¼“å­˜åŠ è½½OCRæ¨¡å‹")
            return self.models[model_type]
        
        # è·å–åŠ è½½é”
        with self._get_model_lock(model_type):
            # åŒé‡æ£€æŸ¥
            if not force_reload and model_type in self.models:
                self._update_model_access(model_type)
                return self.models[model_type]
            
            # é˜²æ­¢é‡å¤åŠ è½½
            if model_type in self.loading_flags:
                logger.warning(f"æ¨¡å‹æ­£åœ¨åŠ è½½ä¸­: {model_type}")
                return None
            
            self.loading_flags[model_type] = True
            
            try:
                # è·å–OCRé…ç½®
                ocr_config = config_loader.get_nested_value("model.ocr", {})
                det_model_dir = ocr_config.get("det_model_dir", "./models/ocr")
                rec_model_dir = ocr_config.get("rec_model_dir", "./models/ocr")
                cls_model_dir = ocr_config.get("cls_model_dir", "./models/ocr")
                
                # GPUè®¾ç½®
                use_gpu = self.adaptive_settings["enable_gpu"]
                
                logger.info(f"ğŸ”ğŸ”ğŸ” å¼€å§‹åŠ è½½OCRæ¨¡å‹")
                logger.info(f"ğŸ” æ£€æµ‹æ¨¡å‹: {det_model_dir}")
                logger.info(f"ğŸ” è¯†åˆ«æ¨¡å‹: {rec_model_dir}")
                logger.info(f"ğŸ” åˆ†ç±»æ¨¡å‹: {cls_model_dir}")
                logger.info(f"ğŸ” ä½¿ç”¨GPU: {use_gpu}")
                
                # ä¿å®ˆæ¨¡å¼è®¾ç½®
                if self.adaptive_settings["conservative_mode"]:
                    logger.info(f"ğŸ” ä¿å®ˆæ¨¡å¼: ä½¿ç”¨è½»é‡åŒ–OCRé…ç½®")
                    use_gpu = False  # ä¿å®ˆæ¨¡å¼å¼ºåˆ¶ä½¿ç”¨CPU
                
                # åˆ›å»ºOCRå®ä¾‹
                ocr = PaddleOCR(
                    use_angle_cls=True,
                    lang='ch',
                    use_gpu=use_gpu,
                    det_model_dir=det_model_dir if os.path.exists(det_model_dir) else None,
                    rec_model_dir=rec_model_dir if os.path.exists(rec_model_dir) else None,
                    cls_model_dir=cls_model_dir if os.path.exists(cls_model_dir) else None
                )
                
                # ç¼“å­˜æ¨¡å‹
                if self.adaptive_settings["enable_model_cache"]:
                    self._ensure_cache_space()
                    self.models[model_type] = ocr
                    self._update_model_access(model_type)
                    logger.info(f"âœ… OCRæ¨¡å‹å·²ç¼“å­˜")
                
                logger.info(f"âœ…âœ…âœ… OCRæ¨¡å‹åŠ è½½æˆåŠŸ")
                return ocr
                
            except Exception as e:
                logger.error(f"âŒâŒâŒ OCRæ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
                raise
            finally:
                # æ¸…é™¤åŠ è½½æ ‡å¿—
                if model_type in self.loading_flags:
                    del self.loading_flags[model_type]
    
    def _ensure_cache_space(self) -> None:
        """ç¡®ä¿ç¼“å­˜ç©ºé—´å……è¶³"""
        if len(self.models) >= self.max_models_in_memory:
            # æ‰¾åˆ°æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹
            oldest_model = min(
                self.last_access_time.items(),
                key=lambda x: x[1]
            )[0]
            self._unload_model(oldest_model)
            logger.debug(f"ç¼“å­˜ç©ºé—´ä¸è¶³ï¼Œå¸è½½æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹: {oldest_model}")
    
    def _update_model_access(self, model_type: str) -> None:
        """æ›´æ–°æ¨¡å‹è®¿é—®è®°å½•"""
        self.last_access_time[model_type] = time.time()
        self.model_usage[model_type] = self.model_usage.get(model_type, 0) + 1
    
    def get_embedding(self, texts: List[str]) -> List[List[float]]:
        """
        è·å–æ–‡æœ¬åµŒå…¥å‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not texts:
            return []
        
        try:
            batch_size = self.adaptive_settings["batch_size"]
            
            # æ‰¹é‡å¤„ç†
            if len(texts) > batch_size:
                return self._get_embedding_batched(texts, batch_size)
            
            # ç›´æ¥å¤„ç†
            model = self.load_embedding_model()
            if model is None:
                raise ValueError("åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥")
            
            embeddings = model.encode(texts, convert_to_tensor=False, show_progress_bar=False)
            return embeddings.tolist() if hasattr(embeddings, 'tolist') else embeddings
            
        except Exception as e:
            logger.error(f"ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}")
            raise
    
    def _get_embedding_batched(self, texts: List[str], batch_size: int) -> List[List[float]]:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡"""
        try:
            model = self.load_embedding_model()
            if model is None:
                raise ValueError("åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥")
            
            all_embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_embeddings = model.encode(
                    batch_texts,
                    convert_to_tensor=False,
                    show_progress_bar=False
                )
                
                if hasattr(batch_embeddings, 'tolist'):
                    all_embeddings.extend(batch_embeddings.tolist())
                else:
                    all_embeddings.extend(batch_embeddings)
                
                logger.debug(f"æ‰¹å¤„ç†è¿›åº¦: {min(i + batch_size, len(texts))}/{len(texts)}")
            
            return all_embeddings
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}")
            raise
    
    def extract_text_from_image(self, image_path: str) -> List[Dict[str, Any]]:
        """
        ä»å›¾åƒä¸­æå–æ–‡æœ¬ï¼ˆOCRï¼‰ - æ”¯æŒå¤šå¼•æ“
        
        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            
        Returns:
            OCRç»“æœåˆ—è¡¨
        """
        try:
            # ä½¿ç”¨å½“å‰OCRå¼•æ“
            if self.current_ocr_engine:
                return self.current_ocr_engine.extract_text_from_image(image_path)
            else:
                logger.warning("OCRå¼•æ“æœªåˆå§‹åŒ–ï¼Œå°è¯•åˆå§‹åŒ–é»˜è®¤å¼•æ“")
                self._init_ocr_engines()
                if self.current_ocr_engine:
                    return self.current_ocr_engine.extract_text_from_image(image_path)
                else:
                    logger.error("OCRå¼•æ“åˆå§‹åŒ–å¤±è´¥")
                    return []
                
        except Exception as e:
            logger.error(f"å½“å‰OCRå¼•æ“å¤„ç†å¤±è´¥: {e}")
            
            # è‡ªåŠ¨å›é€€åˆ°PaddleOCR
            if self.current_ocr_engine and not isinstance(self.current_ocr_engine, type(self)):
                logger.info("è‡ªåŠ¨å›é€€åˆ°PaddleOCRå¼•æ“")
                try:
                    return self._extract_with_paddleocr(image_path)
                except Exception as fallback_error:
                    logger.error(f"PaddleOCRå›é€€ä¹Ÿå¤±è´¥: {fallback_error}")
            
            return []
    
    def get_model_stats(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡"""
        return {
            "cached_models": list(self.models.keys()),
            "model_usage": self.model_usage.copy(),
            "cache_size": len(self.models),
            "max_cache_size": self.max_models_in_memory,
            "adaptive_settings": self.adaptive_settings.copy()
        }
    
    def preload_models(self, model_types: List[str] = None) -> None:
        """
        é¢„åŠ è½½æŒ‡å®šæ¨¡å‹
        
        Args:
            model_types: è¦é¢„åŠ è½½çš„æ¨¡å‹ç±»å‹åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºé¢„åŠ è½½æ‰€æœ‰
        """
        # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„é¢„åŠ è½½è®¾ç½®
        from utils.config_loader import config_loader
        app_config = config_loader.get_app_config()
        dev_config = app_config.get("development", {})
        config_preload_enabled = dev_config.get("preload_models", False)
        
        # æ£€æŸ¥è‡ªé€‚åº”è®¾ç½®ä¸­çš„é¢„åŠ è½½
        adaptive_preload_enabled = self.adaptive_settings.get("preload_models", False)
        
        # åªè¦ä»»ä¸€é…ç½®å¯ç”¨é¢„åŠ è½½å°±æ‰§è¡Œï¼ˆä¼˜å…ˆé…ç½®æ–‡ä»¶ï¼‰
        preload_enabled = config_preload_enabled or adaptive_preload_enabled
        
        if not preload_enabled:
            logger.debug("é¢„åŠ è½½å·²ç¦ç”¨ï¼Œè·³è¿‡æ¨¡å‹é¢„åŠ è½½")
            return
        
        if model_types is None:
            model_types = ["embedding", "ocr"]
        
        logger.info(f"ğŸš€ å¼€å§‹é¢„åŠ è½½æ¨¡å‹: {model_types}")
        logger.info(f"é¢„åŠ è½½åŸå› : é…ç½®æ–‡ä»¶={config_preload_enabled}, è‡ªé€‚åº”è®¾ç½®={adaptive_preload_enabled}")
        
        for model_type in model_types:
            try:
                logger.info(f"ğŸ“¥ æ­£åœ¨é¢„åŠ è½½ {model_type} æ¨¡å‹...")
                if model_type == "embedding":
                    self.load_embedding_model()
                    logger.info(f"âœ… {model_type} æ¨¡å‹é¢„åŠ è½½å®Œæˆ")
                elif model_type == "ocr":
                    self.load_ocr_model()
                    logger.info(f"âœ… {model_type} æ¨¡å‹é¢„åŠ è½½å®Œæˆ")
                else:
                    logger.warning(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
            except Exception as e:
                logger.error(f"âŒ é¢„åŠ è½½æ¨¡å‹å¤±è´¥ {model_type}: {e}")
        
        logger.info("ğŸ‰ æ‰€æœ‰æ¨¡å‹é¢„åŠ è½½å®Œæˆï¼")
    
    def _init_ocr_engines(self):
        """åˆå§‹åŒ–OCRå¼•æ“"""
        try:
            # è·å–é»˜è®¤å¼•æ“é…ç½®
            default_engine = config_loader.get_nested_value("model.ocr.default_engine", self.default_ocr_engine)
            
            logger.info(f"åˆå§‹åŒ–OCRå¼•æ“ï¼Œé»˜è®¤å¼•æ“: {default_engine}")
            self._switch_ocr_engine(default_engine)
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–OCRå¼•æ“å¤±è´¥: {e}")
            # å›é€€åˆ°EasyOCR
            try:
                self._switch_ocr_engine("easyocr")
            except Exception as fallback_error:
                logger.error(f"å›é€€åˆ°EasyOCRä¹Ÿå¤±è´¥: {fallback_error}")
    
    def _switch_ocr_engine(self, engine_type: str):
        """åˆ‡æ¢OCRå¼•æ“"""
        try:
            if engine_type not in self.ocr_engines:
                if engine_type == "easyocr":
                    self.ocr_engines[engine_type] = EasyOCRManager()
                    # åº”ç”¨ç¡¬ä»¶é…ç½®
                    self.ocr_engines[engine_type].apply_hardware_config(self.adaptive_settings)
                elif engine_type == "paddleocr":
                    # ä½¿ç”¨ç°æœ‰çš„selfä½œä¸ºPaddleOCRç®¡ç†å™¨
                    self.ocr_engines[engine_type] = self
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„OCRå¼•æ“: {engine_type}")
            
            self.current_ocr_engine = self.ocr_engines[engine_type]
            logger.info(f"OCRå¼•æ“å·²åˆ‡æ¢åˆ°: {engine_type}")
            
        except Exception as e:
            logger.error(f"åˆ‡æ¢OCRå¼•æ“å¤±è´¥: {e}")
            raise
    
    def _extract_with_paddleocr(self, image_path: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨PaddleOCRæå–æ–‡å­—ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        try:
            # åŠ è½½PaddleOCRæ¨¡å‹
            ocr = self.load_ocr_model()
            if ocr is None:
                logger.warning("PaddleOCRæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡å›¾åƒæ–‡å­—æå–")
                return []
            
            # æ‰§è¡ŒOCR
            results = ocr.ocr(image_path, cls=True)
            
            # è§£æç»“æœ
            ocr_results = []
            if results and results[0]:
                for line in results[0]:
                    if len(line) >= 2:
                        bbox = line[0]  # è¾¹ç•Œæ¡†
                        text_info = line[1]  # æ–‡å­—ä¿¡æ¯
                        if text_info and len(text_info) >= 2:
                            text = text_info[0]  # æ–‡å­—å†…å®¹
                            confidence = text_info[1]  # ç½®ä¿¡åº¦
                            
                            ocr_results.append({
                                "text": text,
                                "confidence": confidence,
                                "bbox": bbox
                            })
            
            logger.debug(f"PaddleOCRæå–å®Œæˆ: {len(ocr_results)}ä¸ªæ–‡æœ¬å—")
            return ocr_results
            
        except Exception as e:
            logger.error(f"PaddleOCRæ–‡å­—æå–å¤±è´¥: {e}")
            return []
    
    def get_ocr_engine_info(self) -> Dict[str, Any]:
        """è·å–OCRå¼•æ“ä¿¡æ¯"""
        current_engine_name = "unknown"
        if self.current_ocr_engine == self:
            current_engine_name = "paddleocr"
        elif self.current_ocr_engine:
            if hasattr(self.current_ocr_engine, 'get_model_stats'):
                stats = self.current_ocr_engine.get_model_stats()
                current_engine_name = stats.get('engine_type', 'unknown')
            else:
                current_engine_name = type(self.current_ocr_engine).__name__.lower()
        
        return {
            "current_engine": current_engine_name,
            "default_engine": self.default_ocr_engine,
            "available_engines": list(self.ocr_engines.keys()),
            "engine_stats": self.current_ocr_engine.get_model_stats() if hasattr(self.current_ocr_engine, 'get_model_stats') else {}
        }
    
    def switch_ocr_engine_runtime(self, engine_type: str) -> bool:
        """è¿è¡Œæ—¶åˆ‡æ¢OCRå¼•æ“"""
        try:
            logger.info(f"è¿è¡Œæ—¶åˆ‡æ¢OCRå¼•æ“åˆ°: {engine_type}")
            self._switch_ocr_engine(engine_type)
            return True
        except Exception as e:
            logger.error(f"è¿è¡Œæ—¶åˆ‡æ¢OCRå¼•æ“å¤±è´¥: {e}")
            return False
    
    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        try:
            logger.info("æ­£åœ¨æ¸…ç†æ¨¡å‹ç®¡ç†å™¨...")
            self._clear_model_cache()
            
            # æ¸…ç†OCRå¼•æ“
            for engine in self.ocr_engines.values():
                if hasattr(engine, 'cleanup') and engine != self:
                    engine.cleanup()
            
            # åœæ­¢æ¸…ç†çº¿ç¨‹ï¼ˆçº¿ç¨‹æ˜¯daemonï¼Œä¼šè‡ªåŠ¨ç»“æŸï¼‰
            
            logger.info("æ¨¡å‹ç®¡ç†å™¨æ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†æ¨¡å‹ç®¡ç†å™¨å¤±è´¥: {e}")

# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
model_manager = ModelManager() 