"""
ç¡¬ä»¶æ£€æµ‹å·¥å…·ç±»
æ£€æµ‹ç³»ç»Ÿç¡¬ä»¶èµ„æºï¼Œä¸ºæ¨¡å‹åŠ è½½å’Œå¤„ç†ç­–ç•¥æä¾›ä¾æ®
"""
import os
import platform
import psutil
import torch
import logging
from typing import Dict, Any, Tuple, Optional, List
import multiprocessing
import subprocess

logger = logging.getLogger(__name__)

class HardwareDetector:
    """ç¡¬ä»¶æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.hardware_info = {}
        self.performance_score = 0
        self.recommendations = []
    
    def detect_all(self) -> Dict[str, Any]:
        """
        æ£€æµ‹æ‰€æœ‰ç¡¬ä»¶ä¿¡æ¯
        
        Returns:
            ç¡¬ä»¶ä¿¡æ¯å­—å…¸
        """
        logger.info("å¼€å§‹ç¡¬ä»¶æ£€æµ‹...")
        
        self.hardware_info = {
            "system": self._detect_system_info(),
            "cpu": self._detect_cpu_info(),
            "memory": self._detect_memory_info(),
            "gpu": self._detect_gpu_info(),
            "storage": self._detect_storage_info(),
            "python": self._detect_python_env()
        }
        
        # è®¡ç®—æ€§èƒ½è¯„åˆ†
        self.performance_score = self._calculate_performance_score()
        self.hardware_info["performance_score"] = self.performance_score
        
        # ç”Ÿæˆå»ºè®®
        self.recommendations = self._generate_recommendations()
        self.hardware_info["recommendations"] = self.recommendations
        
        logger.info(f"ç¡¬ä»¶æ£€æµ‹å®Œæˆï¼Œæ€§èƒ½è¯„åˆ†: {self.performance_score}/100")
        return self.hardware_info
    
    def _detect_system_info(self) -> Dict[str, str]:
        """æ£€æµ‹ç³»ç»Ÿä¿¡æ¯"""
        try:
            return {
                "platform": platform.platform(),
                "system": platform.system(),
                "machine": platform.machine(),
                "processor": platform.processor(),
                "architecture": platform.architecture()[0],
                "python_version": platform.python_version()
            }
        except Exception as e:
            logger.warning(f"ç³»ç»Ÿä¿¡æ¯æ£€æµ‹å¤±è´¥: {e}")
            return {}
    
    def _detect_cpu_info(self) -> Dict[str, Any]:
        """æ£€æµ‹CPUä¿¡æ¯"""
        try:
            # åŸºç¡€CPUä¿¡æ¯
            physical_cores = psutil.cpu_count(logical=False) or psutil.cpu_count(logical=True) or 1
            logical_cores = psutil.cpu_count(logical=True) or 1
            
            cpu_info = {
                "physical_cores": physical_cores,
                "logical_cores": logical_cores,
                "usage_percent": psutil.cpu_percent(interval=1),
                "load_average": os.getloadavg() if hasattr(os, 'getloadavg') else None
            }
            
            # å°è¯•è·å–CPUé¢‘ç‡ä¿¡æ¯
            try:
                cpu_freq = psutil.cpu_freq()
                if cpu_freq:
                    cpu_info["max_frequency"] = cpu_freq.max
                    cpu_info["current_frequency"] = cpu_freq.current
            except Exception as e:
                logger.debug(f"è·å–CPUé¢‘ç‡å¤±è´¥: {e}")
                cpu_info["max_frequency"] = None
                cpu_info["current_frequency"] = None
            
            # å°è¯•è·å–æ›´è¯¦ç»†çš„CPUä¿¡æ¯
            try:
                if platform.system() == "Linux":
                    with open('/proc/cpuinfo', 'r') as f:
                        cpuinfo = f.read()
                        if 'model name' in cpuinfo:
                            for line in cpuinfo.split('\n'):
                                if 'model name' in line:
                                    cpu_info["model_name"] = line.split(':')[1].strip()
                                    break
                elif platform.system() == "Darwin":  # macOS
                    result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                          capture_output=True, text=True)
                    if result.returncode == 0:
                        cpu_info["model_name"] = result.stdout.strip()
            except Exception as e:
                logger.debug(f"è·å–è¯¦ç»†CPUä¿¡æ¯å¤±è´¥: {e}")
            
            return cpu_info
        except Exception as e:
            logger.warning(f"CPUä¿¡æ¯æ£€æµ‹å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼ï¼Œç¡®ä¿ç³»ç»Ÿèƒ½ç»§ç»­è¿è¡Œ
            return {
                "physical_cores": 1,
                "logical_cores": 1,
                "max_frequency": None,
                "current_frequency": None,
                "usage_percent": 0,
                "load_average": None
            }
    
    def _detect_memory_info(self) -> Dict[str, Any]:
        """æ£€æµ‹å†…å­˜ä¿¡æ¯"""
        try:
            memory = psutil.virtual_memory()
            swap = psutil.swap_memory()
            
            return {
                "total_gb": round(memory.total / (1024**3), 2),
                "available_gb": round(memory.available / (1024**3), 2),
                "used_gb": round(memory.used / (1024**3), 2),
                "usage_percent": memory.percent,
                "swap_total_gb": round(swap.total / (1024**3), 2),
                "swap_used_gb": round(swap.used / (1024**3), 2),
                "swap_percent": swap.percent
            }
        except Exception as e:
            logger.warning(f"å†…å­˜ä¿¡æ¯æ£€æµ‹å¤±è´¥: {e}")
            return {}
    
    def _detect_gpu_info(self) -> Dict[str, Any]:
        """æ£€æµ‹GPUä¿¡æ¯"""
        gpu_info = {
            "cuda_available": torch.cuda.is_available(),
            "gpu_count": 0,
            "gpus": []
        }
        
        try:
            if torch.cuda.is_available():
                gpu_info["gpu_count"] = torch.cuda.device_count()
                gpu_info["cuda_version"] = torch.version.cuda
                gpu_info["pytorch_version"] = torch.__version__
                
                for i in range(torch.cuda.device_count()):
                    gpu_properties = torch.cuda.get_device_properties(i)
                    memory_info = torch.cuda.mem_get_info(i)
                    
                    gpu_data = {
                        "id": i,
                        "name": gpu_properties.name,
                        "memory_total_gb": round(gpu_properties.total_memory / (1024**3), 2),
                        "memory_free_gb": round(memory_info[0] / (1024**3), 2),
                        "memory_used_gb": round((gpu_properties.total_memory - memory_info[0]) / (1024**3), 2),
                        "compute_capability": f"{gpu_properties.major}.{gpu_properties.minor}",
                        "multiprocessor_count": gpu_properties.multi_processor_count
                    }
                    gpu_info["gpus"].append(gpu_data)
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰GPUä½†æ²¡æœ‰CUDAæ”¯æŒ
                try:
                    if platform.system() == "Linux":
                        result = subprocess.run(['lspci', '|', 'grep', '-i', 'vga'], 
                                              shell=True, capture_output=True, text=True)
                        if result.returncode == 0 and result.stdout:
                            gpu_info["note"] = "æ£€æµ‹åˆ°GPUè®¾å¤‡ä½†CUDAä¸å¯ç”¨"
                except Exception:
                    pass
                    
        except Exception as e:
            logger.warning(f"GPUä¿¡æ¯æ£€æµ‹å¤±è´¥: {e}")
        
        return gpu_info
    
    def _detect_storage_info(self) -> Dict[str, Any]:
        """æ£€æµ‹å­˜å‚¨ä¿¡æ¯"""
        try:
            disk_usage = psutil.disk_usage('/')
            return {
                "total_gb": round(disk_usage.total / (1024**3), 2),
                "used_gb": round(disk_usage.used / (1024**3), 2),
                "free_gb": round(disk_usage.free / (1024**3), 2),
                "usage_percent": round((disk_usage.used / disk_usage.total) * 100, 1)
            }
        except Exception as e:
            logger.warning(f"å­˜å‚¨ä¿¡æ¯æ£€æµ‹å¤±è´¥: {e}")
            return {}
    
    def _detect_python_env(self) -> Dict[str, Any]:
        """æ£€æµ‹Pythonç¯å¢ƒä¿¡æ¯"""
        try:
            return {
                "version": platform.python_version(),
                "implementation": platform.python_implementation(),
                "compiler": platform.python_compiler(),
                "multiprocessing_method": multiprocessing.get_start_method(),
                "max_workers": multiprocessing.cpu_count()
            }
        except Exception as e:
            logger.warning(f"Pythonç¯å¢ƒæ£€æµ‹å¤±è´¥: {e}")
            return {}
    
    def _calculate_performance_score(self) -> int:
        """
        è®¡ç®—ç³»ç»Ÿæ€§èƒ½è¯„åˆ† (0-100)
        è€ƒè™‘CPUã€å†…å­˜ã€GPUç­‰å› ç´ 
        """
        score = 0
        
        try:
            cpu_info = self.hardware_info.get("cpu", {})
            memory_info = self.hardware_info.get("memory", {})
            gpu_info = self.hardware_info.get("gpu", {})
            
            # CPUè¯„åˆ† (40%)
            cpu_score = 0
            logical_cores = cpu_info.get("logical_cores", 1)
            if logical_cores >= 16:
                cpu_score = 40
            elif logical_cores >= 8:
                cpu_score = 35
            elif logical_cores >= 4:
                cpu_score = 25
            else:
                cpu_score = 15
            
            # å†…å­˜è¯„åˆ† (30%)
            memory_score = 0
            total_memory = memory_info.get("total_gb", 0)
            if total_memory >= 32:
                memory_score = 30
            elif total_memory >= 16:
                memory_score = 25
            elif total_memory >= 8:
                memory_score = 20
            else:
                memory_score = 10
            
            # GPUè¯„åˆ† (30%)
            gpu_score = 0
            if gpu_info.get("cuda_available", False):
                gpu_count = gpu_info.get("gpu_count", 0)
                if gpu_count > 0:
                    gpus = gpu_info.get("gpus", [])
                    if gpus:
                        max_gpu_memory = max([gpu.get("memory_total_gb", 0) for gpu in gpus])
                        if max_gpu_memory >= 24:
                            gpu_score = 30
                        elif max_gpu_memory >= 12:
                            gpu_score = 25
                        elif max_gpu_memory >= 8:
                            gpu_score = 20
                        elif max_gpu_memory >= 4:
                            gpu_score = 15
                        else:
                            gpu_score = 10
            
            score = cpu_score + memory_score + gpu_score
            
            # è°ƒæ•´è¯„åˆ†ï¼Œç¡®ä¿åœ¨0-100èŒƒå›´å†…
            score = max(0, min(100, score))
            
        except Exception as e:
            logger.warning(f"æ€§èƒ½è¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            score = 50  # é»˜è®¤ä¸­ç­‰è¯„åˆ†
        
        return score
    
    def _generate_recommendations(self) -> List[str]:
        """æ ¹æ®ç¡¬ä»¶ä¿¡æ¯ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        try:
            cpu_info = self.hardware_info.get("cpu", {})
            memory_info = self.hardware_info.get("memory", {})
            gpu_info = self.hardware_info.get("gpu", {})
            
            # CPUå»ºè®®
            logical_cores = cpu_info.get("logical_cores", 1)
            if logical_cores < 4:
                recommendations.append("CPUæ ¸å¿ƒæ•°è¾ƒå°‘ï¼Œå»ºè®®å¯ç”¨è½»é‡åŒ–å¤„ç†æ¨¡å¼")
            elif logical_cores >= 16:
                recommendations.append("CPUæ€§èƒ½ä¼˜ç§€ï¼Œå¯ä»¥å¯ç”¨å¹¶è¡Œå¤„ç†ä¼˜åŒ–")
            
            # å†…å­˜å»ºè®®
            total_memory = memory_info.get("total_gb", 0)
            if total_memory < 8:
                recommendations.append("å†…å­˜å®¹é‡ä¸è¶³ï¼Œå»ºè®®å‡å°‘æ‰¹å¤„ç†å¤§å°å’Œæ¨¡å‹ç¼“å­˜")
            elif total_memory >= 32:
                recommendations.append("å†…å­˜å……è¶³ï¼Œå¯ä»¥å¯ç”¨å¤§æ‰¹é‡å¤„ç†å’Œæ¨¡å‹é¢„åŠ è½½")
            
            # GPUå»ºè®®
            if not gpu_info.get("cuda_available", False):
                recommendations.append("æœªæ£€æµ‹åˆ°CUDA GPUï¼Œå»ºè®®ä½¿ç”¨CPUä¼˜åŒ–æ¨¡å¼")
            else:
                gpu_count = gpu_info.get("gpu_count", 0)
                if gpu_count > 0:
                    gpus = gpu_info.get("gpus", [])
                    if gpus:
                        max_gpu_memory = max([gpu.get("memory_total_gb", 0) for gpu in gpus])
                        if max_gpu_memory >= 12:
                            recommendations.append("GPUæ˜¾å­˜å……è¶³ï¼Œå¯ä»¥å¯ç”¨GPUåŠ é€Ÿå’Œå¤§æ¨¡å‹")
                        elif max_gpu_memory >= 4:
                            recommendations.append("GPUæ˜¾å­˜é€‚ä¸­ï¼Œå»ºè®®ä½¿ç”¨ä¸­ç­‰å¤§å°çš„æ¨¡å‹")
                        else:
                            recommendations.append("GPUæ˜¾å­˜è¾ƒå°ï¼Œå»ºè®®ä½¿ç”¨è½»é‡åŒ–æ¨¡å‹")
            
            # æ€§èƒ½è¯„åˆ†å»ºè®®
            if self.performance_score < 30:
                recommendations.append("ç³»ç»Ÿæ€§èƒ½è¾ƒä½ï¼Œå¼ºçƒˆå»ºè®®å¯ç”¨è½»é‡åŒ–æ¨¡å¼")
            elif self.performance_score > 80:
                recommendations.append("ç³»ç»Ÿæ€§èƒ½ä¼˜ç§€ï¼Œå¯ä»¥å¯ç”¨æ‰€æœ‰é«˜çº§åŠŸèƒ½")
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆå»ºè®®å¤±è´¥: {e}")
            recommendations.append("ç¡¬ä»¶æ£€æµ‹å¼‚å¸¸ï¼Œå»ºè®®ä½¿ç”¨é»˜è®¤é…ç½®")
        
        return recommendations
    
    def get_recommended_config(self) -> Dict[str, Any]:
        """
        æ ¹æ®ç¡¬ä»¶ä¿¡æ¯ç”Ÿæˆæ¨èé…ç½®
        
        Returns:
            æ¨èçš„ç³»ç»Ÿé…ç½®
        """
        config = {
            "gpu_acceleration": False,
            "batch_size": 1,
            "max_workers": 1,
            "model_cache_enabled": True,
            "processing_mode": "conservative",  # conservative, balanced, aggressive
            "preload_models": False  # æ·»åŠ é¢„åŠ è½½æ¨¡å‹é…ç½®
        }
        
        try:
            cpu_info = self.hardware_info.get("cpu", {})
            memory_info = self.hardware_info.get("memory", {})
            gpu_info = self.hardware_info.get("gpu", {})
            
            logical_cores = cpu_info.get("logical_cores", 1)
            total_memory = memory_info.get("total_gb", 0)
            cuda_available = gpu_info.get("cuda_available", False)
            
            # æ ¹æ®æ€§èƒ½è¯„åˆ†è®¾ç½®å¤„ç†æ¨¡å¼
            if self.performance_score >= 80:
                config["processing_mode"] = "aggressive"
                config["batch_size"] = min(8, logical_cores)
                config["max_workers"] = min(4, logical_cores // 2)
            elif self.performance_score >= 50:
                config["processing_mode"] = "balanced"
                config["batch_size"] = min(4, logical_cores)
                config["max_workers"] = min(2, logical_cores // 4)
            else:
                config["processing_mode"] = "conservative"
                config["batch_size"] = 1
                config["max_workers"] = 1
            
            # GPUé…ç½®
            if cuda_available and gpu_info.get("gpu_count", 0) > 0:
                gpus = gpu_info.get("gpus", [])
                if gpus:
                    max_gpu_memory = max([gpu.get("memory_total_gb", 0) for gpu in gpus])
                    if max_gpu_memory >= 4:  # è‡³å°‘4GBæ˜¾å­˜æ‰å»ºè®®å¯ç”¨GPU
                        config["gpu_acceleration"] = True
            
            # æ ¹æ®æ€§èƒ½å’Œå†…å­˜æƒ…å†µå†³å®šæ˜¯å¦å»ºè®®é¢„åŠ è½½æ¨¡å‹
            if self.performance_score >= 60 and total_memory >= 8:
                config["preload_models"] = True
            
            logger.debug(f"ç¡¬ä»¶æ¨èé…ç½®: {config}")
            
            # å†…å­˜é…ç½®
            if total_memory < 8:
                config["model_cache_enabled"] = False
                config["batch_size"] = 1
            elif total_memory >= 16:
                config["model_cache_enabled"] = True
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆæ¨èé…ç½®å¤±è´¥: {e}")
        
        return config
    
    def check_requirements(self, min_requirements: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """
        æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€ä½ç¡¬ä»¶è¦æ±‚
        
        Args:
            min_requirements: æœ€ä½ç¡¬ä»¶è¦æ±‚
            
        Returns:
            (æ˜¯å¦æ»¡è¶³è¦æ±‚, ä¸æ»¡è¶³çš„é¡¹ç›®åˆ—è¡¨)
        """
        issues = []
        
        try:
            cpu_info = self.hardware_info.get("cpu", {})
            memory_info = self.hardware_info.get("memory", {})
            
            # æ£€æŸ¥CPU
            min_cores = min_requirements.get("min_cpu_cores", 2)
            logical_cores = cpu_info.get("logical_cores", 0)
            if logical_cores < min_cores:
                issues.append(f"CPUæ ¸å¿ƒæ•°ä¸è¶³: éœ€è¦è‡³å°‘{min_cores}æ ¸å¿ƒï¼Œå½“å‰{logical_cores}æ ¸å¿ƒ")
            
            # æ£€æŸ¥å†…å­˜
            min_memory = min_requirements.get("min_memory_gb", 4)
            total_memory = memory_info.get("total_gb", 0)
            if total_memory < min_memory:
                issues.append(f"å†…å­˜ä¸è¶³: éœ€è¦è‡³å°‘{min_memory}GBï¼Œå½“å‰{total_memory}GB")
            
            # æ£€æŸ¥å¯ç”¨å†…å­˜
            available_memory = memory_info.get("available_gb", 0)
            min_available = min_requirements.get("min_available_memory_gb", 2)
            if available_memory < min_available:
                issues.append(f"å¯ç”¨å†…å­˜ä¸è¶³: éœ€è¦è‡³å°‘{min_available}GBï¼Œå½“å‰{available_memory}GB")
            
        except Exception as e:
            logger.warning(f"ç¡¬ä»¶è¦æ±‚æ£€æŸ¥å¤±è´¥: {e}")
            issues.append("ç¡¬ä»¶æ£€æµ‹å¼‚å¸¸ï¼Œæ— æ³•éªŒè¯è¦æ±‚")
        
        return len(issues) == 0, issues
    
    def generate_report(self) -> str:
        """ç”Ÿæˆç¡¬ä»¶æ£€æµ‹æŠ¥å‘Š"""
        if not self.hardware_info:
            return "ç¡¬ä»¶ä¿¡æ¯æœªæ£€æµ‹"
        
        report_lines = [
            "=" * 60,
            "ç¡¬ä»¶æ£€æµ‹æŠ¥å‘Š",
            "=" * 60,
            ""
        ]
        
        # ç³»ç»Ÿä¿¡æ¯
        system_info = self.hardware_info.get("system", {})
        if system_info:
            report_lines.extend([
                "ğŸ“Ÿ ç³»ç»Ÿä¿¡æ¯:",
                f"  æ“ä½œç³»ç»Ÿ: {system_info.get('platform', 'Unknown')}",
                f"  æ¶æ„: {system_info.get('architecture', 'Unknown')}",
                f"  Pythonç‰ˆæœ¬: {system_info.get('python_version', 'Unknown')}",
                ""
            ])
        
        # CPUä¿¡æ¯
        cpu_info = self.hardware_info.get("cpu", {})
        if cpu_info:
            report_lines.extend([
                "ğŸ”§ CPUä¿¡æ¯:",
                f"  å‹å·: {cpu_info.get('model_name', 'Unknown')}",
                f"  ç‰©ç†æ ¸å¿ƒ: {cpu_info.get('physical_cores', 'Unknown')}",
                f"  é€»è¾‘æ ¸å¿ƒ: {cpu_info.get('logical_cores', 'Unknown')}",
                f"  å½“å‰ä½¿ç”¨ç‡: {cpu_info.get('usage_percent', 0)}%",
                ""
            ])
        
        # å†…å­˜ä¿¡æ¯
        memory_info = self.hardware_info.get("memory", {})
        if memory_info:
            report_lines.extend([
                "ğŸ’¾ å†…å­˜ä¿¡æ¯:",
                f"  æ€»å†…å­˜: {memory_info.get('total_gb', 0)}GB",
                f"  å¯ç”¨å†…å­˜: {memory_info.get('available_gb', 0)}GB",
                f"  ä½¿ç”¨ç‡: {memory_info.get('usage_percent', 0)}%",
                ""
            ])
        
        # GPUä¿¡æ¯
        gpu_info = self.hardware_info.get("gpu", {})
        if gpu_info:
            report_lines.extend([
                "ğŸ® GPUä¿¡æ¯:",
                f"  CUDAå¯ç”¨: {'æ˜¯' if gpu_info.get('cuda_available', False) else 'å¦'}",
                f"  GPUæ•°é‡: {gpu_info.get('gpu_count', 0)}",
            ])
            
            for gpu in gpu_info.get("gpus", []):
                report_lines.extend([
                    f"  GPU {gpu['id']}: {gpu['name']}",
                    f"    æ˜¾å­˜: {gpu['memory_total_gb']}GB (å¯ç”¨: {gpu['memory_free_gb']}GB)",
                ])
            report_lines.append("")
        
        # æ€§èƒ½è¯„åˆ†
        report_lines.extend([
            f"ğŸ“Š æ€§èƒ½è¯„åˆ†: {self.performance_score}/100",
            ""
        ])
        
        # å»ºè®®
        if self.recommendations:
            report_lines.extend([
                "ğŸ’¡ ä¼˜åŒ–å»ºè®®:",
            ])
            for rec in self.recommendations:
                report_lines.append(f"  â€¢ {rec}")
            report_lines.append("")
        
        report_lines.append("=" * 60)
        
        return "\n".join(report_lines)

# å…¨å±€ç¡¬ä»¶æ£€æµ‹å™¨å®ä¾‹
hardware_detector = HardwareDetector() 