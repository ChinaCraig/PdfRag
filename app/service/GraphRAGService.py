"""
GraphRAGæœåŠ¡ - å®Œå…¨é‡æ„ç‰ˆ
è´Ÿè´£PDFæ–‡ä»¶çš„å¤šæ¨¡æ€å†…å®¹è¯†åˆ«å’ŒçŸ¥è¯†å›¾è°±æ„å»º
æ”¯æŒæ–‡å­—ã€è¡¨æ ¼ã€å›¾ç‰‡ã€å›¾è¡¨çš„å®Œæ•´è¯†åˆ«å’Œç†è§£
"""
import os
import uuid
import logging
import json
import threading
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from PIL import Image
import fitz  # PyMuPDF
import io
import tempfile
import requests
import cv2
import numpy as np

from utils.config_loader import config_loader
from utils.database import mysql_manager, milvus_manager, neo4j_manager
from utils.model_manager import model_manager

logger = logging.getLogger(__name__)

class GraphRAGService:
    """GraphRAGæœåŠ¡ç±» - å¤šæ¨¡æ€å†…å®¹è¯†åˆ«å’ŒçŸ¥è¯†å›¾è°±æ„å»º"""
    
    def __init__(self):
        self.config = config_loader.get_app_config()
        self.model_config = config_loader.get_model_config()
        self.prompt_config = config_loader.get_prompt_config()
        self.graphrag_config = self.config.get("graph_rag", {})
        self.multimedia_config = self.config.get("multimedia", {})
        
        # å¤šæ¨¡æ€é…ç½®
        self.multimodal_config = self.graphrag_config.get("multimodal", {})
        self.image_config = self.multimodal_config.get("image_processing", {})
        self.table_config = self.multimodal_config.get("table_processing", {})
        self.chart_config = self.multimodal_config.get("chart_processing", {})
        
        # å¤„ç†çŠ¶æ€è·Ÿè¸ª
        self.processing_status = {}
        
        # ç¡®ä¿å¤šåª’ä½“ç›®å½•å­˜åœ¨
        self._ensure_multimedia_directories()
        
        logger.info("GraphRAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ - å¤šæ¨¡æ€ç‰ˆæœ¬")
    
    def _ensure_multimedia_directories(self):
        """ç¡®ä¿å¤šåª’ä½“ç›®å½•å­˜åœ¨"""
        directories = [
            self.multimedia_config.get("images", {}).get("save_dir", "uploads/images"),
            self.multimedia_config.get("tables", {}).get("export_dir", "uploads/tables"),
            self.multimedia_config.get("charts", {}).get("save_dir", "uploads/charts")
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def process_pdf_file(self, file_id: str, file_path: str) -> Dict[str, Any]:
        """
        å¤„ç†PDFæ–‡ä»¶çš„å®Œæ•´æµç¨‹
        
        Args:
            file_id: æ–‡ä»¶ID
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹GraphRAGå¤„ç†: {file_id}")
            
            # åˆå§‹åŒ–å¤„ç†çŠ¶æ€
            self._update_processing_status(file_id, "processing", 0, "å¼€å§‹å¤„ç†...")
            
            # ç¬¬ä¸€æ­¥ï¼šPDFå¤šæ¨¡æ€å†…å®¹æå–
            logger.info(f"ğŸ“– æ­¥éª¤1: PDFå¤šæ¨¡æ€å†…å®¹æå–")
            extraction_result = self._extract_multimodal_content(file_id, file_path)
            self._update_processing_status(file_id, "processing", 30, f"å†…å®¹æå–å®Œæˆ")
            
            if not extraction_result["success"]:
                raise ValueError(f"PDFå†…å®¹æå–å¤±è´¥: {extraction_result['message']}")
            
            content_chunks = extraction_result["content_chunks"]
            
            # ç¬¬äºŒæ­¥ï¼šç”ŸæˆåµŒå…¥å‘é‡
            logger.info(f"ğŸ”¤ æ­¥éª¤2: ç”ŸæˆåµŒå…¥å‘é‡")
            self._generate_embeddings_for_chunks(content_chunks, file_id)
            self._update_processing_status(file_id, "processing", 50, "åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆ")
            
            # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜åˆ°å‘é‡æ•°æ®åº“
            logger.info(f"ğŸ’¾ æ­¥éª¤3: ä¿å­˜åˆ°å‘é‡æ•°æ®åº“")
            self._save_chunks_to_vector_db(content_chunks, file_id)
            self._update_processing_status(file_id, "processing", 65, "å‘é‡æ•°æ®ä¿å­˜å®Œæˆ")
            
            # ç¬¬å››æ­¥ï¼šçŸ¥è¯†å›¾è°±æ„å»º
            logger.info(f"ğŸ§  æ­¥éª¤4: çŸ¥è¯†å›¾è°±æ„å»º")
            self._update_processing_status(file_id, "processing", 67, "ğŸ§  å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            kg_result = self._build_knowledge_graph(content_chunks, file_id)
            self._update_processing_status(file_id, "processing", 86, 
                                         f"ğŸ§  çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼šæ•´ç†å‡º{kg_result['entities_count']}ä¸ªå®ä½“ï¼Œ{kg_result['relations_count']}ä¸ªå…³ç³»")
            
            # ç¬¬äº”æ­¥ï¼šä¿å­˜åˆ°å›¾æ•°æ®åº“
            logger.info(f"ğŸ•¸ï¸ æ­¥éª¤5: ä¿å­˜åˆ°å›¾æ•°æ®åº“")
            self._update_processing_status(file_id, "processing", 88, 
                                         f"ğŸ•¸ï¸ å¼€å§‹ä¿å­˜{kg_result['entities_count']}ä¸ªå®ä½“å’Œ{kg_result['relations_count']}ä¸ªå…³ç³»åˆ°Neo4j...")
            self._save_knowledge_graph_to_db(kg_result["entities"], kg_result["relations"], file_id)
            self._update_processing_status(file_id, "processing", 98, 
                                         f"ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±ä¿å­˜å®Œæˆï¼Œå…±å­˜å‚¨{kg_result['entities_count']}ä¸ªå®ä½“å’Œ{kg_result['relations_count']}ä¸ªå…³ç³»")
            
            # æ›´æ–°æœ€ç»ˆçŠ¶æ€
            total_statistics = f"å¤„ç†å®Œæˆï¼å…±æå–{len(content_chunks)}ä¸ªå†…å®¹å—ï¼Œç”Ÿæˆ{kg_result['entities_count']}ä¸ªå®ä½“ï¼Œ{kg_result['relations_count']}ä¸ªå…³ç³»"
            self._update_processing_status(file_id, "completed", 100, f"ğŸ‰ GraphRAG {total_statistics}")
            
            result = {
                "success": True,
                "message": "GraphRAGå¤„ç†å®Œæˆ",
                "statistics": {
                    "total_chunks": len(content_chunks),
                    "text_chunks": len([c for c in content_chunks if c["content_type"] == "text"]),
                    "image_chunks": len([c for c in content_chunks if c["content_type"] == "image"]),
                    "table_chunks": len([c for c in content_chunks if c["content_type"] == "table"]),
                    "chart_chunks": len([c for c in content_chunks if c["content_type"] == "chart"]),
                    "entities_count": kg_result["entities_count"],
                    "relations_count": kg_result["relations_count"]
                }
            }
            
            logger.info(f"âœ… GraphRAGå¤„ç†å®Œæˆ: {file_id}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ GraphRAGå¤„ç†å¤±è´¥: {file_id}, é”™è¯¯: {e}", exc_info=True)
            self._update_processing_status(file_id, "failed", 0, f"å¤„ç†å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "message": f"GraphRAGå¤„ç†å¤±è´¥: {str(e)}"
            }
    
    def _extract_multimodal_content(self, file_id: str, file_path: str) -> Dict[str, Any]:
        """
        æå–PDFçš„å¤šæ¨¡æ€å†…å®¹
        
        Args:
            file_id: æ–‡ä»¶ID
            file_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            æå–ç»“æœ
        """
        try:
            content_chunks = []
            
            # æ‰“å¼€PDFæ–‡æ¡£
            doc = fitz.open(file_path)
            total_pages = len(doc)
            logger.info(f"ğŸ“„ PDFå…±{total_pages}é¡µ")
            
            # åˆå§‹çŠ¶æ€
            self._update_processing_status(file_id, "processing", 5, 
                                         f"ğŸ“„ å¼€å§‹æå–PDFå†…å®¹ï¼Œå…±{total_pages}é¡µ")
            
            for page_num in range(total_pages):
                logger.info(f"ğŸ“– å¤„ç†ç¬¬{page_num + 1}/{total_pages}é¡µ")
                page = doc[page_num]
                
                # æå–æ–‡æœ¬å†…å®¹
                self._update_processing_status(file_id, "processing", 
                                             5 + int((page_num + 0.2) / total_pages * 25), 
                                             f"ğŸ“– æ­£åœ¨æå–ç¬¬{page_num + 1}é¡µæ–‡æœ¬å†…å®¹...")
                text_chunks = self._extract_text_content(page, file_id, page_num)
                content_chunks.extend(text_chunks)
                
                # æå–å›¾åƒå†…å®¹
                self._update_processing_status(file_id, "processing", 
                                             5 + int((page_num + 0.4) / total_pages * 25), 
                                             f"ğŸ–¼ï¸ æ­£åœ¨æå–ç¬¬{page_num + 1}é¡µå›¾åƒå†…å®¹...")
                image_chunks = self._extract_image_content(page, file_id, page_num)
                content_chunks.extend(image_chunks)
                
                # æå–è¡¨æ ¼å†…å®¹
                self._update_processing_status(file_id, "processing", 
                                             5 + int((page_num + 0.6) / total_pages * 25), 
                                             f"ğŸ“Š æ­£åœ¨æå–ç¬¬{page_num + 1}é¡µè¡¨æ ¼å†…å®¹...")
                table_chunks = self._extract_table_content(page, file_id, page_num)
                content_chunks.extend(table_chunks)
                
                # æå–å›¾è¡¨å†…å®¹
                self._update_processing_status(file_id, "processing", 
                                             5 + int((page_num + 0.8) / total_pages * 25), 
                                             f"ğŸ“ˆ æ­£åœ¨æå–ç¬¬{page_num + 1}é¡µå›¾è¡¨å†…å®¹...")
                chart_chunks = self._extract_chart_content(page, file_id, page_num)
                content_chunks.extend(chart_chunks)
                
                # é¡µé¢å¤„ç†å®Œæˆ - æ˜¾ç¤ºå½“å‰é¡µå‘ç°çš„å†…å®¹ç»Ÿè®¡
                page_text_count = len([c for c in text_chunks])
                page_image_count = len([c for c in image_chunks])
                page_table_count = len([c for c in table_chunks])
                page_chart_count = len([c for c in chart_chunks])
                
                page_summary = []
                if page_text_count > 0:
                    page_summary.append(f"{page_text_count}ä¸ªæ–‡æœ¬å—")
                if page_image_count > 0:
                    page_summary.append(f"{page_image_count}ä¸ªå›¾åƒ")
                if page_table_count > 0:
                    page_summary.append(f"{page_table_count}ä¸ªè¡¨æ ¼")
                if page_chart_count > 0:
                    page_summary.append(f"{page_chart_count}ä¸ªå›¾è¡¨")
                
                progress = 5 + int((page_num + 1) / total_pages * 25)
                if page_summary:
                    status_message = f"âœ… ç¬¬{page_num + 1}/{total_pages}é¡µå®Œæˆï¼Œå‘ç°{', '.join(page_summary)}"
                else:
                    status_message = f"âœ… ç¬¬{page_num + 1}/{total_pages}é¡µå®Œæˆ"
                
                self._update_processing_status(file_id, "processing", progress, status_message)
            
            doc.close()
            
            logger.info(f"âœ… PDFå¤šæ¨¡æ€å†…å®¹æå–å®Œæˆï¼Œå…±{len(content_chunks)}ä¸ªå†…å®¹å—")
            return {
                "success": True,
                "content_chunks": content_chunks,
                "statistics": {
                    "total_chunks": len(content_chunks),
                    "text_chunks": len([c for c in content_chunks if c["content_type"] == "text"]),
                    "image_chunks": len([c for c in content_chunks if c["content_type"] == "image"]),
                    "table_chunks": len([c for c in content_chunks if c["content_type"] == "table"]),
                    "chart_chunks": len([c for c in content_chunks if c["content_type"] == "chart"])
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ PDFå¤šæ¨¡æ€å†…å®¹æå–å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"PDFå†…å®¹æå–å¤±è´¥: {str(e)}",
                "content_chunks": []
            }
    
    def _extract_text_content(self, page, file_id: str, page_num: int) -> List[Dict[str, Any]]:
        """æå–æ–‡æœ¬å†…å®¹"""
        chunks = []
        
        try:
            # è·å–é¡µé¢æ–‡æœ¬
            text = page.get_text()
            if not text.strip():
                return chunks
            
            # åˆ†å—è®¾ç½®
            chunk_size = self.graphrag_config.get("chunk_size", 1000)
            chunk_overlap = self.graphrag_config.get("chunk_overlap", 200)
            
            # æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼ˆæŒ‰æ®µè½å’Œå¥å­ï¼‰
            text_chunks = self._smart_text_chunking(text, chunk_size, chunk_overlap)
            
            for chunk_index, chunk_text in enumerate(text_chunks):
                if chunk_text.strip():
                    chunk_id = f"{file_id}_page_{page_num}_text_{chunk_index}"
                    chunks.append({
                        "chunk_id": chunk_id,
                        "file_id": file_id,
                        "content": chunk_text.strip(),
                        "content_type": "text",
                        "page_number": page_num,
                        "chunk_index": chunk_index,
                        "metadata": {
                            "page": page_num,
                            "type": "text",
                            "length": len(chunk_text.strip()),
                            "language": self._detect_language(chunk_text.strip())
                        }
                    })
            
            logger.debug(f"ğŸ“ ç¬¬{page_num + 1}é¡µæå–{len(chunks)}ä¸ªæ–‡æœ¬å—")
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬æå–å¤±è´¥: {e}")
            return []
    
    def _extract_image_content(self, page, file_id: str, page_num: int) -> List[Dict[str, Any]]:
        """æå–å¹¶åˆ†æå›¾åƒå†…å®¹"""
        chunks = []
        
        try:
            if not self.image_config.get("enabled", True):
                return chunks
                
            image_list = page.get_images()
            logger.debug(f"ğŸ“· ç¬¬{page_num + 1}é¡µå‘ç°{len(image_list)}ä¸ªå›¾åƒ")
            
            for img_index, img in enumerate(image_list):
                try:
                    # æå–å›¾åƒæ•°æ®
                    xref = img[0]
                    pix = fitz.Pixmap(page.parent, xref)
                    
                    if pix.n - pix.alpha < 4:  # ç¡®ä¿ä¸æ˜¯CMYK
                        # æ£€æŸ¥å›¾åƒå¤§å°
                        if pix.width < 50 or pix.height < 50:
                            logger.debug(f"è·³è¿‡è¿‡å°çš„å›¾åƒ: {pix.width}x{pix.height}")
                            continue
                        
                        # ä¿å­˜å›¾åƒåˆ°æ–‡ä»¶ç³»ç»Ÿ
                        image_path = None
                        if self.image_config.get("save_to_filesystem", True):
                            image_path = self._save_image_to_filesystem(pix, file_id, page_num, img_index)
                        
                        # è¿›è¡Œå›¾åƒç†è§£åˆ†æ
                        image_analysis = self._analyze_image_content(pix, file_id, page_num, img_index)
                        
                        chunk_id = f"{file_id}_page_{page_num}_image_{img_index}"
                        chunks.append({
                            "chunk_id": chunk_id,
                            "file_id": file_id,
                            "content": image_analysis["description"],
                            "content_type": "image",
                            "page_number": page_num,
                            "chunk_index": img_index,
                            "image_path": image_path,
                            "metadata": {
                                "page": page_num,
                                "type": "image",
                                "width": pix.width,
                                "height": pix.height,
                                "format": img[8] if len(img) > 8 else "unknown",
                                "objects_detected": image_analysis.get("objects", []),
                                "scene_description": image_analysis.get("scene", ""),
                                "text_content": image_analysis.get("text_content", ""),
                                "visual_elements": image_analysis.get("visual_elements", [])
                            }
                        })
                        
                        logger.debug(f"âœ… ç¬¬{page_num + 1}é¡µç¬¬{img_index + 1}ä¸ªå›¾åƒåˆ†æå®Œæˆ")
                    
                    pix = None  # é‡Šæ”¾å†…å­˜
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ç¬¬{page_num + 1}é¡µç¬¬{img_index + 1}ä¸ªå›¾åƒå¤„ç†å¤±è´¥: {e}")
                    continue
            
            logger.debug(f"ğŸ“· ç¬¬{page_num + 1}é¡µæå–{len(chunks)}ä¸ªå›¾åƒå—")
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ å›¾åƒæå–å¤±è´¥: {e}")
            return []
    
    def _extract_table_content(self, page, file_id: str, page_num: int) -> List[Dict[str, Any]]:
        """æå–å¹¶åˆ†æè¡¨æ ¼å†…å®¹"""
        chunks = []
        
        try:
            if not self.table_config.get("enabled", True):
                return chunks
                
            # ä½¿ç”¨PyMuPDFçš„è¡¨æ ¼æ£€æµ‹
            table_finder = page.find_tables()
            tables = list(table_finder)
            
            for table_index, table in enumerate(tables):
                try:
                    # æå–è¡¨æ ¼æ•°æ®
                    table_data = table.extract()
                    if table_data and len(table_data) > 1:  # è‡³å°‘æœ‰æ ‡é¢˜è¡Œå’Œæ•°æ®è¡Œ
                        
                        # æ ¼å¼åŒ–è¡¨æ ¼å†…å®¹
                        table_analysis = self._analyze_table_content(table_data, page_num, table_index)
                        
                        # ä¿å­˜è¡¨æ ¼åˆ°æ–‡ä»¶ç³»ç»Ÿï¼ˆå¦‚æœé…ç½®äº†ï¼‰
                        table_path = None
                        if self.table_config.get("keep_full_table", True):
                            table_path = self._save_table_to_filesystem(table_data, file_id, page_num, table_index)
                        
                        chunk_id = f"{file_id}_page_{page_num}_table_{table_index}"
                        chunks.append({
                            "chunk_id": chunk_id,
                            "file_id": file_id,
                            "content": table_analysis["formatted_content"],
                            "content_type": "table",
                            "page_number": page_num,
                            "chunk_index": table_index,
                            "table_path": table_path,
                            "table_data": table_data,  # ä¿ç•™åŸå§‹è¡¨æ ¼æ•°æ®ç”¨äºæ£€ç´¢
                            "metadata": {
                                "page": page_num,
                                "type": "table",
                                "table_index": table_index,
                                "rows": len(table_data),
                                "columns": len(table_data[0]) if table_data else 0,
                                "summary": table_analysis.get("summary", ""),
                                "data_types": table_analysis.get("data_types", []),
                                "key_insights": table_analysis.get("key_insights", [])
                            }
                        })
                        
                        logger.debug(f"âœ… ç¬¬{page_num + 1}é¡µè¡¨æ ¼{table_index + 1}åˆ†æå®Œæˆ")
                
                except Exception as e:
                    logger.warning(f"âš ï¸ å¤„ç†è¡¨æ ¼å¤±è´¥: {e}")
                    continue
            
            logger.debug(f"ğŸ“Š ç¬¬{page_num + 1}é¡µæå–{len(chunks)}ä¸ªè¡¨æ ¼")
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ è¡¨æ ¼æå–å¤±è´¥: {e}")
            return []
    
    def _extract_chart_content(self, page, file_id: str, page_num: int) -> List[Dict[str, Any]]:
        """æå–å¹¶åˆ†æå›¾è¡¨å†…å®¹"""
        chunks = []
        
        try:
            if not self.chart_config.get("enabled", True):
                return chunks
                
            # è·å–é¡µé¢å›¾åƒç”¨äºå›¾è¡¨æ£€æµ‹
            pix = page.get_pixmap()
            img_data = pix.tobytes("png")
            
            # ä½¿ç”¨å›¾è¡¨è¯†åˆ«æ¨¡å‹åˆ†æ
            chart_analysis = self._analyze_chart_content(img_data, file_id, page_num)
            
            if chart_analysis and chart_analysis.get("charts_detected"):
                for chart_index, chart_info in enumerate(chart_analysis["charts_detected"]):
                    # ä¿å­˜å›¾è¡¨å›¾åƒ
                    chart_path = self._save_chart_to_filesystem(chart_info["image_data"], 
                                                              file_id, page_num, chart_index)
                    
                    chunk_id = f"{file_id}_page_{page_num}_chart_{chart_index}"
                    chunks.append({
                        "chunk_id": chunk_id,
                        "file_id": file_id,
                        "content": chart_info["description"],
                        "content_type": "chart",
                        "page_number": page_num,
                        "chunk_index": chart_index,
                        "chart_path": chart_path,
                        "metadata": {
                            "page": page_num,
                            "type": "chart",
                            "chart_type": chart_info.get("chart_type", "unknown"),
                            "data_points": chart_info.get("data_points", []),
                            "trend_analysis": chart_info.get("trend_analysis", ""),
                            "statistical_summary": chart_info.get("statistical_summary", {}),
                            "axis_labels": chart_info.get("axis_labels", {}),
                            "legend_info": chart_info.get("legend_info", [])
                        }
                    })
                    
                    logger.debug(f"âœ… ç¬¬{page_num + 1}é¡µå›¾è¡¨{chart_index + 1}åˆ†æå®Œæˆ")
            
            logger.debug(f"ğŸ“ˆ ç¬¬{page_num + 1}é¡µæå–{len(chunks)}ä¸ªå›¾è¡¨")
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ å›¾è¡¨æå–å¤±è´¥: {e}")
            return []
    
    def _smart_text_chunking(self, text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å—"""
        try:
            chunks = []
            
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = text.split('\n\n')
            current_chunk = ""
            
            for paragraph in paragraphs:
                paragraph = paragraph.strip()
                if not paragraph:
                    continue
                
                # å¦‚æœå½“å‰æ®µè½åŠ å…¥åè¶…è¿‡å—å¤§å°
                if len(current_chunk) + len(paragraph) > chunk_size:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                        
                        # å¤„ç†é‡å 
                        if chunk_overlap > 0 and len(current_chunk) > chunk_overlap:
                            current_chunk = current_chunk[-chunk_overlap:] + "\n" + paragraph
                        else:
                            current_chunk = paragraph
                    else:
                        # å•ä¸ªæ®µè½è¿‡é•¿ï¼ŒæŒ‰å¥å­åˆ†å‰²
                        sentences = self._split_long_paragraph(paragraph, chunk_size)
                        chunks.extend(sentences[:-1])
                        current_chunk = sentences[-1] if sentences else ""
                else:
                    if current_chunk:
                        current_chunk += "\n\n" + paragraph
                    else:
                        current_chunk = paragraph
            
            # æ·»åŠ æœ€åä¸€ä¸ªå—
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            
            return chunks
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æ–‡æœ¬åˆ†å—å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•åˆ†å—
            return self._simple_text_chunking(text, chunk_size, chunk_overlap)
    
    def _split_long_paragraph(self, paragraph: str, max_size: int) -> List[str]:
        """åˆ†å‰²è¿‡é•¿çš„æ®µè½"""
        try:
            # æŒ‰å¥å­åˆ†å‰²
            import re
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', paragraph)
            
            chunks = []
            current_chunk = ""
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                    
                if len(current_chunk) + len(sentence) > max_size:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                        current_chunk = sentence
                    else:
                        # å•ä¸ªå¥å­è¿‡é•¿ï¼Œå¼ºåˆ¶åˆ†å‰²
                        chunks.append(sentence[:max_size])
                        current_chunk = sentence[max_size:]
                else:
                    if current_chunk:
                        current_chunk += "ã€‚" + sentence
                    else:
                        current_chunk = sentence
            
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            
            return chunks
            
        except Exception as e:
            logger.error(f"æ®µè½åˆ†å‰²å¤±è´¥: {e}")
            return [paragraph]
    
    def _simple_text_chunking(self, text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
        """ç®€å•æ–‡æœ¬åˆ†å—ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end].strip()
            
            if chunk:
                chunks.append(chunk)
            
            start = end - chunk_overlap
            if start >= len(text):
                break
        
        return chunks
    
    def _detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        try:
            # ç®€å•çš„è¯­è¨€æ£€æµ‹
            chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
            total_chars = len([c for c in text if c.isalpha()])
            
            if total_chars > 0 and chinese_chars / total_chars > 0.3:
                return "zh"
            else:
                return "en"
        except:
            return "unknown"
    
    def _save_image_to_filesystem(self, pix, file_id: str, page_num: int, img_index: int) -> str:
        """ä¿å­˜å›¾åƒåˆ°æ–‡ä»¶ç³»ç»Ÿ"""
        try:
            image_dir = self.multimedia_config.get("images", {}).get("save_dir", "uploads/images")
            os.makedirs(image_dir, exist_ok=True)
            
            image_filename = f"{file_id}_page_{page_num}_image_{img_index}.png"
            image_path = os.path.join(image_dir, image_filename)
            
            pix.save(image_path)
            
            return image_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜å›¾åƒå¤±è´¥: {e}")
            return None
    
    # å¾…ç»­...ï¼ˆç”±äºå“åº”é•¿åº¦é™åˆ¶ï¼Œè¿™é‡Œå…ˆæä¾›éƒ¨åˆ†ä»£ç ï¼‰

    def _analyze_image_content(self, pix, file_id: str, page_num: int, img_index: int) -> Dict[str, Any]:
        """æ·±åº¦åˆ†æå›¾åƒå†…å®¹"""
        try:
            # åŸºç¡€ä¿¡æ¯
            width, height = pix.width, pix.height
            
            result = {
                "description": f"å›¾åƒ(ç¬¬{page_num + 1}é¡µ)ï¼šå°ºå¯¸{width}x{height}",
                "objects": [],
                "scene": "",
                "text_content": "",
                "visual_elements": []
            }
            
            # ä¿å­˜ä¸´æ—¶å›¾åƒç”¨äºåˆ†æ
            temp_path = f"temp_img_{file_id}_{page_num}_{img_index}.png"
            
            try:
                pix.save(temp_path)
                
                # ä½¿ç”¨OCRæå–å›¾åƒä¸­çš„æ–‡å­—
                if self.image_config.get("text_detection", True):
                    ocr_text = self._extract_text_from_image(temp_path)
                    if ocr_text:
                        result["text_content"] = ocr_text
                        result["description"] += f"ï¼ŒåŒ…å«æ–‡å­—ï¼š{ocr_text[:100]}"
                
                # ä½¿ç”¨å›¾åƒç†è§£æ¨¡å‹åˆ†æ
                if self.image_config.get("understanding_model"):
                    understanding_result = self._image_understanding_analysis(temp_path)
                    if understanding_result:
                        result.update(understanding_result)
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_path):
                    os.remove(temp_path)
                
                return result
                
            except Exception as e:
                # ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_path):
                    os.remove(temp_path)
                logger.warning(f"å›¾åƒå†…å®¹åˆ†æå¤±è´¥: {e}")
                return result
            
        except Exception as e:
            logger.error(f"å›¾åƒåˆ†æå¤±è´¥: {e}")
            return {
                "description": f"å›¾åƒ(ç¬¬{page_num + 1}é¡µ)ï¼šåˆ†æå¤±è´¥",
                "objects": [],
                "scene": "",
                "text_content": "",
                "visual_elements": []
            }
    
    def _extract_text_from_image(self, image_path: str) -> str:
        """ä»å›¾åƒä¸­æå–æ–‡å­—"""
        try:
            # ä½¿ç”¨æ¨¡å‹ç®¡ç†å™¨çš„OCRåŠŸèƒ½
            ocr_results = model_manager.extract_text_from_image(image_path)
            
            if ocr_results:
                return " ".join([result.get("text", "") for result in ocr_results if result.get("text")])
            return ""
            
        except Exception as e:
            logger.warning(f"å›¾åƒOCRå¤±è´¥: {e}")
            return ""
    
    def _image_understanding_analysis(self, image_path: str) -> Dict[str, Any]:
        """å›¾åƒç†è§£åˆ†æ"""
        try:
            # è¿™é‡Œå¯ä»¥é›†æˆå›¾åƒç†è§£æ¨¡å‹ï¼ˆå¦‚BLIP2ï¼‰
            # æš‚æ—¶è¿”å›åŸºç¡€åˆ†æç»“æœ
            logger.debug("å›¾åƒç†è§£åˆ†æåŠŸèƒ½å¾…å®ç°")
            return {
                "scene": "åœºæ™¯åˆ†æå¾…å®ç°",
                "objects": ["å¯¹è±¡æ£€æµ‹å¾…å®ç°"],
                "visual_elements": ["è§†è§‰å…ƒç´ åˆ†æå¾…å®ç°"]
            }
            
        except Exception as e:
            logger.error(f"å›¾åƒç†è§£åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _analyze_table_content(self, table_data: List[List[str]], page_num: int, table_index: int) -> Dict[str, Any]:
        """åˆ†æè¡¨æ ¼å†…å®¹"""
        try:
            if not table_data:
                return {"formatted_content": f"è¡¨æ ¼(ç¬¬{page_num + 1}é¡µ)ï¼šç©ºè¡¨æ ¼"}
            
            # æ ¼å¼åŒ–è¡¨æ ¼å†…å®¹
            content_lines = [f"è¡¨æ ¼(ç¬¬{page_num + 1}é¡µï¼Œè¡¨{table_index + 1})ï¼š"]
            
            # è¡¨å¤´
            if table_data:
                headers = table_data[0]
                content_lines.append("è¡¨å¤´ï¼š" + " | ".join(str(cell) for cell in headers))
                
                # æ•°æ®è¡Œ
                max_rows = self.table_config.get("max_embed_rows", 50)
                data_rows = table_data[1:min(max_rows + 1, len(table_data))]
                
                for i, row in enumerate(data_rows, 1):
                    row_text = " | ".join(str(cell) for cell in row)
                    content_lines.append(f"ç¬¬{i}è¡Œï¼š{row_text}")
                
                if len(table_data) > max_rows + 1:
                    content_lines.append(f"... (å…±{len(table_data) - 1}è¡Œæ•°æ®)")
            
            # ç”Ÿæˆè¡¨æ ¼æ‘˜è¦
            summary = ""
            if self.table_config.get("generate_summary", True):
                summary = self._generate_table_summary(table_data)
            
            # åˆ†ææ•°æ®ç±»å‹
            data_types = self._analyze_table_data_types(table_data)
            
            return {
                "formatted_content": "\n".join(content_lines),
                "summary": summary,
                "data_types": data_types,
                "key_insights": self._extract_table_insights(table_data)
            }
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼å†…å®¹åˆ†æå¤±è´¥: {e}")
            return {"formatted_content": f"è¡¨æ ¼(ç¬¬{page_num + 1}é¡µ)ï¼šåˆ†æå¤±è´¥"}
    
    def _generate_table_summary(self, table_data: List[List[str]]) -> str:
        """ç”Ÿæˆè¡¨æ ¼æ‘˜è¦"""
        try:
            if not table_data or len(table_data) < 2:
                return "è¡¨æ ¼æ•°æ®ä¸è¶³"
            
            rows = len(table_data) - 1  # æ’é™¤è¡¨å¤´
            cols = len(table_data[0])
            
            summary = f"åŒ…å«{rows}è¡Œ{cols}åˆ—æ•°æ®"
            
            # ç®€å•çš„æ•°æ®ç‰¹å¾åˆ†æ
            if len(table_data) > 1:
                headers = table_data[0]
                summary += f"ï¼Œä¸»è¦å­—æ®µï¼š{', '.join(headers[:3])}"
                if len(headers) > 3:
                    summary += "ç­‰"
            
            return summary
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return "æ‘˜è¦ç”Ÿæˆå¤±è´¥"
    
    def _analyze_table_data_types(self, table_data: List[List[str]]) -> List[str]:
        """åˆ†æè¡¨æ ¼æ•°æ®ç±»å‹"""
        try:
            if not table_data or len(table_data) < 2:
                return []
            
            data_types = []
            cols = len(table_data[0])
            
            for col_idx in range(cols):
                # æ”¶é›†è¯¥åˆ—çš„æ•°æ®æ ·æœ¬
                col_data = []
                for row_idx in range(1, min(6, len(table_data))):  # å–å‰5è¡Œæ•°æ®åˆ†æ
                    if col_idx < len(table_data[row_idx]):
                        col_data.append(str(table_data[row_idx][col_idx]).strip())
                
                # ç®€å•çš„ç±»å‹æ¨æ–­
                if not col_data:
                    data_types.append("empty")
                elif all(self._is_number(val) for val in col_data if val):
                    data_types.append("number")
                elif all(self._is_date(val) for val in col_data if val):
                    data_types.append("date")
                else:
                    data_types.append("text")
            
            return data_types
            
        except Exception as e:
            logger.error(f"æ•°æ®ç±»å‹åˆ†æå¤±è´¥: {e}")
            return []
    
    def _is_number(self, value: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—"""
        try:
            float(value.replace(",", "").replace("%", ""))
            return True
        except:
            return False
    
    def _is_date(self, value: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ—¥æœŸ"""
        try:
            import re
            # ç®€å•çš„æ—¥æœŸæ ¼å¼æ£€æŸ¥
            date_patterns = [
                r'\d{4}-\d{2}-\d{2}',
                r'\d{2}/\d{2}/\d{4}',
                r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥'
            ]
            return any(re.match(pattern, value) for pattern in date_patterns)
        except:
            return False
    
    def _extract_table_insights(self, table_data: List[List[str]]) -> List[str]:
        """æå–è¡¨æ ¼å…³é”®è§è§£"""
        try:
            insights = []
            
            if not table_data or len(table_data) < 2:
                return insights
            
            # åŸºç¡€ç»Ÿè®¡
            rows = len(table_data) - 1
            cols = len(table_data[0])
            insights.append(f"æ•°æ®ç»´åº¦ï¼š{rows}è¡ŒÃ—{cols}åˆ—")
            
            # æ£€æŸ¥æ•°å€¼åˆ—çš„ç»Ÿè®¡ä¿¡æ¯
            for col_idx in range(cols):
                col_name = table_data[0][col_idx] if table_data[0] else f"åˆ—{col_idx + 1}"
                numbers = []
                
                for row_idx in range(1, len(table_data)):
                    if col_idx < len(table_data[row_idx]):
                        val = str(table_data[row_idx][col_idx]).strip()
                        if self._is_number(val):
                            try:
                                numbers.append(float(val.replace(",", "").replace("%", "")))
                            except:
                                pass
                
                if len(numbers) > 0:
                    avg = sum(numbers) / len(numbers)
                    insights.append(f"{col_name}å¹³å‡å€¼ï¼š{avg:.2f}")
            
            return insights[:5]  # é™åˆ¶è§è§£æ•°é‡
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼è§è§£æå–å¤±è´¥: {e}")
            return []
    
    def _save_table_to_filesystem(self, table_data: List[List[str]], file_id: str, page_num: int, table_index: int) -> str:
        """ä¿å­˜è¡¨æ ¼åˆ°æ–‡ä»¶ç³»ç»Ÿ"""
        try:
            table_dir = self.multimedia_config.get("tables", {}).get("export_dir", "uploads/tables")
            os.makedirs(table_dir, exist_ok=True)
            
            # ä¿å­˜ä¸ºCSVæ ¼å¼
            import csv
            table_filename = f"{file_id}_page_{page_num}_table_{table_index}.csv"
            table_path = os.path.join(table_dir, table_filename)
            
            with open(table_path, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerows(table_data)
            
            return table_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜è¡¨æ ¼å¤±è´¥: {e}")
            return None
    
    def _analyze_chart_content(self, img_data: bytes, file_id: str, page_num: int) -> Dict[str, Any]:
        """åˆ†æå›¾è¡¨å†…å®¹"""
        try:
            # å›¾è¡¨æ£€æµ‹å’Œåˆ†æé€»è¾‘ï¼ˆå¾…å®Œæ•´å®ç°ï¼‰
            logger.debug("å›¾è¡¨åˆ†æåŠŸèƒ½å¾…å®Œæ•´å®ç°")
            
            # æš‚æ—¶è¿”å›åŸºç¡€ç»“æœ
            return {
                "charts_detected": [],
                "analysis_complete": False
            }
            
        except Exception as e:
            logger.error(f"å›¾è¡¨åˆ†æå¤±è´¥: {e}")
            return {"charts_detected": []}
    
    def _save_chart_to_filesystem(self, chart_image_data: bytes, file_id: str, page_num: int, chart_index: int) -> str:
        """ä¿å­˜å›¾è¡¨åˆ°æ–‡ä»¶ç³»ç»Ÿ"""
        try:
            chart_dir = self.multimedia_config.get("charts", {}).get("save_dir", "uploads/charts")
            os.makedirs(chart_dir, exist_ok=True)
            
            chart_filename = f"{file_id}_page_{page_num}_chart_{chart_index}.png"
            chart_path = os.path.join(chart_dir, chart_filename)
            
            with open(chart_path, 'wb') as f:
                f.write(chart_image_data)
            
            return chart_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜å›¾è¡¨å¤±è´¥: {e}")
            return None
    
    def _generate_embeddings_for_chunks(self, chunks: List[Dict[str, Any]], file_id: str) -> None:
        """ä¸ºå†…å®¹å—ç”ŸæˆåµŒå…¥å‘é‡"""
        try:
            # æå–æ–‡æœ¬å†…å®¹
            texts = [chunk["content"] for chunk in chunks]
            total_chunks = len(texts)
            logger.info(f"ğŸ”¤ å¼€å§‹ç”Ÿæˆ{total_chunks}ä¸ªå†…å®¹å—çš„åµŒå…¥å‘é‡...")
            
            # åˆå§‹çŠ¶æ€æ›´æ–°
            self._update_processing_status(file_id, "processing", 32, 
                                         f"ğŸ”¤ å‡†å¤‡ä¸º{total_chunks}ä¸ªå†…å®¹å—ç”ŸæˆåµŒå…¥å‘é‡...")
            
            # åˆ†æ‰¹å¤„ç†åµŒå…¥å‘é‡ç”Ÿæˆï¼Œé¿å…å†…å­˜è¿‡è½½
            batch_size = 50  # æ¯æ‰¹å¤„ç†50ä¸ªæ–‡æœ¬
            all_embeddings = []
            total_batches = (total_chunks + batch_size - 1) // batch_size
            
            for i in range(0, total_chunks, batch_size):
                batch_texts = texts[i:i + batch_size]
                current_batch = i // batch_size + 1
                batch_progress = 32 + int((i / total_chunks) * 18)  # 32% åˆ° 50%
                
                self._update_processing_status(file_id, "processing", batch_progress, 
                                             f"ğŸ”¤ æ­£åœ¨ç”Ÿæˆç¬¬{current_batch}/{total_batches}æ‰¹åµŒå…¥å‘é‡ ({i+1}-{min(i+batch_size, total_chunks)}/{total_chunks})")
                
                # ç”Ÿæˆå½“å‰æ‰¹æ¬¡çš„åµŒå…¥å‘é‡
                batch_embeddings = model_manager.get_embedding(batch_texts)
                all_embeddings.extend(batch_embeddings)
                
                logger.info(f"ğŸ”¤ å®Œæˆæ‰¹æ¬¡ {current_batch}/{total_batches}")
            
            # åˆ†é…ç»™å„ä¸ªå—
            for chunk, embedding in zip(chunks, all_embeddings):
                chunk["embedding"] = embedding
            
            logger.info(f"âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œå…±{len(all_embeddings)}ä¸ª768ç»´å‘é‡")
            
        except Exception as e:
            logger.error(f"âŒ åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def _save_chunks_to_vector_db(self, chunks: List[Dict[str, Any]], file_id: str) -> None:
        """ä¿å­˜å†…å®¹å—åˆ°å‘é‡æ•°æ®åº“"""
        try:
            total_chunks = len(chunks)
            logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜{total_chunks}ä¸ªå‘é‡åˆ°Milvusæ•°æ®åº“...")
            
            # åˆå§‹çŠ¶æ€æ›´æ–°
            self._update_processing_status(file_id, "processing", 52, 
                                         "ğŸ’¾ æ­£åœ¨è¿æ¥å‘é‡æ•°æ®åº“...")
            
            # ç¡®ä¿Milvusè¿æ¥
            if not milvus_manager.collection:
                logger.info("åˆå§‹åŒ–Milvusè¿æ¥...")
                milvus_manager.connect()
            
            self._update_processing_status(file_id, "processing", 55, 
                                         f"ğŸ’¾ å‡†å¤‡ä¿å­˜{total_chunks}ä¸ªå‘é‡åˆ°Milvusæ•°æ®åº“...")
            
            # å‡†å¤‡å‘é‡æ•°æ®
            vector_data = []
            for i, chunk in enumerate(chunks):
                if i % 100 == 0:  # æ¯100ä¸ªchunkæ›´æ–°ä¸€æ¬¡è¿›åº¦
                    prep_progress = 55 + int((i / total_chunks) * 5)  # 55% åˆ° 60%
                    self._update_processing_status(file_id, "processing", prep_progress, 
                                                 f"ğŸ’¾ æ­£åœ¨å‡†å¤‡å‘é‡æ•°æ® ({i+1}/{total_chunks})")
                
                vector_data.append({
                    "file_id": chunk["file_id"],
                    "chunk_id": chunk["chunk_id"],
                    "content": chunk["content"],
                    "embedding": chunk["embedding"],
                    "metadata": json.dumps(chunk.get("metadata", {}))
                })
            
            self._update_processing_status(file_id, "processing", 60, 
                                         f"ğŸ’¾ å¼€å§‹æ‰¹é‡æ’å…¥{total_chunks}ä¸ªå‘é‡åˆ°æ•°æ®åº“...")
            
            # åˆ†æ‰¹æ’å…¥æ•°æ®ï¼Œé¿å…ä¸€æ¬¡æ€§æ’å…¥è¿‡å¤šæ•°æ®
            batch_size = 100
            total_insert_batches = (len(vector_data) + batch_size - 1) // batch_size
            for i in range(0, len(vector_data), batch_size):
                batch_data = vector_data[i:i + batch_size]
                current_insert_batch = i // batch_size + 1
                insert_progress = 60 + int((i / len(vector_data)) * 5)  # 60% åˆ° 65%
                
                self._update_processing_status(file_id, "processing", insert_progress, 
                                             f"ğŸ’¾ æ­£åœ¨æ’å…¥ç¬¬{current_insert_batch}/{total_insert_batches}æ‰¹å‘é‡æ•°æ® ({i+1}-{min(i+batch_size, len(vector_data))}/{len(vector_data)})")
                
                milvus_manager.insert_vectors(batch_data)
                logger.info(f"ğŸ’¾ æ’å…¥æ‰¹æ¬¡ {current_insert_batch}/{total_insert_batches}")
            
            logger.info(f"âœ… æˆåŠŸä¿å­˜{len(vector_data)}ä¸ªå‘é‡åˆ°Milvus")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å‘é‡æ•°æ®å¤±è´¥: {e}")
            raise
    
    def _build_knowledge_graph(self, chunks: List[Dict[str, Any]], file_id: str) -> Dict[str, Any]:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        try:
            # åˆ†ç¦»ä¸åŒç±»å‹çš„å†…å®¹å—
            text_chunks = [c for c in chunks if c["content_type"] == "text"]
            image_chunks = [c for c in chunks if c["content_type"] == "image"]
            table_chunks = [c for c in chunks if c["content_type"] == "table"]
            chart_chunks = [c for c in chunks if c["content_type"] == "chart"]
            
            self._update_processing_status(file_id, "processing", 68, 
                                         f"ğŸ§  å¼€å§‹åˆ†æå†…å®¹ï¼šæ–‡æœ¬{len(text_chunks)}å—ï¼Œè¡¨æ ¼{len(table_chunks)}ä¸ªï¼Œå›¾åƒ{len(image_chunks)}ä¸ªï¼Œå›¾è¡¨{len(chart_chunks)}ä¸ª")
            
            all_entities = []
            all_relations = []
            
            # ä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»
            if text_chunks:
                self._update_processing_status(file_id, "processing", 70, 
                                             f"ğŸ§  æ­£åœ¨ä»{len(text_chunks)}ä¸ªæ–‡æœ¬å—æå–å®ä½“å’Œå…³ç³»...")
                text_entities, text_relations = self._extract_entities_relations_from_text(text_chunks)
                all_entities.extend(text_entities)
                all_relations.extend(text_relations)
                self._update_processing_status(file_id, "processing", 75, 
                                             f"ğŸ“ æ–‡æœ¬åˆ†æå®Œæˆï¼šå‘ç°{len(text_entities)}ä¸ªå®ä½“ï¼Œ{len(text_relations)}ä¸ªå…³ç³»")
            
            # ä»è¡¨æ ¼ä¸­æå–å®ä½“å’Œå…³ç³»
            if table_chunks:
                self._update_processing_status(file_id, "processing", 77, 
                                             f"ğŸ“Š æ­£åœ¨ä»{len(table_chunks)}ä¸ªè¡¨æ ¼æå–å®ä½“å’Œå…³ç³»...")
                table_entities, table_relations = self._extract_entities_relations_from_tables(table_chunks)
                all_entities.extend(table_entities)
                all_relations.extend(table_relations)
                self._update_processing_status(file_id, "processing", 79, 
                                             f"ğŸ“Š è¡¨æ ¼åˆ†æå®Œæˆï¼šå‘ç°{len(table_entities)}ä¸ªå®ä½“ï¼Œ{len(table_relations)}ä¸ªå…³ç³»")
            
            # ä»å›¾åƒä¸­æå–å®ä½“
            if image_chunks:
                self._update_processing_status(file_id, "processing", 80, 
                                             f"ğŸ–¼ï¸ æ­£åœ¨ä»{len(image_chunks)}ä¸ªå›¾åƒè¯†åˆ«å®ä½“...")
                image_entities = self._extract_entities_from_images(image_chunks)
                all_entities.extend(image_entities)
                self._update_processing_status(file_id, "processing", 81, 
                                             f"ğŸ–¼ï¸ å›¾åƒåˆ†æå®Œæˆï¼šè¯†åˆ«å‡º{len(image_entities)}ä¸ªå®ä½“")
            
            # ä»å›¾è¡¨ä¸­æå–å®ä½“å’Œå…³ç³»
            if chart_chunks:
                self._update_processing_status(file_id, "processing", 82, 
                                             f"ğŸ“ˆ æ­£åœ¨ä»{len(chart_chunks)}ä¸ªå›¾è¡¨æå–å®ä½“å’Œå…³ç³»...")
                chart_entities, chart_relations = self._extract_entities_relations_from_charts(chart_chunks)
                all_entities.extend(chart_entities)
                all_relations.extend(chart_relations)
                self._update_processing_status(file_id, "processing", 83, 
                                             f"ğŸ“ˆ å›¾è¡¨åˆ†æå®Œæˆï¼šå‘ç°{len(chart_entities)}ä¸ªå®ä½“ï¼Œ{len(chart_relations)}ä¸ªå…³ç³»")
            
            # å®ä½“å»é‡å’Œåˆå¹¶
            self._update_processing_status(file_id, "processing", 84, 
                                         f"ğŸ”— æ­£åœ¨æ•´ç†å®ä½“å»é‡ï¼ŒåŸå§‹å‘ç°{len(all_entities)}ä¸ªå®ä½“...")
            deduplicated_entities = self._deduplicate_entities(all_entities)
            
            # å…³ç³»ä¼˜åŒ–
            self._update_processing_status(file_id, "processing", 85, 
                                         f"ğŸ”— æ­£åœ¨ä¼˜åŒ–å…³ç³»è¿æ¥ï¼ŒåŸå§‹å‘ç°{len(all_relations)}ä¸ªå…³ç³»...")
            optimized_relations = self._optimize_relations(all_relations, deduplicated_entities)
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼š{len(deduplicated_entities)}ä¸ªå®ä½“ï¼Œ{len(optimized_relations)}ä¸ªå…³ç³»")
            
            return {
                "entities": deduplicated_entities,
                "relations": optimized_relations,
                "entities_count": len(deduplicated_entities),
                "relations_count": len(optimized_relations)
            }
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {e}")
            return {
                "entities": [],
                "relations": [],
                "entities_count": 0,
                "relations_count": 0
            }
    
    def _extract_entities_relations_from_text(self, text_chunks: List[Dict[str, Any]]) -> Tuple[List[Dict], List[Dict]]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»"""
        entities = []
        relations = []
        
        try:
            # æ‰¹é‡å¤„ç†æ–‡æœ¬å—
            batch_size = self.graphrag_config.get("batch_processing", {}).get("default_batch_size", 2)
            
            for i in range(0, len(text_chunks), batch_size):
                batch = text_chunks[i:i + batch_size]
                combined_text = "\n\n".join([chunk["content"] for chunk in batch])
                
                # æå–å®ä½“
                batch_entities = self._extract_entities_from_text(combined_text)
                
                # æå–å…³ç³»
                batch_relations = self._extract_relations_from_text(combined_text, batch_entities)
                
                # ä¸ºå®ä½“å’Œå…³ç³»æ·»åŠ æ¥æºä¿¡æ¯
                for entity in batch_entities:
                    entity["source_chunks"] = [chunk["chunk_id"] for chunk in batch]
                    entity["source_type"] = "text"
                
                for relation in batch_relations:
                    relation["source_chunks"] = [chunk["chunk_id"] for chunk in batch]
                    relation["source_type"] = "text"
                
                entities.extend(batch_entities)
                relations.extend(batch_relations)
            
            return entities, relations
            
        except Exception as e:
            logger.error(f"ä»æ–‡æœ¬æå–å®ä½“å…³ç³»å¤±è´¥: {e}")
            return [], []
    
    def _extract_entities_from_text(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        try:
            if "entity_extraction" not in self.prompt_config.get("document_parsing", {}):
                logger.warning("å®ä½“æå–æç¤ºè¯æœªé…ç½®")
                return []
            
            prompt_template = self.prompt_config["document_parsing"]["entity_extraction"]
            prompt = prompt_template.format(text=text[:2000])  # é™åˆ¶æ–‡æœ¬é•¿åº¦
            
            response = self._call_llm(prompt)
            
            # è§£æå“åº”
            entities = self._parse_entities_response(response)
            
            return entities
            
        except Exception as e:
            logger.error(f"å®ä½“æå–å¤±è´¥: {e}")
            return []
    
    def _extract_relations_from_text(self, text: str, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–å…³ç³»"""
        try:
            if not entities or "relation_extraction" not in self.prompt_config.get("document_parsing", {}):
                return []
            
            entities_str = "\n".join([f"- {entity['name']} ({entity['type']})" for entity in entities[:10]])
            prompt_template = self.prompt_config["document_parsing"]["relation_extraction"]
            prompt = prompt_template.format(text=text[:2000], entities=entities_str)
            
            response = self._call_llm(prompt)
            
            # è§£æå“åº”
            relations = self._parse_relations_response(response)
            
            return relations
            
        except Exception as e:
            logger.error(f"å…³ç³»æå–å¤±è´¥: {e}")
            return []
    
    def _extract_entities_relations_from_tables(self, table_chunks: List[Dict[str, Any]]) -> Tuple[List[Dict], List[Dict]]:
        """ä»è¡¨æ ¼ä¸­æå–å®ä½“å’Œå…³ç³»"""
        entities = []
        relations = []
        
        try:
            for table_chunk in table_chunks:
                table_data = table_chunk.get("table_data", [])
                if not table_data or len(table_data) < 2:
                    continue
                
                # ä»è¡¨æ ¼å¤´éƒ¨æå–å®ä½“ï¼ˆå­—æ®µåï¼‰
                headers = table_data[0]
                for header in headers:
                    if header and str(header).strip():
                        entities.append({
                            "entity_id": str(uuid.uuid4()),
                            "name": str(header).strip(),
                            "type": "TABLE_FIELD",
                            "confidence": 0.9,
                            "source_chunks": [table_chunk["chunk_id"]],
                            "source_type": "table"
                        })
                
                # ä»è¡¨æ ¼æ•°æ®ä¸­æå–å®ä½“ï¼ˆæ•°æ®å€¼ï¼‰
                for row_idx, row in enumerate(table_data[1:6]):  # é™åˆ¶å¤„ç†è¡Œæ•°
                    for col_idx, cell in enumerate(row):
                        if cell and str(cell).strip() and not self._is_number(str(cell)):
                            entities.append({
                                "entity_id": str(uuid.uuid4()),
                                "name": str(cell).strip(),
                                "type": "TABLE_VALUE",
                                "confidence": 0.7,
                                "source_chunks": [table_chunk["chunk_id"]],
                                "source_type": "table",
                                "table_position": {"row": row_idx + 1, "col": col_idx}
                            })
                
                # åˆ›å»ºå­—æ®µä¹‹é—´çš„å…³ç³»
                for i in range(len(headers) - 1):
                    relations.append({
                        "relationship_id": str(uuid.uuid4()),
                        "subject": headers[i],
                        "predicate": "RELATED_FIELD",
                        "object": headers[i + 1],
                        "confidence": 0.6,
                        "source_chunks": [table_chunk["chunk_id"]],
                        "source_type": "table"
                    })
            
            return entities, relations
            
        except Exception as e:
            logger.error(f"ä»è¡¨æ ¼æå–å®ä½“å…³ç³»å¤±è´¥: {e}")
            return [], []
    
    def _extract_entities_from_images(self, image_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä»å›¾åƒä¸­æå–å®ä½“"""
        entities = []
        
        try:
            for image_chunk in image_chunks:
                # ä»å›¾åƒçš„OCRæ–‡æœ¬ä¸­æå–å®ä½“
                text_content = image_chunk.get("metadata", {}).get("text_content", "")
                if text_content:
                    text_entities = self._extract_entities_from_text(text_content)
                    for entity in text_entities:
                        entity["source_chunks"] = [image_chunk["chunk_id"]]
                        entity["source_type"] = "image"
                    entities.extend(text_entities)
                
                # ä»æ£€æµ‹åˆ°çš„å¯¹è±¡ä¸­åˆ›å»ºå®ä½“
                objects_detected = image_chunk.get("metadata", {}).get("objects_detected", [])
                for obj in objects_detected:
                    if isinstance(obj, str) and obj.strip():
                        entities.append({
                            "entity_id": str(uuid.uuid4()),
                            "name": obj.strip(),
                            "type": "IMAGE_OBJECT",
                            "confidence": 0.8,
                            "source_chunks": [image_chunk["chunk_id"]],
                            "source_type": "image"
                        })
            
            return entities
            
        except Exception as e:
            logger.error(f"ä»å›¾åƒæå–å®ä½“å¤±è´¥: {e}")
            return []
    
    def _extract_entities_relations_from_charts(self, chart_chunks: List[Dict[str, Any]]) -> Tuple[List[Dict], List[Dict]]:
        """ä»å›¾è¡¨ä¸­æå–å®ä½“å’Œå…³ç³»"""
        entities = []
        relations = []
        
        try:
            for chart_chunk in chart_chunks:
                metadata = chart_chunk.get("metadata", {})
                
                # ä»è½´æ ‡ç­¾æå–å®ä½“
                axis_labels = metadata.get("axis_labels", {})
                for axis, label in axis_labels.items():
                    if label and str(label).strip():
                        entities.append({
                            "entity_id": str(uuid.uuid4()),
                            "name": str(label).strip(),
                            "type": "CHART_AXIS",
                            "confidence": 0.9,
                            "source_chunks": [chart_chunk["chunk_id"]],
                            "source_type": "chart"
                        })
                
                # ä»å›¾ä¾‹ä¿¡æ¯æå–å®ä½“
                legend_info = metadata.get("legend_info", [])
                for legend_item in legend_info:
                    if isinstance(legend_item, str) and legend_item.strip():
                        entities.append({
                            "entity_id": str(uuid.uuid4()),
                            "name": legend_item.strip(),
                            "type": "CHART_SERIES",
                            "confidence": 0.8,
                            "source_chunks": [chart_chunk["chunk_id"]],
                            "source_type": "chart"
                        })
                
                # åˆ›å»ºå›¾è¡¨ç±»å‹ä¸æ•°æ®çš„å…³ç³»
                chart_type = metadata.get("chart_type", "")
                if chart_type:
                    for entity in entities[-len(legend_info):]:  # å¯¹æœ€è¿‘æ·»åŠ çš„å›¾ä¾‹å®ä½“
                        relations.append({
                            "relationship_id": str(uuid.uuid4()),
                            "subject": entity["name"],
                            "predicate": "DISPLAYED_IN",
                            "object": chart_type,
                            "confidence": 0.8,
                            "source_chunks": [chart_chunk["chunk_id"]],
                            "source_type": "chart"
                        })
            
            return entities, relations
            
        except Exception as e:
            logger.error(f"ä»å›¾è¡¨æå–å®ä½“å…³ç³»å¤±è´¥: {e}")
            return [], []
    
    def _deduplicate_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å®ä½“å»é‡å’Œåˆå¹¶"""
        try:
            if not entities:
                return []
            
            # æŒ‰åç§°åˆ†ç»„
            entity_groups = {}
            for entity in entities:
                name = entity["name"].lower().strip()
                if name not in entity_groups:
                    entity_groups[name] = []
                entity_groups[name].append(entity)
            
            deduplicated = []
            similarity_threshold = self.graphrag_config.get("knowledge_graph", {}).get("similarity_threshold", 0.85)
            
            for name, group in entity_groups.items():
                if len(group) == 1:
                    deduplicated.append(group[0])
                else:
                    # åˆå¹¶ç›¸ä¼¼å®ä½“
                    merged_entity = self._merge_entities(group)
                    deduplicated.append(merged_entity)
            
            return deduplicated
            
        except Exception as e:
            logger.error(f"å®ä½“å»é‡å¤±è´¥: {e}")
            return entities
    
    def _merge_entities(self, entity_group: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶ç›¸ä¼¼å®ä½“"""
        try:
            # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„å®ä½“ä½œä¸ºåŸºç¡€
            base_entity = max(entity_group, key=lambda x: x.get("confidence", 0))
            
            # åˆå¹¶æ¥æºä¿¡æ¯
            all_source_chunks = []
            for entity in entity_group:
                all_source_chunks.extend(entity.get("source_chunks", []))
            
            base_entity["source_chunks"] = list(set(all_source_chunks))
            base_entity["merged_count"] = len(entity_group)
            
            return base_entity
            
        except Exception as e:
            logger.error(f"å®ä½“åˆå¹¶å¤±è´¥: {e}")
            return entity_group[0] if entity_group else {}
    
    def _optimize_relations(self, relations: List[Dict[str, Any]], entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–å…³ç³»"""
        try:
            # åˆ›å»ºå®ä½“åç§°åˆ°IDçš„æ˜ å°„
            entity_name_to_id = {entity["name"]: entity["entity_id"] for entity in entities}
            
            # è¿‡æ»¤å’Œä¼˜åŒ–å…³ç³»
            optimized = []
            seen_relations = set()
            
            for relation in relations:
                subject = relation.get("subject", "")
                object_name = relation.get("object", "")
                predicate = relation.get("predicate", "")
                
                # æ£€æŸ¥å®ä½“æ˜¯å¦å­˜åœ¨
                if subject in entity_name_to_id and object_name in entity_name_to_id:
                    relation_key = f"{subject}_{predicate}_{object_name}"
                    if relation_key not in seen_relations:
                        seen_relations.add(relation_key)
                        optimized.append(relation)
            
            return optimized
            
        except Exception as e:
            logger.error(f"å…³ç³»ä¼˜åŒ–å¤±è´¥: {e}")
            return relations
    
    def _save_knowledge_graph_to_db(self, entities: List[Dict[str, Any]], relations: List[Dict[str, Any]], file_id: str) -> None:
        """ä¿å­˜çŸ¥è¯†å›¾è°±åˆ°æ•°æ®åº“"""
        try:
            # ä¿å­˜å®ä½“åˆ°Neo4j
            for entity in entities:
                entity_data = {
                    "entity_id": entity["entity_id"],
                    "name": entity["name"],
                    "type": entity["type"],
                    "file_id": file_id,
                    "confidence": entity["confidence"],
                    "source_type": entity.get("source_type", "unknown")
                }
                neo4j_manager.create_entity(entity["type"], entity_data)
            
            # ä¿å­˜å…³ç³»åˆ°Neo4j
            for relation in relations:
                subject_entity = {"name": relation["subject"]}
                object_entity = {"name": relation["object"]}
                relation_props = {
                    "confidence": relation["confidence"],
                    "file_id": file_id,
                    "source_type": relation.get("source_type", "unknown")
                }
                
                neo4j_manager.create_relationship(
                    subject_entity,
                    object_entity,
                    relation["predicate"],
                    relation_props
                )
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±ä¿å­˜å®Œæˆï¼š{len(entities)}ä¸ªå®ä½“ï¼Œ{len(relations)}ä¸ªå…³ç³»")
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±ä¿å­˜å¤±è´¥: {e}")
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹"""
        try:
            llm_config = self.model_config.get("llm", {})
            
            headers = {
                "Authorization": f"Bearer {llm_config['api_key']}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": llm_config["model_name"],
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": min(llm_config.get("max_tokens", 2048), 2048),
                "temperature": llm_config.get("temperature", 0.7)
            }
            
            response = requests.post(
                f"{llm_config['api_url']}/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                return content
            else:
                logger.error(f"LLM APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}")
                return '{"entities": [], "relations": []}'
                
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return '{"entities": [], "relations": []}'
    
    def _parse_entities_response(self, response: str) -> List[Dict[str, Any]]:
        """è§£æå®ä½“æå–å“åº”"""
        try:
            # æ¸…ç†å“åº”
            cleaned_response = response.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]
            elif cleaned_response.startswith('```'):
                cleaned_response = cleaned_response[3:]
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]
            
            # æå–JSON
            start_idx = cleaned_response.find('{')
            end_idx = cleaned_response.rfind('}')
            
            if start_idx != -1 and end_idx != -1:
                json_str = cleaned_response[start_idx:end_idx+1]
                result = json.loads(json_str)
                
                entities = result.get("entities", [])
                standardized = []
                
                for entity in entities:
                    if isinstance(entity, dict) and entity.get("name"):
                        standardized.append({
                            "entity_id": str(uuid.uuid4()),
                            "name": entity.get("name", "").strip(),
                            "type": entity.get("type", "UNKNOWN").strip(),
                            "confidence": 0.8
                        })
                
                return standardized
            
            return []
            
        except Exception as e:
            logger.error(f"å®ä½“å“åº”è§£æå¤±è´¥: {e}")
            return []
    
    def _parse_relations_response(self, response: str) -> List[Dict[str, Any]]:
        """è§£æå…³ç³»æå–å“åº”"""
        try:
            # æ¸…ç†å“åº”
            cleaned_response = response.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]
            elif cleaned_response.startswith('```'):
                cleaned_response = cleaned_response[3:]
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]
            
            # æå–JSON
            start_idx = cleaned_response.find('{')
            end_idx = cleaned_response.rfind('}')
            
            if start_idx != -1 and end_idx != -1:
                json_str = cleaned_response[start_idx:end_idx+1]
                result = json.loads(json_str)
                
                relations = result.get("relations", [])
                standardized = []
                
                for relation in relations:
                    if isinstance(relation, dict) and relation.get("subject") and relation.get("object"):
                        standardized.append({
                            "relationship_id": str(uuid.uuid4()),
                            "subject": relation.get("subject", "").strip(),
                            "predicate": relation.get("predicate", "RELATED_TO").strip(),
                            "object": relation.get("object", "").strip(),
                            "confidence": relation.get("confidence", 0.8)
                        })
                
                return standardized
            
            return []
            
        except Exception as e:
            logger.error(f"å…³ç³»å“åº”è§£æå¤±è´¥: {e}")
            return []
    
    def _update_processing_status(self, file_id: str, status: str, progress: int, message: str) -> None:
        """æ›´æ–°å¤„ç†çŠ¶æ€"""
        try:
            self.processing_status[file_id] = {
                "status": status,
                "progress": progress,
                "message": message,
                "updated_at": datetime.now()
            }
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            mysql_manager.execute_update(
                "UPDATE files SET status = %s, processing_progress = %s WHERE file_id = %s",
                (status, progress, file_id)
            )
            
            logger.info(f"ğŸ“Š {file_id}: {status} - {progress}% - {message}")
            
        except Exception as e:
            logger.warning(f"æ›´æ–°å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
    
    def get_processing_status(self, file_id: str) -> Dict[str, Any]:
        """è·å–å¤„ç†çŠ¶æ€"""
        return self.processing_status.get(file_id, {
            "status": "unknown",
            "progress": 0,
            "message": "çŠ¶æ€æœªçŸ¥"
        })

# å…¨å±€GraphRAGæœåŠ¡å®ä¾‹
graphrag_service = GraphRAGService()