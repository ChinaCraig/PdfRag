"""
æ™ºèƒ½æ£€ç´¢æœåŠ¡ - å¤šæ¨¡æ€ç‰ˆ
å®ç°åŸºäºGraphRAGçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒæ–‡å­—ã€å›¾ç‰‡ã€è¡¨æ ¼ã€å›¾è¡¨çš„å¤šæ¨¡æ€å†…å®¹å±•ç¤º
"""
import logging
import json
import requests
import os
import base64
from typing import Dict, List, Any, Optional, Generator
from datetime import datetime

from utils.config_loader import config_loader
from utils.database import mysql_manager, milvus_manager, neo4j_manager
from utils.model_manager import model_manager

logger = logging.getLogger(__name__)

class SearchService:
    """æ™ºèƒ½æ£€ç´¢æœåŠ¡ç±» - å¤šæ¨¡æ€ç‰ˆ"""
    
    def __init__(self):
        self.config = config_loader.get_app_config()
        self.model_config = config_loader.get_model_config()
        self.prompt_config = config_loader.get_prompt_config()
        self.multimedia_config = self.config.get("multimedia", {})
        self.conversation_history = {}  # å­˜å‚¨å¯¹è¯å†å²
        
        logger.info("æ™ºèƒ½æ£€ç´¢æœåŠ¡åˆå§‹åŒ–å®Œæˆ - å¤šæ¨¡æ€ç‰ˆ")
    
    def search(self, query: str, session_id: str = None, stream: bool = False) -> Dict[str, Any]:
        """
        æ™ºèƒ½æ£€ç´¢ä¸»å‡½æ•°
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            session_id: ä¼šè¯IDï¼Œç”¨äºå¤šè½®å¯¹è¯
            stream: æ˜¯å¦æµå¼è¿”å›
            
        Returns:
            æ£€ç´¢ç»“æœ
        """
        try:
            # è·å–æˆ–åˆ›å»ºä¼šè¯
            if not session_id:
                session_id = self._create_session()
            
            # è®°å½•ç”¨æˆ·æŸ¥è¯¢
            self._add_to_conversation(session_id, "user", query)
            
            if stream:
                return self._stream_search(query, session_id)
            else:
                return self._direct_search(query, session_id)
                
        except Exception as e:
            logger.error(f"æ™ºèƒ½æ£€ç´¢å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ£€ç´¢å¤±è´¥: {str(e)}",
                "session_id": session_id
            }
    
    def _direct_search(self, query: str, session_id: str) -> Dict[str, Any]:
        """éæµå¼æ£€ç´¢"""
        # å‘é‡æ£€ç´¢
        vector_results = self._vector_search(query)
        
        # å›¾æ£€ç´¢
        graph_results = self._graph_search(query)
        
        # èåˆæ£€ç´¢ç»“æœ
        combined_results = self._combine_search_results(vector_results, graph_results)
        
        # ç”Ÿæˆå›ç­”
        answer = self._generate_answer(query, combined_results, session_id)
        
        # è®°å½•AIå›ç­”
        self._add_to_conversation(session_id, "assistant", answer)
        
        return {
            "success": True,
            "answer": answer,
            "sources": combined_results,
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        }
    
    def _stream_search(self, query: str, session_id: str) -> Generator[str, None, None]:
        """æµå¼æ£€ç´¢ - å¸¦æ€è€ƒè¿‡ç¨‹å¯è§†åŒ–"""
        import json
        
        # å‘é€æ€è€ƒå¼€å§‹ä¿¡å·
        yield json.dumps({
            "type": "thinking_start",
            "stage": "analyzing_query",
            "message": "æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜...",
            "progress": 0
        }) + "\n"
        
        # å®ä½“æå–é˜¶æ®µ
        yield json.dumps({
            "type": "thinking_update", 
            "stage": "extracting_entities",
            "message": "æ­£åœ¨æå–é—®é¢˜ä¸­çš„å…³é”®å®ä½“...",
            "progress": 10
        }) + "\n"
        
        # å‘é‡æ£€ç´¢é˜¶æ®µ
        yield json.dumps({
            "type": "thinking_update",
            "stage": "vector_search", 
            "message": "æ­£åœ¨è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢...",
            "progress": 25
        }) + "\n"
        vector_results = self._vector_search(query)
        
        yield json.dumps({
            "type": "thinking_update",
            "stage": "vector_search_complete",
            "message": f"æ‰¾åˆ° {len(vector_results)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ",
            "progress": 40,
            "data": {"vector_count": len(vector_results)}
        }) + "\n"
        
        # å›¾æ£€ç´¢é˜¶æ®µ
        yield json.dumps({
            "type": "thinking_update",
            "stage": "graph_search",
            "message": "æ­£åœ¨æœç´¢çŸ¥è¯†å›¾è°±ä¸­çš„ç›¸å…³ä¿¡æ¯...",
            "progress": 55
        }) + "\n"
        graph_results = self._graph_search(query)
        
        yield json.dumps({
            "type": "thinking_update", 
            "stage": "graph_search_complete",
            "message": f"å‘ç° {len(graph_results)} ä¸ªç›¸å…³çš„çŸ¥è¯†å…³è”",
            "progress": 70,
            "data": {"graph_count": len(graph_results)}
        }) + "\n"
        
        # ç»“æœèåˆé˜¶æ®µ
        yield json.dumps({
            "type": "thinking_update",
            "stage": "combining_results",
            "message": "æ­£åœ¨èåˆå¤šæºæ£€ç´¢ç»“æœ...",
            "progress": 80
        }) + "\n"
        combined_results = self._combine_search_results(vector_results, graph_results)
        
        # åˆ†æå¤šæ¨¡æ€å†…å®¹
        multimedia_count = sum(1 for r in combined_results if r.get("content_type") != "text")
        yield json.dumps({
            "type": "thinking_update",
            "stage": "analyzing_content",
            "message": f"æ­£åœ¨åˆ†æå†…å®¹ï¼ŒåŒ…å« {multimedia_count} ä¸ªå¤šåª’ä½“å…ƒç´ ...",
            "progress": 90,
            "data": {
                "total_results": len(combined_results),
                "multimedia_count": multimedia_count,
                "content_types": list(set(r.get("content_type", "text") for r in combined_results))
            }
        }) + "\n"
        
        # æ€è€ƒå®Œæˆï¼Œå¼€å§‹ç”Ÿæˆ
        yield json.dumps({
            "type": "thinking_complete",
            "stage": "generating_answer",
            "message": "æ€è€ƒå®Œæˆï¼Œæ­£åœ¨ä¸ºæ‚¨ç”Ÿæˆè¯¦ç»†å›ç­”...",
            "progress": 100
        }) + "\n"
        
        # ç”ŸæˆåŒ…å«å¤šåª’ä½“å ä½ç¬¦çš„å®Œæ•´å›ç­”
        yield json.dumps({
            "type": "answer_start",
            "message": "æ­£åœ¨ç”Ÿæˆå®Œæ•´å›ç­”..."
        }) + "\n"
        
        # ç”Ÿæˆå¸¦æœ‰å ä½ç¬¦çš„å®Œæ•´å›ç­”
        full_structured_answer = self._generate_unified_answer_with_multimedia(query, combined_results, session_id)
        
        # ä¸€æ¬¡æ€§å‘é€å®Œæ•´çš„ç»“æ„åŒ–å›ç­”
        yield json.dumps({
            "type": "answer_complete",
            "content": full_structured_answer
        }) + "\n"
        
        # è®°å½•å®Œæ•´å›ç­”ï¼ˆæå–æ–‡æœ¬å†…å®¹ï¼‰
        answer_text = full_structured_answer.get("text_content", "")
        self._add_to_conversation(session_id, "assistant", answer_text)
    
    def _vector_search(self, query: str) -> List[Dict[str, Any]]:
        """å‘é‡æ£€ç´¢"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ–‡æ¡£æ•°æ®
            if not self._has_vector_data():
                logger.info("æš‚æ— å‘é‡æ•°æ®ï¼Œè·³è¿‡å‘é‡æ£€ç´¢")
                return []
            
            # è·å–æŸ¥è¯¢çš„åµŒå…¥å‘é‡
            query_embedding = model_manager.get_embedding([query])[0]
            
            # åœ¨Milvusä¸­æœç´¢ç›¸ä¼¼å‘é‡
            search_config = self.config["vector_search"]
            results = milvus_manager.search_vectors(
                query_embedding,
                top_k=search_config["top_k"]
            )
            
            # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
            threshold = search_config["similarity_threshold"]
            filtered_results = [
                result for result in results 
                if result["score"] >= threshold
            ]
            
            logger.info(f"å‘é‡æ£€ç´¢æ‰¾åˆ° {len(filtered_results)} ä¸ªç›¸å…³ç»“æœ")
            return filtered_results
            
        except Exception as e:
            logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _has_vector_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å‘é‡æ•°æ®"""
        try:
            # æ£€æŸ¥Milvusé›†åˆæ˜¯å¦æœ‰æ•°æ®
            return milvus_manager.has_data()
        except Exception:
            return False
    
    def _graph_search(self, query: str) -> List[Dict[str, Any]]:
        """å›¾æ£€ç´¢"""
        try:
            # æå–æŸ¥è¯¢ä¸­çš„å®ä½“
            entities = self._extract_query_entities(query)
            
            if not entities:
                return []
            
            # åœ¨Neo4jä¸­æœç´¢ç›¸å…³çš„å›¾ç»“æ„
            graph_config = self.config["graph_search"]
            results = []
            
            for entity in entities:
                # æœç´¢å®ä½“ç›¸å…³çš„è·¯å¾„
                # Neo4jè¯­æ³•ï¼šè·¯å¾„é•¿åº¦éœ€è¦æ˜¯å…·ä½“æ•°å­—ï¼Œä¸èƒ½ä½¿ç”¨å‚æ•°
                max_hops = graph_config["max_hops"]
                cypher_query = f"""
                MATCH path = (n {{name: $entity_name}})-[*1..{max_hops}]-(m)
                RETURN path, n, m
                LIMIT $max_paths
                """
                
                graph_results = neo4j_manager.execute_query(
                    cypher_query,
                    {
                        "entity_name": entity,
                        "max_paths": graph_config["max_paths"]
                    }
                )
                
                results.extend(graph_results)
            
            logger.info(f"å›¾æ£€ç´¢æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³è·¯å¾„")
            return results
            
        except Exception as e:
            logger.error(f"å›¾æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _extract_query_entities(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å®ä½“"""
        # è¿™é‡Œéœ€è¦å®ç°å®ä½“è¯†åˆ«
        # å¯ä»¥ä½¿ç”¨NERæ¨¡å‹æˆ–è°ƒç”¨LLM
        
        # æš‚æ—¶ä½¿ç”¨ç®€å•çš„å…³é”®è¯æå–
        # å®é™…å®ç°åº”è¯¥è°ƒç”¨LLMè¿›è¡Œå®ä½“è¯†åˆ«
        entities = []
        
        # ç®€å•çš„å®ä½“è¯†åˆ«é€»è¾‘ï¼ˆç¤ºä¾‹ï¼‰
        # å®é™…åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„NERæ–¹æ³•
        words = query.split()
        for word in words:
            if len(word) > 2:  # ç®€å•è¿‡æ»¤
                entities.append(word)
        
        return entities[:3]  # é™åˆ¶å®ä½“æ•°é‡
    
    def _combine_search_results(self, vector_results: List[Dict], graph_results: List[Dict]) -> List[Dict[str, Any]]:
        """èåˆæ£€ç´¢ç»“æœ - å¤šæ¨¡æ€ç‰ˆ"""
        combined = []
        
        # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
        for result in vector_results:
            metadata = json.loads(result["metadata"]) if result["metadata"] else {}
            content_type = metadata.get("type", "text")
            
            # åŸºç¡€ç»“æœç»“æ„
            combined_item = {
                "type": "vector",
                "content_type": content_type,
                "content": result["content"],
                "file_id": result["file_id"],
                "chunk_id": result["chunk_id"],
                "score": result["score"],
                "metadata": metadata
            }
            
            # æ ¹æ®å†…å®¹ç±»å‹æ·»åŠ å¤šæ¨¡æ€ä¿¡æ¯
            if content_type == "image":
                combined_item.update(self._process_image_result(result, metadata))
            elif content_type == "table":
                combined_item.update(self._process_table_result(result, metadata))
            elif content_type == "chart":
                combined_item.update(self._process_chart_result(result, metadata))
            
            combined.append(combined_item)
        
        # å¤„ç†å›¾æ£€ç´¢ç»“æœ
        for result in graph_results:
            combined.append({
                "type": "graph",
                "content_type": "graph",
                "path": result,
                "score": 0.8  # å›¾ç»“æœé»˜è®¤åˆ†æ•°
            })
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œä½†ä¼˜å…ˆå±•ç¤ºå¤šæ¨¡æ€å†…å®¹
        combined.sort(key=lambda x: (x.get("content_type") != "text", x["score"]), reverse=True)
        
        return combined[:15]  # å¢åŠ ç»“æœæ•°é‡ä»¥å®¹çº³å¤šæ¨¡æ€å†…å®¹
    
    def _generate_answer(self, query: str, search_results: List[Dict], session_id: str) -> str:
        """ç”Ÿæˆå›ç­”"""
        try:
            # å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = self._prepare_context(search_results)
            
            # è·å–å¯¹è¯å†å²
            history = self._get_conversation_history(session_id)
            
            # é€‰æ‹©åˆé€‚çš„æç¤ºè¯æ¨¡æ¿
            if history:
                prompt_template = self.prompt_config["intelligent_search"]["conversation_context"]
                prompt = prompt_template.format(
                    conversation_history=self._format_conversation_history(history),
                    current_question=query,
                    document_info=context_info
                )
            else:
                prompt_template = self.prompt_config["intelligent_search"]["result_integration"]
                prompt = prompt_template.format(
                    question=query,
                    retrieved_info=context_info
                )
            
            # è°ƒç”¨LLMç”Ÿæˆå›ç­”
            answer = self._call_llm(prompt)
            
            return answer
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def _generate_streaming_answer(self, query: str, search_results: List[Dict], session_id: str) -> Generator[str, None, None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        try:
            # å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = self._prepare_context(search_results)
            
            # è·å–å¯¹è¯å†å²
            history = self._get_conversation_history(session_id)
            
            # å‡†å¤‡æç¤ºè¯
            if history:
                prompt_template = self.prompt_config["intelligent_search"]["conversation_context"]
                prompt = prompt_template.format(
                    conversation_history=self._format_conversation_history(history),
                    current_question=query,
                    document_info=context_info
                )
            else:
                prompt_template = self.prompt_config["intelligent_search"]["result_integration"]
                prompt = prompt_template.format(
                    question=query,
                    retrieved_info=context_info
                )
            
            # æµå¼è°ƒç”¨LLM
            for chunk in self._call_llm_stream(prompt):
                yield chunk
                
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def _prepare_context(self, search_results: List[Dict]) -> str:
        """å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context_parts = []
        
        for i, result in enumerate(search_results, 1):
            if result["type"] == "vector":
                context_parts.append(f"æ–‡æ¡£ç‰‡æ®µ {i}ï¼š\n{result['content']}\n")
            elif result["type"] == "graph":
                # å¤„ç†å›¾ç»“æ„ä¿¡æ¯
                context_parts.append(f"å…³ç³»ä¿¡æ¯ {i}ï¼š\n{self._format_graph_result(result)}\n")
        
        return "\n".join(context_parts)
    
    def _format_graph_result(self, graph_result: Dict) -> str:
        """æ ¼å¼åŒ–å›¾ç»“æœ"""
        # ç®€åŒ–çš„å›¾ç»“æœæ ¼å¼åŒ–
        return "ç›¸å…³å®ä½“å…³ç³»ä¿¡æ¯"
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLMç”Ÿæˆæ–‡æœ¬"""
        try:
            llm_config = self.model_config["llm"]
            
            headers = {
                "Authorization": f"Bearer {llm_config['api_key']}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": llm_config["model_name"],
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": llm_config["max_tokens"],
                "temperature": llm_config["temperature"]
            }
            
            response = requests.post(
                f"{llm_config['api_url']}/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                logger.error(f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
                
        except Exception as e:
            logger.error(f"è°ƒç”¨LLMå¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
    
    def _call_llm_stream(self, prompt: str) -> Generator[str, None, None]:
        """æµå¼è°ƒç”¨LLM"""
        try:
            llm_config = self.model_config["llm"]
            
            headers = {
                "Authorization": f"Bearer {llm_config['api_key']}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": llm_config["model_name"],
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": llm_config["max_tokens"],
                "temperature": llm_config["temperature"],
                "stream": True
            }
            
            response = requests.post(
                f"{llm_config['api_url']}/chat/completions",
                headers=headers,
                json=data,
                stream=True,
                timeout=30
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        line = line.decode('utf-8')
                        if line.startswith('data: '):
                            data_str = line[6:]
                            if data_str.strip() == '[DONE]':
                                break
                            try:
                                data = json.loads(data_str)
                                if 'choices' in data and len(data['choices']) > 0:
                                    delta = data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        yield delta['content']
                            except json.JSONDecodeError:
                                continue
            else:
                yield "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
                
        except Exception as e:
            logger.error(f"æµå¼è°ƒç”¨LLMå¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
    
    def _create_session(self) -> str:
        """åˆ›å»ºæ–°ä¼šè¯"""
        import uuid
        session_id = str(uuid.uuid4())
        self.conversation_history[session_id] = []
        return session_id
    
    def _add_to_conversation(self, session_id: str, role: str, content: str) -> None:
        """æ·»åŠ å¯¹è¯è®°å½•"""
        if session_id not in self.conversation_history:
            self.conversation_history[session_id] = []
        
        self.conversation_history[session_id].append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        })
        
        # é™åˆ¶å¯¹è¯å†å²é•¿åº¦
        max_history = 10
        if len(self.conversation_history[session_id]) > max_history:
            self.conversation_history[session_id] = self.conversation_history[session_id][-max_history:]
    
    def _get_conversation_history(self, session_id: str) -> List[Dict]:
        """è·å–å¯¹è¯å†å²"""
        return self.conversation_history.get(session_id, [])
    
    def _format_conversation_history(self, history: List[Dict]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        formatted = []
        for item in history:
            role = "ç”¨æˆ·" if item["role"] == "user" else "åŠ©æ‰‹"
            formatted.append(f"{role}: {item['content']}")
        
        return "\n".join(formatted)
    
    def get_conversation_history(self, session_id: str) -> List[Dict]:
        """è·å–ä¼šè¯çš„å¯¹è¯å†å²"""
        return self._get_conversation_history(session_id)
    
    def clear_conversation(self, session_id: str) -> bool:
        """æ¸…ç©ºä¼šè¯å†å²"""
        if session_id in self.conversation_history:
            del self.conversation_history[session_id]
            return True
        return False
    
    def get_search_suggestions(self, query: str) -> List[str]:
        """è·å–æœç´¢å»ºè®®"""
        # åŸºäºæŸ¥è¯¢å†…å®¹å’Œç°æœ‰æ–‡æ¡£æä¾›æœç´¢å»ºè®®
        suggestions = []
        
        try:
            # å¯ä»¥åŸºäºæ–‡æ¡£å†…å®¹æˆ–å¸¸è§æŸ¥è¯¢æ¨¡å¼ç”Ÿæˆå»ºè®®
            # è¿™é‡Œæä¾›ä¸€äº›åŸºç¡€å»ºè®®
            if "è¡¨æ ¼" in query or "æ•°æ®" in query:
                suggestions.extend([
                    "æ˜¾ç¤ºç›¸å…³çš„æ•°æ®è¡¨æ ¼",
                    "åˆ†æè¡¨æ ¼ä¸­çš„è¶‹åŠ¿",
                    "æ¯”è¾ƒä¸åŒæ•°æ®é¡¹"
                ])
            
            if "å›¾" in query or "å›¾ç‰‡" in query:
                suggestions.extend([
                    "è§£é‡Šå›¾è¡¨å†…å®¹",
                    "æè¿°å›¾åƒä¿¡æ¯",
                    "åˆ†æè§†è§‰å…ƒç´ "
                ])
            
            # é€šç”¨å»ºè®®
            suggestions.extend([
                "æ€»ç»“æ–‡æ¡£ä¸»è¦å†…å®¹",
                "æå–å…³é”®ä¿¡æ¯",
                "æŸ¥æ‰¾ç›¸å…³ç« èŠ‚"
            ])
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæœç´¢å»ºè®®å¤±è´¥: {e}")
        
        return suggestions[:5]  # é™åˆ¶å»ºè®®æ•°é‡
    
    # ===== å¤šæ¨¡æ€å†…å®¹å¤„ç†æ–¹æ³• =====
    
    def _process_image_result(self, result: Dict, metadata: Dict) -> Dict[str, Any]:
        """å¤„ç†å›¾åƒæœç´¢ç»“æœ"""
        try:
            # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶è·¯å¾„
            file_id = result.get("file_id", "")
            chunk_id = result.get("chunk_id", "")
            
            logger.debug(f"æ­£åœ¨å¤„ç†å›¾ç‰‡: file_id={file_id}, chunk_id={chunk_id}")
            
            image_path = self._find_image_path(file_id, chunk_id)
            logger.debug(f"å›¾ç‰‡è·¯å¾„æŸ¥æ‰¾ç»“æœ: {image_path}")
            
            # ç¼–ç å›¾ç‰‡ä¸ºbase64
            image_base64 = None
            if image_path and os.path.exists(image_path):
                image_base64 = self._encode_image_to_base64(image_path)
                logger.debug(f"å›¾ç‰‡base64ç¼–ç {'æˆåŠŸ' if image_base64 else 'å¤±è´¥'}")
            else:
                logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                # ç”Ÿæˆå ä½å›¾ç‰‡
                image_base64 = self._generate_placeholder_image()
            
            # å›¾åƒå±•ç¤ºä¿¡æ¯
            image_info = {
                "image_path": image_path,
                "image_base64": image_base64,
                "image_description": result.get("content", "å›¾ç‰‡å†…å®¹"),
                "image_metadata": {
                    "width": metadata.get("width"),
                    "height": metadata.get("height"),
                    "format": metadata.get("format"),
                    "objects_detected": metadata.get("objects_detected", []),
                    "text_content": metadata.get("text_content", "")
                },
                "display_type": "image",
                "status": "loaded" if image_base64 else "failed"
            }
            
            return image_info
            
        except Exception as e:
            logger.error(f"å¤„ç†å›¾åƒç»“æœå¤±è´¥: {e}")
            return {
                "display_type": "image", 
                "error": str(e),
                "image_base64": self._generate_placeholder_image(),
                "status": "error"
            }
    
    def _process_table_result(self, result: Dict, metadata: Dict) -> Dict[str, Any]:
        """å¤„ç†è¡¨æ ¼æœç´¢ç»“æœ"""
        try:
            # æŸ¥æ‰¾è¡¨æ ¼æ•°æ®
            file_id = result.get("file_id", "")
            chunk_id = result.get("chunk_id", "")
            
            logger.debug(f"æ­£åœ¨å¤„ç†è¡¨æ ¼: file_id={file_id}, chunk_id={chunk_id}")
            
            table_data = self._find_table_data(file_id, chunk_id)
            
            # å¦‚æœæ‰¾ä¸åˆ°å®é™…è¡¨æ ¼æ•°æ®ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®
            if not table_data:
                logger.warning(f"è¡¨æ ¼æ•°æ®ä¸å­˜åœ¨ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®")
                table_data = self._generate_sample_table_data(result.get("content", ""))
            
            logger.debug(f"è¡¨æ ¼æ•°æ®åŠ è½½{'æˆåŠŸ' if table_data else 'å¤±è´¥'}: {len(table_data) if table_data else 0}è¡Œ")
            
            # è¡¨æ ¼å±•ç¤ºä¿¡æ¯
            table_info = {
                "table_data": table_data,
                "table_summary": result.get("content", "è¡¨æ ¼å†…å®¹"),
                "table_metadata": {
                    "rows": len(table_data) - 1 if table_data and len(table_data) > 1 else 0,
                    "columns": len(table_data[0]) if table_data and len(table_data) > 0 else 0,
                    "data_types": metadata.get("data_types", []),
                    "key_insights": metadata.get("key_insights", [])
                },
                "table_export_formats": self.multimedia_config.get("tables", {}).get("export_formats", ["csv"]),
                "display_type": "table",
                "status": "loaded" if table_data else "failed"
            }
            
            return table_info
            
        except Exception as e:
            logger.error(f"å¤„ç†è¡¨æ ¼ç»“æœå¤±è´¥: {e}")
            return {
                "display_type": "table", 
                "error": str(e),
                "table_data": self._generate_sample_table_data("é”™è¯¯ç¤ºä¾‹"),
                "status": "error"
            }
    
    def _process_chart_result(self, result: Dict, metadata: Dict) -> Dict[str, Any]:
        """å¤„ç†å›¾è¡¨æœç´¢ç»“æœ"""
        try:
            # æŸ¥æ‰¾å›¾è¡¨æ–‡ä»¶è·¯å¾„
            file_id = result.get("file_id", "")
            chunk_id = result.get("chunk_id", "")
            
            logger.debug(f"æ­£åœ¨å¤„ç†å›¾è¡¨: file_id={file_id}, chunk_id={chunk_id}")
            
            chart_path = self._find_chart_path(file_id, chunk_id)
            logger.debug(f"å›¾è¡¨è·¯å¾„æŸ¥æ‰¾ç»“æœ: {chart_path}")
            
            # ç¼–ç å›¾è¡¨ä¸ºbase64
            chart_base64 = None
            if chart_path and os.path.exists(chart_path):
                chart_base64 = self._encode_image_to_base64(chart_path)
                logger.debug(f"å›¾è¡¨base64ç¼–ç {'æˆåŠŸ' if chart_base64 else 'å¤±è´¥'}")
            else:
                logger.warning(f"å›¾è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {chart_path}")
                # ç”Ÿæˆå ä½å›¾è¡¨
                chart_base64 = self._generate_placeholder_chart()
            
            # å›¾è¡¨å±•ç¤ºä¿¡æ¯
            chart_info = {
                "chart_path": chart_path,
                "chart_base64": chart_base64,
                "chart_description": result.get("content", "å›¾è¡¨å†…å®¹"),
                "chart_metadata": {
                    "chart_type": metadata.get("chart_type", "æœªçŸ¥"),
                    "data_points": metadata.get("data_points", []),
                    "trend_analysis": metadata.get("trend_analysis", ""),
                    "statistical_summary": metadata.get("statistical_summary", {}),
                    "axis_labels": metadata.get("axis_labels", {}),
                    "legend_info": metadata.get("legend_info", [])
                },
                "display_type": "chart",
                "status": "loaded" if chart_base64 else "failed"
            }
            
            return chart_info
            
        except Exception as e:
            logger.error(f"å¤„ç†å›¾è¡¨ç»“æœå¤±è´¥: {e}")
            return {
                "display_type": "chart", 
                "error": str(e),
                "chart_base64": self._generate_placeholder_chart(),
                "status": "error"
            }
    
    def _find_image_path(self, file_id: str, chunk_id: str) -> Optional[str]:
        """æŸ¥æ‰¾å›¾åƒæ–‡ä»¶è·¯å¾„"""
        try:
            # æ”¹è¿›çš„chunk_idè§£æé€»è¾‘
            # chunk_idæ ¼å¼: file_id_page_pagenum_image_imgindex
            
            # ä½¿ç”¨file_idä½œä¸ºåˆ†å‰²åŸºå‡†ï¼Œæ›´åŠ ç¨³å®š
            if chunk_id.startswith(file_id + "_page_"):
                remaining = chunk_id[len(file_id + "_page_"):]
                parts = remaining.split("_")
                
                # æœŸæœ›æ ¼å¼: pagenum_image_imgindex
                if len(parts) >= 3 and parts[1] == "image":
                    page_num = parts[0]
                    img_index = parts[2]
                    
                    image_dir = self.multimedia_config.get("images", {}).get("save_dir", "uploads/images")
                    image_filename = f"{file_id}_page_{page_num}_image_{img_index}.png"
                    image_path = os.path.join(image_dir, image_filename)
                    
                    logger.debug(f"æŸ¥æ‰¾å›¾ç‰‡è·¯å¾„: {image_path}")
                    
                    if os.path.exists(image_path):
                        logger.debug(f"âœ… å›¾ç‰‡æ–‡ä»¶å­˜åœ¨: {image_path}")
                        return image_path
                    else:
                        logger.warning(f"âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            else:
                logger.warning(f"chunk_idæ ¼å¼ä¸åŒ¹é…: {chunk_id}, æœŸæœ›ä»¥ {file_id}_page_ å¼€å¤´")
            
            return None
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾å›¾åƒè·¯å¾„å¤±è´¥: {e}")
            return None
    
    def _find_table_data(self, file_id: str, chunk_id: str) -> Optional[List[List[str]]]:
        """æŸ¥æ‰¾è¡¨æ ¼æ•°æ®"""
        try:
            # æ”¹è¿›çš„chunk_idè§£æé€»è¾‘ï¼ˆä¸å›¾ç‰‡ä¿æŒä¸€è‡´ï¼‰
            table_dir = self.multimedia_config.get("tables", {}).get("export_dir", "uploads/tables")
            
            # ä½¿ç”¨file_idä½œä¸ºåˆ†å‰²åŸºå‡†
            if chunk_id.startswith(file_id + "_page_"):
                remaining = chunk_id[len(file_id + "_page_"):]
                parts = remaining.split("_")
                
                # æœŸæœ›æ ¼å¼: pagenum_table_tableindex
                if len(parts) >= 3 and parts[1] == "table":
                    page_num = parts[0]
                    table_index = parts[2]
                    
                    table_filename = f"{file_id}_page_{page_num}_table_{table_index}.csv"
                    table_path = os.path.join(table_dir, table_filename)
                
                if os.path.exists(table_path):
                    import csv
                    with open(table_path, 'r', encoding='utf-8') as f:
                        reader = csv.reader(f)
                        return list(reader)
            
            return None
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾è¡¨æ ¼æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _find_chart_path(self, file_id: str, chunk_id: str) -> Optional[str]:
        """æŸ¥æ‰¾å›¾è¡¨æ–‡ä»¶è·¯å¾„"""
        try:
            # æ”¹è¿›çš„chunk_idè§£æé€»è¾‘ï¼ˆä¸å›¾ç‰‡ä¿æŒä¸€è‡´ï¼‰
            if chunk_id.startswith(file_id + "_page_"):
                remaining = chunk_id[len(file_id + "_page_"):]
                parts = remaining.split("_")
                
                # æœŸæœ›æ ¼å¼: pagenum_chart_chartindex
                if len(parts) >= 3 and parts[1] == "chart":
                    page_num = parts[0]
                    chart_index = parts[2]
                    
                    chart_dir = self.multimedia_config.get("charts", {}).get("save_dir", "uploads/charts")
                    chart_filename = f"{file_id}_page_{page_num}_chart_{chart_index}.png"
                    chart_path = os.path.join(chart_dir, chart_filename)
                    
                    logger.debug(f"æŸ¥æ‰¾å›¾è¡¨è·¯å¾„: {chart_path}")
                    
                    if os.path.exists(chart_path):
                        logger.debug(f"âœ… å›¾è¡¨æ–‡ä»¶å­˜åœ¨: {chart_path}")
                        return chart_path
                    else:
                        logger.warning(f"âŒ å›¾è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {chart_path}")
            else:
                logger.warning(f"chunk_idæ ¼å¼ä¸åŒ¹é…: {chunk_id}, æœŸæœ›ä»¥ {file_id}_page_ å¼€å¤´")
            
            return None
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾å›¾è¡¨è·¯å¾„å¤±è´¥: {e}")
            return None
    
    def _encode_image_to_base64(self, image_path: str) -> Optional[str]:
        """å°†å›¾åƒç¼–ç ä¸ºbase64å­—ç¬¦ä¸²"""
        try:
            if not image_path or not os.path.exists(image_path):
                return None
            
            with open(image_path, 'rb') as f:
                image_data = f.read()
                base64_str = base64.b64encode(image_data).decode('utf-8')
                
                # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šMIMEç±»å‹
                ext = os.path.splitext(image_path)[1].lower()
                mime_type = {
                    '.png': 'image/png',
                    '.jpg': 'image/jpeg', 
                    '.jpeg': 'image/jpeg',
                    '.gif': 'image/gif',
                    '.bmp': 'image/bmp',
                    '.svg': 'image/svg+xml'
                }.get(ext, 'image/png')
                
                return f"data:{mime_type};base64,{base64_str}"
            
        except Exception as e:
            logger.error(f"å›¾åƒbase64ç¼–ç å¤±è´¥: {e}")
            return None
    
    def _prepare_display_data(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """å‡†å¤‡ç”¨äºå‰ç«¯æ˜¾ç¤ºçš„æ•°æ®"""
        content_type = result.get("content_type", "text")
        display_data = {}
        
        if content_type == "image":
            display_data = self._process_image_result(result, result.get("metadata", {}))
        elif content_type == "table":
            display_data = self._process_table_result(result, result.get("metadata", {}))
        elif content_type == "chart":
            display_data = self._process_chart_result(result, result.get("metadata", {}))
        
        return display_data
    
    def _generate_unified_answer_with_multimedia(self, query: str, search_results: List[Dict], session_id: str) -> Dict[str, Any]:
        """
        ç”ŸæˆåŒ…å«å¤šåª’ä½“å†…å®¹çš„ç»Ÿä¸€å›ç­”
        """
        try:
            # å‡†å¤‡å¤šåª’ä½“å†…å®¹æ˜ å°„
            multimedia_map = {}
            for result in search_results:
                if result.get("content_type") != "text":
                    chunk_id = result.get("chunk_id")
                    multimedia_map[chunk_id] = {
                        "type": result.get("content_type"),
                        "content_description": result.get("content", ""),
                        "display_data": self._prepare_display_data(result),
                        "file_id": result.get("file_id"),
                        "metadata": result.get("metadata", {})
                    }
            
            # ç”ŸæˆåŒ…å«å ä½ç¬¦çš„æ–‡æœ¬å†…å®¹
            text_with_placeholders = self._generate_answer_with_placeholders(query, search_results, session_id, multimedia_map)
            
            # æ„å»ºç»Ÿä¸€çš„å›ç­”ç»“æ„
            unified_answer = {
                "text_content": text_with_placeholders,
                "multimedia_map": multimedia_map,
                "structure": {
                    "has_images": any(v["type"] == "image" for v in multimedia_map.values()),
                    "has_tables": any(v["type"] == "table" for v in multimedia_map.values()),
                    "has_charts": any(v["type"] == "chart" for v in multimedia_map.values()),
                    "multimedia_count": len(multimedia_map)
                }
            }
            
            return unified_answer
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç»Ÿä¸€å›ç­”å¤±è´¥: {e}")
            return {
                "text_content": "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶é‡åˆ°é—®é¢˜ã€‚",
                "multimedia_map": {},
                "structure": {"has_images": False, "has_tables": False, "has_charts": False, "multimedia_count": 0}
            }
    
    def _generate_answer_with_placeholders(self, query: str, search_results: List[Dict], session_id: str, multimedia_map: Dict) -> str:
        """
        ç”ŸæˆåŒ…å«å¤šåª’ä½“å ä½ç¬¦çš„å›ç­”æ–‡æœ¬
        """
        try:
            # å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…å«å¤šåª’ä½“å¼•ç”¨è¯´æ˜
            context_parts = []
            
            for i, result in enumerate(search_results, 1):
                if result["type"] == "vector":
                    content_type = result.get("content_type", "text")
                    chunk_id = result.get("chunk_id", "")
                    
                    if content_type == "text":
                        context_parts.append(f"æ–‡æ¡£ç‰‡æ®µ {i}ï¼š\n{result['content']}\n")
                    else:
                        # ä¸ºå¤šåª’ä½“å†…å®¹æ·»åŠ å ä½ç¬¦è¯´æ˜
                        placeholder = f"[{content_type.upper()}:{chunk_id}]"
                        context_parts.append(f"å¤šåª’ä½“å†…å®¹ {i}ï¼ˆ{content_type}ï¼‰ï¼š{placeholder}\næè¿°ï¼š{result['content']}\n")
                
                elif result["type"] == "graph":
                    context_parts.append(f"å…³ç³»ä¿¡æ¯ {i}ï¼š\n{self._format_graph_result(result)}\n")
            
            context_info = "\n".join(context_parts)
            
            # è·å–å¯¹è¯å†å²
            history = self._get_conversation_history(session_id)
            
            # æ„å»ºç‰¹æ®Šçš„promptï¼ŒæŒ‡å¯¼LLMåœ¨é€‚å½“ä½ç½®æ’å…¥å¤šåª’ä½“å ä½ç¬¦
            multimedia_instructions = ""
            if multimedia_map:
                multimedia_instructions = f"""

åœ¨å›ç­”ä¸­ï¼Œä½ å¯ä»¥åœ¨é€‚å½“çš„ä½ç½®å¼•ç”¨ä»¥ä¸‹å¤šåª’ä½“å†…å®¹ï¼š
{chr(10).join([f'- {content_type.upper()}å†…å®¹: [{content_type.upper()}:{chunk_id}] - {data["content_description"][:100]}...' 
              for chunk_id, data in multimedia_map.items() 
              for content_type in [data["type"]]])}

è¯·åœ¨å›ç­”ä¸­çš„åˆé€‚ä½ç½®ä½¿ç”¨è¿™äº›å ä½ç¬¦ï¼Œä¾‹å¦‚ï¼š
- å½“éœ€è¦å±•ç¤ºå›¾ç‰‡æ—¶ï¼Œå†™: [IMAGE:chunk_id]
- å½“éœ€è¦å±•ç¤ºè¡¨æ ¼æ—¶ï¼Œå†™: [TABLE:chunk_id]  
- å½“éœ€è¦å±•ç¤ºå›¾è¡¨æ—¶ï¼Œå†™: [CHART:chunk_id]

å ä½ç¬¦åº”è¯¥æ”¾åœ¨ç›¸å…³æ–‡å­—æè¿°ä¹‹åï¼Œä½œä¸ºæ”¯æ’‘ææ–™ã€‚
"""
            
            # é€‰æ‹©åˆé€‚çš„æç¤ºè¯æ¨¡æ¿
            if history:
                prompt_template = self.prompt_config["intelligent_search"]["conversation_context"]
                prompt = prompt_template.format(
                    conversation_history=self._format_conversation_history(history),
                    current_question=query,
                    document_info=context_info
                ) + multimedia_instructions
            else:
                prompt_template = self.prompt_config["intelligent_search"]["result_integration"]
                prompt = prompt_template.format(
                    question=query,
                    retrieved_info=context_info
                ) + multimedia_instructions
            
            # è°ƒç”¨LLMç”ŸæˆåŒ…å«å ä½ç¬¦çš„å›ç­”
            answer = self._call_llm(prompt)
            
            return answer
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¸¦å ä½ç¬¦å›ç­”å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    def get_enhanced_answer_with_layout(self, query: str, session_id: str = None) -> Dict[str, Any]:
        """
        è·å–å¸¦æœ‰æ™ºèƒ½å¸ƒå±€çš„å¢å¼ºå›ç­”
        """
        try:
            # è·å–åŸºç¡€æœç´¢ç»“æœ
            basic_result = self.search(query, session_id, stream=False)
            
            if not basic_result.get("success"):
                return basic_result
            
            # åˆ†æå†…å®¹å¹¶ç”Ÿæˆæ™ºèƒ½å¸ƒå±€
            sources = basic_result.get("sources", [])
            layout = self._generate_intelligent_layout(sources, query)
            
            # ç”Ÿæˆç»“æ„åŒ–å›ç­”
            structured_answer = self._generate_structured_answer(query, sources, layout, session_id)
            
            return {
                "success": True,
                "answer": structured_answer,
                "layout": layout,
                "sources": sources,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¢å¼ºå›ç­”å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}",
                "session_id": session_id
            }
    
    def _generate_intelligent_layout(self, sources: List[Dict], query: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ™ºèƒ½å†…å®¹å¸ƒå±€
        """
        layout = {
            "sections": [],
            "content_flow": "mixed",  # mixed, media_first, text_first
            "interaction_level": "basic"  # basic, intermediate, advanced
        }
        
        # åˆ†æå†…å®¹ç±»å‹åˆ†å¸ƒ
        content_types = {}
        for source in sources:
            content_type = source.get("content_type", "text")
            content_types[content_type] = content_types.get(content_type, 0) + 1
        
        # æ ¹æ®æŸ¥è¯¢ç±»å‹è°ƒæ•´å¸ƒå±€ç­–ç•¥
        if "å›¾" in query or "å›¾ç‰‡" in query or "å›¾åƒ" in query:
            layout["content_flow"] = "media_first"
        elif "è¡¨æ ¼" in query or "æ•°æ®" in query or "ç»Ÿè®¡" in query:
            layout["content_flow"] = "mixed"
        else:
            layout["content_flow"] = "text_first"
        
        # ç”Ÿæˆå¸ƒå±€æ®µè½
        if content_types.get("image", 0) > 0:
            layout["sections"].append({
                "type": "images",
                "title": "ç›¸å…³å›¾ç‰‡",
                "count": content_types["image"],
                "interactive": True
            })
        
        if content_types.get("table", 0) > 0:
            layout["sections"].append({
                "type": "tables", 
                "title": "æ•°æ®è¡¨æ ¼",
                "count": content_types["table"],
                "interactive": True
            })
        
        if content_types.get("chart", 0) > 0:
            layout["sections"].append({
                "type": "charts",
                "title": "å›¾è¡¨åˆ†æ", 
                "count": content_types["chart"],
                "interactive": True
            })
        
        # æ€»æ˜¯åŒ…å«æ–‡æœ¬æ€»ç»“
        layout["sections"].append({
            "type": "text_summary",
            "title": "è¯¦ç»†è§£ç­”",
            "count": 1,
            "interactive": False
        })
        
        return layout
    
    def _generate_structured_answer(self, query: str, sources: List[Dict], layout: Dict, session_id: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆç»“æ„åŒ–å›ç­”
        """
        structured = {
            "sections": {},
            "summary": "",
            "key_points": [],
            "recommendations": []
        }
        
        # æŒ‰å¸ƒå±€ç”Ÿæˆå„ä¸ªéƒ¨åˆ†
        for section in layout["sections"]:
            section_type = section["type"]
            
            if section_type in ["images", "tables", "charts"]:
                content_type = section_type[:-1]  # å»æ‰å¤æ•°s
                section_sources = [s for s in sources if s.get("content_type") == content_type]
                structured["sections"][section_type] = {
                    "title": section["title"],
                    "contents": section_sources,
                    "analysis": self._generate_content_analysis(section_sources, content_type)
                }
            elif section_type == "text_summary":
                structured["sections"]["text_summary"] = {
                    "title": section["title"],
                    "content": self._generate_comprehensive_summary(query, sources, session_id)
                }
        
        # ç”Ÿæˆå…³é”®è¦ç‚¹
        structured["key_points"] = self._extract_key_points(sources)
        
        # ç”Ÿæˆå»ºè®®
        structured["recommendations"] = self._generate_recommendations(query, sources)
        
        return structured
    
    def _generate_content_analysis(self, sources: List[Dict], content_type: str) -> str:
        """
        ä¸ºç‰¹å®šç±»å‹å†…å®¹ç”Ÿæˆåˆ†æ
        """
        if not sources:
            return ""
        
        analysis_prompts = {
            "image": "è¯·åˆ†æè¿™äº›å›¾ç‰‡çš„å†…å®¹å’Œå®ƒä»¬ä¹‹é—´çš„å…³è”ï¼š",
            "table": "è¯·åˆ†æè¿™äº›è¡¨æ ¼æ•°æ®çš„å…³é”®ä¿¡æ¯å’Œè¶‹åŠ¿ï¼š", 
            "chart": "è¯·åˆ†æè¿™äº›å›¾è¡¨æ˜¾ç¤ºçš„æ•°æ®æ¨¡å¼å’Œæ´å¯Ÿï¼š"
        }
        
        prompt = analysis_prompts.get(content_type, "è¯·åˆ†æè¿™äº›å†…å®¹ï¼š")
        context = "\n".join([s.get("content", "") for s in sources[:3]])  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
        
        try:
            analysis = self._call_llm(f"{prompt}\n\n{context}\n\nè¯·æä¾›ç®€æ´çš„åˆ†æã€‚")
            return analysis
        except Exception as e:
            logger.error(f"ç”Ÿæˆå†…å®¹åˆ†æå¤±è´¥: {e}")
            return "åˆ†ææš‚æ—¶ä¸å¯ç”¨ã€‚"
    
    def _generate_comprehensive_summary(self, query: str, sources: List[Dict], session_id: str) -> str:
        """
        ç”Ÿæˆç»¼åˆæ€§æ€»ç»“
        """
        # é‡ç”¨ç°æœ‰çš„ç­”æ¡ˆç”Ÿæˆé€»è¾‘
        return self._generate_answer(query, sources, session_id)
    
    def _extract_key_points(self, sources: List[Dict]) -> List[str]:
        """
        æå–å…³é”®è¦ç‚¹
        """
        key_points = []
        
        # ä»ä¸åŒç±»å‹çš„å†…å®¹ä¸­æå–è¦ç‚¹
        for source in sources[:5]:  # é™åˆ¶å¤„ç†æ•°é‡
            content = source.get("content", "")
            content_type = source.get("content_type", "text")
            
            if content and len(content) > 50:
                # ç®€åŒ–çš„è¦ç‚¹æå–ï¼ˆå®é™…å¯ç”¨LLMä¼˜åŒ–ï¼‰
                if content_type == "text":
                    # æå–æ–‡æœ¬è¦ç‚¹
                    sentences = content.split('ã€‚')[:2]  # å–å‰ä¸¤å¥
                    for sentence in sentences:
                        if len(sentence.strip()) > 20:
                            key_points.append(sentence.strip() + "ã€‚")
                elif content_type in ["image", "table", "chart"]:
                    # å¤šåª’ä½“å†…å®¹çš„è¦ç‚¹
                    key_points.append(f"{content_type}å†…å®¹ï¼š{content[:100]}...")
        
        return key_points[:5]  # æœ€å¤š5ä¸ªè¦ç‚¹
    
    def _generate_recommendations(self, query: str, sources: List[Dict]) -> List[str]:
        """
        ç”Ÿæˆç›¸å…³å»ºè®®
        """
        recommendations = []
        
        # æ ¹æ®å†…å®¹ç±»å‹ç”Ÿæˆå»ºè®®
        content_types = set(s.get("content_type", "text") for s in sources)
        
        if "image" in content_types:
            recommendations.append("å»ºè®®ä»”ç»†æŸ¥çœ‹ç›¸å…³å›¾ç‰‡ä»¥è·å¾—æ›´ç›´è§‚çš„ç†è§£")
        
        if "table" in content_types:
            recommendations.append("å»ºè®®åˆ†æè¡¨æ ¼æ•°æ®ä»¥å‘ç°æ›´å¤šæ•°æ®æ¨¡å¼")
        
        if "chart" in content_types:
            recommendations.append("å»ºè®®ç ”ç©¶å›¾è¡¨è¶‹åŠ¿ä»¥äº†è§£æ•°æ®å˜åŒ–")
        
        # æ·»åŠ é€šç”¨å»ºè®®
        recommendations.extend([
            "å¯ä»¥åŸºäºè¿™äº›å†…å®¹æå‡ºæ›´å…·ä½“çš„é—®é¢˜",
            "å»ºè®®å…³æ³¨å…³é”®æ•°æ®ç‚¹å’Œè¶‹åŠ¿åˆ†æ"
        ])
        
        return recommendations[:4]  # æœ€å¤š4ä¸ªå»ºè®®
    
    def _generate_placeholder_image(self) -> str:
        """ç”Ÿæˆå ä½å›¾ç‰‡çš„base64ç¼–ç """
        try:
            # ç”Ÿæˆç®€å•çš„SVGå ä½å›¾ç‰‡
            svg_content = '''<svg width="400" height="300" xmlns="http://www.w3.org/2000/svg">
                <rect width="400" height="300" fill="#f0f0f0" stroke="#ccc" stroke-width="2"/>
                <text x="200" y="140" text-anchor="middle" font-family="Arial" font-size="16" fill="#666">ğŸ“· å›¾ç‰‡å†…å®¹</text>
                <text x="200" y="170" text-anchor="middle" font-family="Arial" font-size="12" fill="#999">æ¥è‡ªPDFæ–‡æ¡£</text>
            </svg>'''
            import base64
            svg_bytes = svg_content.strip().encode('utf-8')
            base64_str = base64.b64encode(svg_bytes).decode('utf-8')
            return f"data:image/svg+xml;base64,{base64_str}"
        except Exception as e:
            logger.error(f"ç”Ÿæˆå ä½å›¾ç‰‡å¤±è´¥: {e}")
            return None
    
    def _generate_placeholder_chart(self) -> str:
        """ç”Ÿæˆå ä½å›¾è¡¨çš„base64ç¼–ç """
        try:
            # ç”Ÿæˆç®€å•çš„SVGå ä½å›¾è¡¨
            svg_content = '''<svg width="400" height="300" xmlns="http://www.w3.org/2000/svg">
                <rect width="400" height="300" fill="#f8f9fa" stroke="#dee2e6" stroke-width="2"/>
                <rect x="50" y="50" width="300" height="200" fill="none" stroke="#6c757d" stroke-width="1"/>
                <line x1="50" y1="250" x2="350" y2="250" stroke="#6c757d" stroke-width="2"/>
                <line x1="50" y1="50" x2="50" y2="250" stroke="#6c757d" stroke-width="2"/>
                <rect x="80" y="200" width="40" height="50" fill="#007bff" opacity="0.7"/>
                <rect x="140" y="150" width="40" height="100" fill="#28a745" opacity="0.7"/>
                <rect x="200" y="100" width="40" height="150" fill="#ffc107" opacity="0.7"/>
                <rect x="260" y="180" width="40" height="70" fill="#dc3545" opacity="0.7"/>
                <text x="200" y="280" text-anchor="middle" font-family="Arial" font-size="14" fill="#495057">ğŸ“Š å›¾è¡¨æ•°æ®å¯è§†åŒ–</text>
            </svg>'''
            import base64
            svg_bytes = svg_content.strip().encode('utf-8')
            base64_str = base64.b64encode(svg_bytes).decode('utf-8')
            return f"data:image/svg+xml;base64,{base64_str}"
        except Exception as e:
            logger.error(f"ç”Ÿæˆå ä½å›¾è¡¨å¤±è´¥: {e}")
            return None
    
    def _generate_sample_table_data(self, description: str) -> List[List[str]]:
        """ç”Ÿæˆç¤ºä¾‹è¡¨æ ¼æ•°æ®"""
        try:
            # æ ¹æ®æè¿°ç”Ÿæˆåˆé€‚çš„ç¤ºä¾‹æ•°æ®
            if "è´¢åŠ¡" in description or "æ”¶å…¥" in description or "åˆ©æ¶¦" in description:
                return [
                    ["é¡¹ç›®", "Q1", "Q2", "Q3", "Q4"],
                    ["è¥ä¸šæ”¶å…¥(ä¸‡å…ƒ)", "2,580", "2,890", "3,120", "3,650"],
                    ["å‡€åˆ©æ¶¦(ä¸‡å…ƒ)", "386", "445", "512", "678"],
                    ["å¢é•¿ç‡", "12.5%", "15.2%", "18.8%", "25.1%"]
                ]
            elif "é”€å”®" in description or "äº§å“" in description:
                return [
                    ["äº§å“", "é”€é‡", "å•ä»·", "æ”¶å…¥"],
                    ["äº§å“A", "1,200", "Â¥85", "Â¥102,000"],
                    ["äº§å“B", "890", "Â¥120", "Â¥106,800"],
                    ["äº§å“C", "650", "Â¥200", "Â¥130,000"]
                ]
            else:
                return [
                    ["ç±»åˆ«", "æ•°å€¼", "ç™¾åˆ†æ¯”", "å¤‡æ³¨"],
                    ["é¡¹ç›®1", "125", "25.0%", "æ­£å¸¸"],
                    ["é¡¹ç›®2", "189", "37.8%", "ä¼˜ç§€"],
                    ["é¡¹ç›®3", "94", "18.8%", "ä¸€èˆ¬"],
                    ["é¡¹ç›®4", "92", "18.4%", "éœ€æ”¹è¿›"]
                ]
        except Exception as e:
            logger.error(f"ç”Ÿæˆç¤ºä¾‹è¡¨æ ¼æ•°æ®å¤±è´¥: {e}")
            return [
                ["åˆ—1", "åˆ—2", "åˆ—3"],
                ["æ•°æ®1", "æ•°æ®2", "æ•°æ®3"],
                ["ç¤ºä¾‹", "å†…å®¹", "å±•ç¤º"]
            ]
    
    def get_multimodal_content(self, file_id: str, content_type: str = None) -> List[Dict[str, Any]]:
        """è·å–æ–‡ä»¶çš„å¤šæ¨¡æ€å†…å®¹"""
        try:
            # ä»å‘é‡æ•°æ®åº“æŸ¥è¯¢æŒ‡å®šæ–‡ä»¶çš„å¤šæ¨¡æ€å†…å®¹
            if not milvus_manager.collection:
                return []
            
            # æ„å»ºæŸ¥è¯¢è¡¨è¾¾å¼
            if content_type:
                expr = f"file_id == '{file_id}'"
            else:
                expr = f"file_id == '{file_id}'"
            
            results = milvus_manager.collection.query(
                expr=expr,
                output_fields=["chunk_id", "content", "metadata"]
            )
            
            multimodal_content = []
            for result in results:
                metadata = json.loads(result["metadata"]) if result["metadata"] else {}
                result_type = metadata.get("type", "text")
                
                if not content_type or result_type == content_type:
                    content_item = {
                        "chunk_id": result["chunk_id"],
                        "content": result["content"],
                        "content_type": result_type,
                        "metadata": metadata
                    }
                    
                    # æ·»åŠ å¤šæ¨¡æ€å±•ç¤ºä¿¡æ¯
                    if result_type == "image":
                        content_item.update(self._process_image_result(result, metadata))
                    elif result_type == "table":
                        content_item.update(self._process_table_result(result, metadata))
                    elif result_type == "chart":
                        content_item.update(self._process_chart_result(result, metadata))
                    
                    multimodal_content.append(content_item)
            
            return multimodal_content
            
        except Exception as e:
            logger.error(f"è·å–å¤šæ¨¡æ€å†…å®¹å¤±è´¥: {e}")
            return []

# å…¨å±€æ™ºèƒ½æ£€ç´¢æœåŠ¡å®ä¾‹
search_service = SearchService() 