"""
æ–‡ä»¶ç®¡ç†æœåŠ¡
è´Ÿè´£PDFæ–‡ä»¶çš„ä¸Šä¼ ã€åˆ é™¤ã€é‡å‘½åä»¥åŠå†…å®¹è§£æå’ŒGraphRAGæ•°æ®æå–
"""
import os
import uuid
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
import json
import fitz  # PyMuPDF
from PIL import Image
import io
import threading
from concurrent.futures import ThreadPoolExecutor
import requests
import re

from utils.config_loader import config_loader
from utils.database import mysql_manager, milvus_manager, neo4j_manager
from utils.model_manager import model_manager

logger = logging.getLogger(__name__)

class FileService:
    """æ–‡ä»¶ç®¡ç†æœåŠ¡ç±»"""
    
    def __init__(self):
        self.config = config_loader.get_app_config()
        self.model_config = config_loader.get_model_config()
        self.prompt_config = config_loader.get_prompt_config()
        self.upload_dir = self.config["upload"]["upload_dir"]
        self.allowed_extensions = set(self.config["upload"]["allowed_extensions"])
        self.max_file_size = self.config["upload"]["max_file_size"] * 1024 * 1024  # MB to bytes
        self.processing_status = {}  # æ–‡ä»¶å¤„ç†çŠ¶æ€
        
        # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
        os.makedirs(self.upload_dir, exist_ok=True)
    
    def upload_file(self, file, filename: str, original_filename: str = None) -> Dict[str, Any]:
        """
        ä¸Šä¼ PDFæ–‡ä»¶
        
        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            filename: å¤„ç†åçš„æ–‡ä»¶åï¼ˆç”¨äºç±»å‹æ£€æŸ¥ï¼‰
            original_filename: åŸå§‹æ–‡ä»¶åï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
            
        Returns:
            ä¸Šä¼ ç»“æœ
        """
        try:
            # ä½¿ç”¨åŸå§‹æ–‡ä»¶åæˆ–å¤„ç†åçš„æ–‡ä»¶åè¿›è¡Œç±»å‹æ£€æŸ¥
            check_filename = original_filename or filename
            display_filename = original_filename or filename
            
            logger.info(f"æ–‡ä»¶ä¸Šä¼ æ£€æŸ¥ - æ£€æŸ¥æ–‡ä»¶å: {check_filename}, æ˜¾ç¤ºæ–‡ä»¶å: {display_filename}")
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if not self._allowed_file(check_filename):
                logger.warning(f"æ–‡ä»¶ç±»å‹æ£€æŸ¥å¤±è´¥ - æ–‡ä»¶å: {check_filename}")
                return {
                    "success": False,
                    "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œä»…æ”¯æŒ: {', '.join(self.allowed_extensions)}"
                }
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
            file_size = file.tell()
            file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            
            if file_size > self.max_file_size:
                return {
                    "success": False,
                    "message": f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ ({self.max_file_size // 1024 // 1024}MB)"
                }
            
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶IDå’Œä¿å­˜è·¯å¾„
            file_id = str(uuid.uuid4())
            
            # ç”Ÿæˆå®‰å…¨çš„å­˜å‚¨æ–‡ä»¶å
            import time
            timestamp = str(int(time.time()))
            safe_filename = f"{file_id}_{timestamp}.pdf"  # ç»Ÿä¸€ä½¿ç”¨.pdfæ‰©å±•å
            file_path = os.path.join(self.upload_dir, safe_filename)
            
            logger.info(f"ä¿å­˜æ–‡ä»¶ - æ–‡ä»¶ID: {file_id}, å­˜å‚¨è·¯å¾„: {file_path}, åŸå§‹æ–‡ä»¶å: {display_filename}")
            
            # ä¿å­˜æ–‡ä»¶
            file.save(file_path)
            
            # ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“
            file_info = {
                "file_id": file_id,
                "original_filename": display_filename,
                "filename": safe_filename,
                "file_path": file_path,
                "file_size": file_size,
                "upload_time": datetime.now(),
                "status": "uploaded",
                "processing_progress": 0
            }
            
            logger.info(f"ğŸ“‹ğŸ“‹ğŸ“‹ å‡†å¤‡ä¿å­˜æ–‡ä»¶ä¿¡æ¯ - æ–‡ä»¶ID: {file_id}")
            logger.info(f"ğŸ“‹ æ–‡ä»¶ä¿¡æ¯å†…å®¹: {file_info}")
            try:
                logger.info(f"ğŸ“‹ è°ƒç”¨_save_file_infoæ–¹æ³•...")
                self._save_file_info(file_info)
                logger.info(f"âœ…âœ…âœ… æ–‡ä»¶ä¿¡æ¯ä¿å­˜æˆåŠŸ - æ–‡ä»¶ID: {file_id}")
            except Exception as db_error:
                logger.error(f"âŒâŒâŒ æ–‡ä»¶ä¿¡æ¯ä¿å­˜å¤±è´¥ï¼Œä½†ç»§ç»­è¿›è¡Œæ–‡ä»¶å¤„ç† - æ–‡ä»¶ID: {file_id}, é”™è¯¯: {db_error}")
                # æ³¨æ„ï¼šå³ä½¿æ•°æ®åº“ä¿å­˜å¤±è´¥ï¼Œæˆ‘ä»¬ä¹Ÿç»§ç»­è¿›è¡Œæ–‡ä»¶å¤„ç†
            
            # å¼‚æ­¥å¼€å§‹æ–‡ä»¶å†…å®¹è¯†åˆ«
            logger.info(f"ğŸš€ğŸš€ğŸš€ å‡†å¤‡å¯åŠ¨æ–‡ä»¶å¤„ç†çº¿ç¨‹ - æ–‡ä»¶ID: {file_id}, æ–‡ä»¶è·¯å¾„: {file_path}")
            logger.info(f"ğŸ” å½“å‰upload_fileæ–¹æ³•æ­£åœ¨æ‰§è¡Œåˆ°æ–‡ä»¶å¤„ç†éƒ¨åˆ†")
            logger.info(f"ğŸ” æ–‡ä»¶å¤„ç†æ–¹æ³•åœ°å€: {self._start_file_processing}")
            try:
                logger.info(f"ğŸ” å³å°†è°ƒç”¨self._start_file_processing...")
                self._start_file_processing(file_id, file_path)
                logger.info(f"âœ…âœ…âœ… æ–‡ä»¶å¤„ç†çº¿ç¨‹å¯åŠ¨å‘½ä»¤å·²å‘é€ - æ–‡ä»¶ID: {file_id}")
            except Exception as thread_error:
                logger.error(f"âŒâŒâŒ æ–‡ä»¶å¤„ç†çº¿ç¨‹å¯åŠ¨å¤±è´¥ - æ–‡ä»¶ID: {file_id}, é”™è¯¯: {thread_error}", exc_info=True)
            
            return {
                "success": True,
                "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
                "file_id": file_id,
                "filename": display_filename
            }
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
            }
    
    def delete_file(self, file_id: str) -> Dict[str, Any]:
        """
        åˆ é™¤æ–‡ä»¶
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            åˆ é™¤ç»“æœ
        """
        try:
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_info = self._get_file_info(file_id)
            if not file_info:
                return {
                    "success": False,
                    "message": "æ–‡ä»¶ä¸å­˜åœ¨"
                }
            
            # åˆ é™¤ç‰©ç†æ–‡ä»¶
            if os.path.exists(file_info["file_path"]):
                os.remove(file_info["file_path"])
            
            # ä»æ•°æ®åº“åˆ é™¤æ–‡ä»¶ä¿¡æ¯
            self._delete_file_info(file_id)
            
            # åˆ é™¤å‘é‡æ•°æ®
            self._delete_vector_data(file_id)
            
            # åˆ é™¤å›¾æ•°æ®
            self._delete_graph_data(file_id)
            
            return {
                "success": True,
                "message": "æ–‡ä»¶åˆ é™¤æˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶åˆ é™¤å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ–‡ä»¶åˆ é™¤å¤±è´¥: {str(e)}"
            }
    
    def rename_file(self, file_id: str, new_filename: str) -> Dict[str, Any]:
        """
        é‡å‘½åæ–‡ä»¶
        
        Args:
            file_id: æ–‡ä»¶ID
            new_filename: æ–°æ–‡ä»¶å
            
        Returns:
            é‡å‘½åç»“æœ
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            file_info = self._get_file_info(file_id)
            if not file_info:
                return {
                    "success": False,
                    "message": "æ–‡ä»¶ä¸å­˜åœ¨"
                }
            
            # æ›´æ–°æ–‡ä»¶å
            mysql_manager.execute_update(
                "UPDATE files SET original_filename = %s WHERE file_id = %s",
                (new_filename, file_id)
            )
            
            return {
                "success": True,
                "message": "æ–‡ä»¶é‡å‘½åæˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶é‡å‘½åå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ–‡ä»¶é‡å‘½åå¤±è´¥: {str(e)}"
            }
    
    def get_file_list(self) -> List[Dict[str, Any]]:
        """
        è·å–æ–‡ä»¶åˆ—è¡¨
        
        Returns:
            æ–‡ä»¶åˆ—è¡¨
        """
        try:
            files = mysql_manager.execute_query(
                "SELECT file_id, original_filename, file_size, upload_time, status, processing_progress FROM files ORDER BY upload_time DESC"
            )
            
            # å¤„ç†æ–‡ä»¶çŠ¶æ€å’Œè¿›åº¦
            for file in files:
                file_id = file["file_id"]
                if file_id in self.processing_status:
                    file["processing_progress"] = self.processing_status[file_id]["progress"]
                    file["status"] = self.processing_status[file_id]["status"]
            
            return files
            
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_processing_status(self, file_id: str) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶å¤„ç†çŠ¶æ€
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            å¤„ç†çŠ¶æ€ä¿¡æ¯
        """
        if file_id in self.processing_status:
            return self.processing_status[file_id]
        
        # ä»æ•°æ®åº“è·å–çŠ¶æ€
        file_info = self._get_file_info(file_id)
        if file_info:
            return {
                "status": file_info["status"],
                "progress": file_info["processing_progress"],
                "message": "å¤„ç†ä¸­..." if file_info["status"] == "processing" else "å¤„ç†å®Œæˆ"
            }
        
        return {
            "status": "not_found",
            "progress": 0,
            "message": "æ–‡ä»¶ä¸å­˜åœ¨"
        }
    
    def get_file_detailed_info(self, file_id: str) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
        """
        try:
            # è·å–åŸºæœ¬æ–‡ä»¶ä¿¡æ¯
            file_info = self._get_file_info(file_id)
            if not file_info:
                return {
                    "success": False,
                    "message": "æ–‡ä»¶ä¸å­˜åœ¨"
                }
            
            # è·å–å¤„ç†çŠ¶æ€
            status_info = self.get_processing_status(file_id)
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self._get_file_statistics(file_id)
            
            return {
                "success": True,
                "file_info": {
                    "file_id": file_info["file_id"],
                    "original_filename": file_info["original_filename"],
                    "file_size": file_info["file_size"],
                    "upload_time": file_info["upload_time"],
                    "status": file_info["status"],
                    "processing_progress": file_info["processing_progress"],
                    "file_path": file_info["file_path"]
                },
                "processing_info": status_info,
                "statistics": stats
            }
            
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}"
            }
    
    def _get_file_statistics(self, file_id: str) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # è·å–å‘é‡æ•°æ®ç»Ÿè®¡
            vector_count = 0
            try:
                if milvus_manager.collection and milvus_manager.has_data():
                    # æŸ¥è¯¢å‘é‡æ•°æ®æ•°é‡
                    vector_result = milvus_manager.collection.query(
                        expr=f"file_id == '{file_id}'",
                        output_fields=["chunk_id"]
                    )
                    vector_count = len(vector_result) if vector_result else 0
            except Exception as e:
                logger.warning(f"è·å–å‘é‡æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
            
            # è·å–å›¾æ•°æ®ç»Ÿè®¡
            entity_count = 0
            relation_count = 0
            try:
                entity_result = neo4j_manager.execute_query(
                    "MATCH (n) WHERE n.file_id = $file_id RETURN count(n) as count",
                    {"file_id": file_id}
                )
                entity_count = entity_result[0]["count"] if entity_result else 0
                
                relation_result = neo4j_manager.execute_query(
                    "MATCH ()-[r]->() WHERE r.file_id = $file_id RETURN count(r) as count",
                    {"file_id": file_id}
                )
                relation_count = relation_result[0]["count"] if relation_result else 0
            except Exception as e:
                logger.warning(f"è·å–å›¾æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
            
            return {
                "chunks_count": vector_count,
                "entities_count": entity_count,
                "relations_count": relation_count
            }
            
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "chunks_count": 0,
                "entities_count": 0,
                "relations_count": 0
            }
    
    def _allowed_file(self, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦å…è®¸"""
        return '.' in filename and \
               '.' + filename.rsplit('.', 1)[1].lower() in self.allowed_extensions
    
    def _save_file_info(self, file_info: Dict[str, Any]) -> None:
        """ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“ - æ–‡ä»¶ID: {file_info['file_id']}")
            mysql_manager.execute_update(
                """
                INSERT INTO files (file_id, original_filename, filename, file_path, file_size, upload_time, status, processing_progress)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """,
                (
                    file_info["file_id"],
                    file_info["original_filename"],
                    file_info["filename"],
                    file_info["file_path"],
                    file_info["file_size"],
                    file_info["upload_time"],
                    file_info["status"],
                    file_info["processing_progress"]
                )
            )
            logger.info(f"âœ… æ–‡ä»¶ä¿¡æ¯å·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“ - æ–‡ä»¶ID: {file_info['file_id']}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“å¤±è´¥ - æ–‡ä»¶ID: {file_info.get('file_id', 'unknown')}, é”™è¯¯: {e}", exc_info=True)
            # ä¸é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©åç»­å¤„ç†ç»§ç»­è¿›è¡Œ
    
    def _get_file_info(self, file_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        results = mysql_manager.execute_query(
            "SELECT * FROM files WHERE file_id = %s",
            (file_id,)
        )
        return results[0] if results else None
    
    def _delete_file_info(self, file_id: str) -> None:
        """ä»æ•°æ®åº“åˆ é™¤æ–‡ä»¶ä¿¡æ¯"""
        mysql_manager.execute_update(
            "DELETE FROM files WHERE file_id = %s",
            (file_id,)
        )
    
    def _delete_vector_data(self, file_id: str) -> None:
        """åˆ é™¤å‘é‡æ•°æ®"""
        try:
            if milvus_manager.collection and milvus_manager.has_data():
                # ä½¿ç”¨Milvusçš„deleteæ¥å£åˆ é™¤æŒ‡å®šfile_idçš„å‘é‡
                expr = f"file_id == '{file_id}'"
                milvus_manager.collection.delete(expr)
                milvus_manager.collection.flush()
                logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶ {file_id} çš„å‘é‡æ•°æ®")
            else:
                logger.info(f"Milvusé›†åˆä¸ºç©ºæˆ–æœªåˆå§‹åŒ–ï¼Œè·³è¿‡åˆ é™¤æ–‡ä»¶ {file_id} çš„å‘é‡æ•°æ®")
        except Exception as e:
            logger.warning(f"åˆ é™¤æ–‡ä»¶ {file_id} çš„å‘é‡æ•°æ®å¤±è´¥: {e}")
    
    def _delete_graph_data(self, file_id: str) -> None:
        """åˆ é™¤å›¾æ•°æ®"""
        try:
            # åˆ é™¤å®ä½“èŠ‚ç‚¹å’Œå…³ç³»
            neo4j_manager.execute_query(
                "MATCH (n {file_id: $file_id}) DETACH DELETE n",
                {"file_id": file_id}
            )
            # åˆ é™¤åªæœ‰file_idå±æ€§çš„å…³ç³»
            neo4j_manager.execute_query(
                "MATCH ()-[r {file_id: $file_id}]-() DELETE r",
                {"file_id": file_id}
            )
            logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶ {file_id} çš„å›¾æ•°æ®")
        except Exception as e:
            logger.warning(f"åˆ é™¤æ–‡ä»¶ {file_id} çš„å›¾æ•°æ®å¤±è´¥: {e}")
    
    def _start_file_processing(self, file_id: str, file_path: str) -> None:
        """å¼€å§‹æ–‡ä»¶å†…å®¹è¯†åˆ«å¤„ç†"""
        logger.info(f"ğŸ”§ğŸ”§ğŸ”§ _start_file_processingæ–¹æ³•è¢«è°ƒç”¨ - æ–‡ä»¶ID: {file_id}")
        logger.info(f"ğŸ”§ğŸ”§ğŸ”§ æ–¹æ³•å‚æ•°: file_id={file_id}, file_path={file_path}")
        try:
            logger.info(f"ğŸ”§ è¿›å…¥tryå—ï¼Œå¼€å§‹åˆ›å»ºæ–‡ä»¶å¤„ç†çº¿ç¨‹ - æ–‡ä»¶ID: {file_id}")
            logger.info(f"ğŸ”§ çº¿ç¨‹ç›®æ ‡: _process_file_content_safe, å‚æ•°: {file_id}, {file_path}")
            
            # é¦–å…ˆæµ‹è¯•æ˜¯å¦èƒ½åˆ›å»ºç®€å•çº¿ç¨‹
            def test_thread():
                logger.info(f"ğŸ§ª æµ‹è¯•çº¿ç¨‹è¿è¡ŒæˆåŠŸ - æ–‡ä»¶ID: {file_id}")
            
            test = threading.Thread(target=test_thread, name=f"Test-{file_id[:8]}")
            test.daemon = True
            test.start()
            test.join(timeout=1)  # ç­‰å¾…1ç§’
            logger.info(f"âœ… æµ‹è¯•çº¿ç¨‹å®Œæˆ")
            
            # åœ¨åå°çº¿ç¨‹ä¸­å¤„ç†æ–‡ä»¶
            thread = threading.Thread(
                target=self._process_file_content_safe,
                args=(file_id, file_path),
                name=f"FileProcessor-{file_id[:8]}"
            )
            thread.daemon = True
            
            logger.info(f"ğŸ”§ æ–‡ä»¶å¤„ç†çº¿ç¨‹å·²åˆ›å»ºï¼Œå‡†å¤‡å¯åŠ¨ - çº¿ç¨‹å: {thread.name}")
            thread.start()
            logger.info(f"âœ… æ–‡ä»¶å¤„ç†çº¿ç¨‹å·²å¯åŠ¨ - çº¿ç¨‹å: {thread.name}, çº¿ç¨‹ID: {thread.ident}")
            
            # ç«‹å³è¿”å›ï¼Œä¸ç­‰å¾…çº¿ç¨‹
            return
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨æ–‡ä»¶å¤„ç†çº¿ç¨‹å¤±è´¥ - æ–‡ä»¶ID: {file_id}, é”™è¯¯: {e}", exc_info=True)
    
    def _process_file_content_safe(self, file_id: str, file_path: str) -> None:
        """å®‰å…¨çš„æ–‡ä»¶å¤„ç†åŒ…è£…å™¨ - ç¬¬ä¸€æ­¥ï¼šPDFæ–‡æœ¬æå–"""
        logger.info(f"ğŸš€ğŸš€ğŸš€ _process_file_content_safeæ–¹æ³•è¢«è°ƒç”¨ - æ–‡ä»¶ID: {file_id}")
        logger.info(f"ğŸš€ğŸš€ğŸš€ çº¿ç¨‹å¼€å§‹æ‰§è¡Œï¼Œå½“å‰çº¿ç¨‹: {threading.current_thread().name}")
        
        import time
        timeout_timer = None
        
        # è¶…æ—¶å¤„ç†å‡½æ•°
        def timeout_handler():
            logger.error(f"â° PDFå¤„ç†è¶…æ—¶ - æ–‡ä»¶ID: {file_id}")
            self._handle_processing_failure(file_id, "å¤„ç†è¶…æ—¶")
        
        try:
            logger.info(f"ğŸš€ è¿›å…¥tryå—ï¼ŒPDFæ™ºèƒ½å¤„ç†å¯åŠ¨ - æ–‡ä»¶ID: {file_id}")
            logger.info(f"ğŸš€ çº¿ç¨‹ä¿¡æ¯: {threading.current_thread().name}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            import os
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            logger.info(f"âœ… æ–‡ä»¶å­˜åœ¨æ£€æŸ¥é€šè¿‡: {file_path}")
            logger.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(file_path)} bytes")
            
            # è®¾ç½®å¤„ç†çŠ¶æ€
            self.processing_status[file_id] = {
                "status": "processing",
                "progress": 10,
                "message": "å¼€å§‹PDFå¤„ç†..."
            }
            
            # è®¾ç½®è¶…æ—¶æ§åˆ¶ï¼ˆ60ç§’ï¼‰ - ä½¿ç”¨Timeræ›¿ä»£signal
            timeout_timer = threading.Timer(60.0, timeout_handler)
            timeout_timer.start()
            logger.info(f"â° è¶…æ—¶æ§åˆ¶å·²å¯åŠ¨ (60ç§’)")
            
            # ç¬¬ä¸€æ­¥ï¼šå®‰å…¨çš„PDFæ–‡æœ¬æå–
            logger.info(f"ğŸ“– å¼€å§‹PDFæ–‡æœ¬æå– - æ–‡ä»¶ID: {file_id}")
            text_content = self._safe_pdf_text_extraction(file_id, file_path)
            
            # æ›´æ–°è¿›åº¦
            self.processing_status[file_id] = {
                "status": "processing",
                "progress": 50,
                "message": "PDFæ–‡æœ¬æå–å®Œæˆï¼Œå‡†å¤‡ä¿å­˜..."
            }
            
            # ç®€å•ä¿å­˜æ–‡æœ¬å†…å®¹åˆ°æ•°æ®åº“ï¼ˆä½œä¸ºå¤„ç†ç»“æœçš„è®°å½•ï¼‰
            logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜å¤„ç†ç»“æœ - æ–‡ä»¶ID: {file_id}")
            self._save_processing_result(file_id, text_content)
            
            # å–æ¶ˆè¶…æ—¶
            if timeout_timer:
                timeout_timer.cancel()
                logger.info(f"â° è¶…æ—¶æ§åˆ¶å·²å–æ¶ˆ")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
            self.processing_status[file_id] = {
                "status": "completed", 
                "progress": 100,
                "message": "PDFå¤„ç†å®Œæˆ"
            }
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            mysql_manager.execute_update(
                "UPDATE files SET status = 'completed', processing_progress = 100 WHERE file_id = %s",
                (file_id,)
            )
            
            logger.info(f"âœ… PDFæ™ºèƒ½å¤„ç†å®Œæˆ - æ–‡ä»¶ID: {file_id}")
            
        except Exception as e:
            logger.error(f"âŒ PDFå¤„ç†å¤±è´¥ - æ–‡ä»¶ID: {file_id}, é”™è¯¯: {e}", exc_info=True)
            self._handle_processing_failure(file_id, str(e))
        
        finally:
            # ç¡®ä¿æ¸…ç†è¶…æ—¶è®¾ç½®
            if timeout_timer:
                timeout_timer.cancel()
            logger.info(f"ğŸ PDFå¤„ç†çº¿ç¨‹ç»“æŸ - æ–‡ä»¶ID: {file_id}")
    
    def _safe_pdf_text_extraction(self, file_id: str, file_path: str) -> str:
        """å®‰å…¨çš„PDFæ–‡æœ¬æå–"""
        try:
            logger.info(f"ğŸ“– æ­£åœ¨æ‰“å¼€PDFæ–‡ä»¶: {file_path}")
            import fitz
            
            # æ‰“å¼€PDFæ–‡æ¡£
            doc = fitz.open(file_path)
            total_pages = len(doc)
            logger.info(f"ğŸ“„ PDFæ–‡ä»¶æ‰“å¼€æˆåŠŸï¼Œå…± {total_pages} é¡µ")
            
            # é™åˆ¶å¤„ç†é¡µæ•°ï¼Œé¿å…å¤„ç†è¿‡å¤§æ–‡ä»¶
            max_pages = min(total_pages, 50)  # æœ€å¤šå¤„ç†50é¡µ
            logger.info(f"ğŸ“Š å°†å¤„ç†å‰ {max_pages} é¡µ")
            
            all_text = []
            
            # é€é¡µæå–æ–‡æœ¬
            for page_num in range(max_pages):
                logger.info(f"ğŸ“– å¤„ç†ç¬¬ {page_num + 1}/{max_pages} é¡µ")
                
                # æ›´æ–°å¤„ç†è¿›åº¦
                progress = 10 + int((page_num / max_pages) * 40)  # 10-50%çš„è¿›åº¦
                self.processing_status[file_id]["progress"] = progress
                self.processing_status[file_id]["message"] = f"æå–ç¬¬ {page_num + 1}/{max_pages} é¡µæ–‡æœ¬..."
                
                try:
                    page = doc[page_num]
                    text = page.get_text()
                    
                    if text.strip():
                        all_text.append(f"--- ç¬¬ {page_num + 1} é¡µ ---\n{text.strip()}")
                        logger.debug(f"âœ… ç¬¬ {page_num + 1} é¡µæ–‡æœ¬æå–å®Œæˆï¼Œé•¿åº¦: {len(text)}")
                    else:
                        logger.debug(f"âš ï¸ ç¬¬ {page_num + 1} é¡µæ— æ–‡æœ¬å†…å®¹")
                
                except Exception as page_error:
                    logger.warning(f"âš ï¸ ç¬¬ {page_num + 1} é¡µå¤„ç†å¤±è´¥: {page_error}")
                    continue
            
            # å…³é—­æ–‡æ¡£
            doc.close()
            logger.info(f"ğŸ“– PDFæ–‡æ¡£å·²å…³é—­")
            
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
            full_text = "\n\n".join(all_text)
            logger.info(f"âœ… PDFæ–‡æœ¬æå–å®Œæˆï¼Œæ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
            
            return full_text
            
        except Exception as e:
            logger.error(f"âŒ PDFæ–‡æœ¬æå–å¤±è´¥: {e}", exc_info=True)
            raise
    
    def _save_processing_result(self, file_id: str, content: str) -> None:
        """ä¿å­˜å¤„ç†ç»“æœ"""
        try:
            # ç®€å•ç»Ÿè®¡ä¿¡æ¯
            char_count = len(content)
            line_count = content.count('\n') + 1 if content else 0
            
            logger.info(f"ğŸ’¾ ä¿å­˜å¤„ç†ç»“æœ - æ–‡ä»¶ID: {file_id}, å­—ç¬¦æ•°: {char_count}, è¡Œæ•°: {line_count}")
            
            # è¿™é‡Œå¯ä»¥ä¿å­˜åˆ°æ•°æ®åº“æˆ–æ–‡ä»¶ç³»ç»Ÿ
            # æš‚æ—¶åªè®°å½•æ—¥å¿—
            logger.info(f"âœ… å¤„ç†ç»“æœç»Ÿè®¡å®Œæˆ - æ–‡ä»¶ID: {file_id}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¤„ç†ç»“æœå¤±è´¥: {e}", exc_info=True)
    
    def _handle_processing_failure(self, file_id: str, error_msg: str) -> None:
        """å¤„ç†å¤±è´¥çš„ç»Ÿä¸€å¤„ç†"""
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            self.processing_status[file_id] = {
                "status": "failed",
                "progress": 0,
                "message": f"å¤„ç†å¤±è´¥: {error_msg}"
            }
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            mysql_manager.execute_update(
                "UPDATE files SET status = 'failed' WHERE file_id = %s",
                (file_id,)
            )
            
            logger.info(f"âœ… å¤±è´¥çŠ¶æ€å·²æ›´æ–° - æ–‡ä»¶ID: {file_id}")
            
        except Exception as db_error:
            logger.error(f"âŒ æ›´æ–°å¤±è´¥çŠ¶æ€å¤±è´¥: {db_error}")
    
    def _process_file_content(self, file_id: str, file_path: str) -> None:
        """
        å¤„ç†æ–‡ä»¶å†…å®¹ï¼Œæå–æ–‡å­—ã€è¡¨æ ¼ã€å›¾ç‰‡ã€å›¾è¡¨ç­‰
        
        Args:
            file_id: æ–‡ä»¶ID
            file_path: æ–‡ä»¶è·¯å¾„
        """
        import threading
        current_thread = threading.current_thread()
        logger.info(f"âœ… æ–‡ä»¶å¤„ç†çº¿ç¨‹å·²å¯åŠ¨ - çº¿ç¨‹å: {current_thread.name}, çº¿ç¨‹ID: {current_thread.ident}")
        logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡ä»¶ {file_id}ï¼Œè·¯å¾„: {file_path}")
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            import os
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            logger.info(f"âœ… æ–‡ä»¶å­˜åœ¨æ£€æŸ¥é€šè¿‡: {file_path}")
            logger.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {os.path.getsize(file_path)} bytes")
            
            # åˆå§‹åŒ–å¤„ç†çŠ¶æ€
            logger.info(f"ğŸ”„ åˆå§‹åŒ–å¤„ç†çŠ¶æ€...")
            self.processing_status[file_id] = {
                "status": "processing",
                "progress": 0,
                "message": "å¼€å§‹å¤„ç†æ–‡ä»¶..."
            }
            logger.info(f"âœ… å¤„ç†çŠ¶æ€åˆå§‹åŒ–å®Œæˆ")
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            logger.info(f"ğŸ’¾ æ›´æ–°æ•°æ®åº“çŠ¶æ€...")
            mysql_manager.execute_update(
                "UPDATE files SET status = 'processing', processing_progress = 0 WHERE file_id = %s",
                (file_id,)
            )
            logger.info(f"âœ… æ–‡ä»¶ {file_id} çŠ¶æ€å·²æ›´æ–°ä¸ºprocessing")
            
            # æ‰“å¼€PDFæ–‡ä»¶
            logger.info(f"ğŸ“– æ­£åœ¨æ‰“å¼€PDFæ–‡ä»¶: {file_path}")
            import fitz
            logger.info(f"âœ… fitzæ¨¡å—å¯¼å…¥æˆåŠŸ")
            doc = fitz.open(file_path)
            total_pages = len(doc)
            logger.info(f"ğŸ‰ PDFæ–‡ä»¶æ‰“å¼€æˆåŠŸï¼Œå…± {total_pages} é¡µ")
            
            all_chunks = []
            entities = []
            relations = []
            
            # é€é¡µå¤„ç†
            for page_num in range(total_pages):
                logger.info(f"å¼€å§‹å¤„ç†ç¬¬ {page_num + 1}/{total_pages} é¡µ")
                page = doc[page_num]
                
                # æ›´æ–°è¿›åº¦
                progress = int((page_num / total_pages) * 80)  # 80%ç”¨äºé¡µé¢å¤„ç†
                self.processing_status[file_id]["progress"] = progress
                self.processing_status[file_id]["message"] = f"å¤„ç†ç¬¬ {page_num + 1}/{total_pages} é¡µ..."
                
                # æå–æ–‡æœ¬
                logger.debug(f"æå–ç¬¬ {page_num + 1} é¡µæ–‡æœ¬")
                text_content = self._extract_text_from_page(page)
                logger.debug(f"ç¬¬ {page_num + 1} é¡µæ–‡æœ¬æå–å®Œæˆï¼Œé•¿åº¦: {len(text_content) if text_content else 0}")
                
                # æå–å›¾åƒ
                logger.debug(f"æå–ç¬¬ {page_num + 1} é¡µå›¾åƒ")
                images = self._extract_images_from_page(page, page_num)
                logger.debug(f"ç¬¬ {page_num + 1} é¡µå›¾åƒæå–å®Œæˆï¼Œæ•°é‡: {len(images)}")
                
                # æå–è¡¨æ ¼
                logger.debug(f"æå–ç¬¬ {page_num + 1} é¡µè¡¨æ ¼")
                tables = self._extract_tables_from_page(page, page_num)
                logger.debug(f"ç¬¬ {page_num + 1} é¡µè¡¨æ ¼æå–å®Œæˆï¼Œæ•°é‡: {len(tables)}")
                
                # å¤„ç†æå–çš„å†…å®¹
                if text_content:
                    chunks = self._create_text_chunks(text_content, file_id, page_num)
                    all_chunks.extend(chunks)
                
                if images:
                    image_chunks = self._process_images(images, file_id, page_num)
                    all_chunks.extend(image_chunks)
                
                if tables:
                    table_chunks = self._process_tables(tables, file_id, page_num)
                    all_chunks.extend(table_chunks)
            
            doc.close()
            logger.info(f"PDFæ–‡ä»¶å·²å…³é—­ï¼Œæ€»å…±æå–äº† {len(all_chunks)} ä¸ªå†…å®¹å—")
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            self.processing_status[file_id]["message"] = "ç”ŸæˆåµŒå…¥å‘é‡..."
            self.processing_status[file_id]["progress"] = 85
            logger.info(f"å¼€å§‹ç”Ÿæˆ {len(all_chunks)} ä¸ªå†…å®¹å—çš„åµŒå…¥å‘é‡")
            self._generate_embeddings(all_chunks)
            logger.info("åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆ")
            
            # æå–å®ä½“å’Œå…³ç³»
            self.processing_status[file_id]["message"] = "æå–å®ä½“å’Œå…³ç³»..."
            self.processing_status[file_id]["progress"] = 90
            logger.info("å¼€å§‹æå–å®ä½“å’Œå…³ç³»")
            entities, relations = self._extract_entities_and_relations(all_chunks)
            logger.info(f"å®ä½“å’Œå…³ç³»æå–å®Œæˆï¼Œå®ä½“æ•°é‡: {len(entities)}ï¼Œå…³ç³»æ•°é‡: {len(relations)}")
            
            # ä¿å­˜åˆ°å‘é‡æ•°æ®åº“
            self.processing_status[file_id]["message"] = "ä¿å­˜åˆ°å‘é‡æ•°æ®åº“..."
            self.processing_status[file_id]["progress"] = 95
            logger.info("å¼€å§‹ä¿å­˜åˆ°å‘é‡æ•°æ®åº“")
            self._save_to_vector_db(all_chunks)
            logger.info("å‘é‡æ•°æ®åº“ä¿å­˜å®Œæˆ")
            
            # ä¿å­˜åˆ°å›¾æ•°æ®åº“
            self.processing_status[file_id]["message"] = "ä¿å­˜åˆ°å›¾æ•°æ®åº“..."
            logger.info("å¼€å§‹ä¿å­˜åˆ°å›¾æ•°æ®åº“")
            self._save_to_graph_db(entities, relations, file_id)
            logger.info("å›¾æ•°æ®åº“ä¿å­˜å®Œæˆ")
            
            # å®Œæˆå¤„ç†
            self.processing_status[file_id] = {
                "status": "completed",
                "progress": 100,
                "message": "å¤„ç†å®Œæˆ"
            }
            
            mysql_manager.execute_update(
                "UPDATE files SET status = 'completed', processing_progress = 100 WHERE file_id = %s",
                (file_id,)
            )
            
            logger.info(f"æ–‡ä»¶ {file_id} å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {file_id} å¤±è´¥: {e}", exc_info=True)
            logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            logger.error(f"âŒ å¼‚å¸¸æ¶ˆæ¯: {str(e)}")
            
            # æ›´æ–°å¤„ç†çŠ¶æ€
            try:
                self.processing_status[file_id] = {
                    "status": "failed",
                    "progress": 0,
                    "message": f"å¤„ç†å¤±è´¥: {str(e)}"
                }
                logger.info(f"âœ… å†…å­˜ä¸­å¤„ç†çŠ¶æ€å·²æ›´æ–°ä¸ºå¤±è´¥")
            except Exception as status_error:
                logger.error(f"âŒ æ›´æ–°å†…å­˜å¤„ç†çŠ¶æ€å¤±è´¥: {status_error}")
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            try:
                mysql_manager.execute_update(
                    "UPDATE files SET status = 'failed' WHERE file_id = %s",
                    (file_id,)
                )
                logger.info(f"âœ… æ–‡ä»¶ {file_id} æ•°æ®åº“çŠ¶æ€å·²æ›´æ–°ä¸ºfailed")
            except Exception as db_error:
                logger.error(f"âŒ æ›´æ–°æ•°æ®åº“æ–‡ä»¶çŠ¶æ€å¤±è´¥: {db_error}")
        
        finally:
            logger.info(f"ğŸ æ–‡ä»¶å¤„ç†çº¿ç¨‹ç»“æŸ - æ–‡ä»¶ID: {file_id}")
    
    def _extract_text_from_page(self, page) -> str:
        """ä»é¡µé¢æå–æ–‡æœ¬"""
        return page.get_text()
    
    def _extract_images_from_page(self, page, page_num: int) -> List[Dict]:
        """ä»é¡µé¢æå–å›¾åƒ"""
        images = []
        image_list = page.get_images()
        
        for img_index, img in enumerate(image_list):
            xref = img[0]
            pix = fitz.Pixmap(page.parent, xref)
            
            if pix.n - pix.alpha < 4:  # ç¡®ä¿ä¸æ˜¯CMYK
                img_data = pix.tobytes("png")
                images.append({
                    "page": page_num,
                    "index": img_index,
                    "data": img_data,
                    "width": pix.width,
                    "height": pix.height
                })
            
            pix = None
        
        return images
    
    def _extract_tables_from_page(self, page, page_num: int) -> List[Dict]:
        """ä»é¡µé¢æå–è¡¨æ ¼"""
        tables = []
        
        try:
            # æ–¹æ³•1: å°è¯•ä½¿ç”¨PyMuPDFçš„è¡¨æ ¼æ£€æµ‹
            tabs = page.find_tables()
            
            if tabs:
                for i, tab in enumerate(tabs):
                    try:
                        # æå–è¡¨æ ¼æ•°æ®
                        table_data = tab.extract()
                        if table_data and len(table_data) > 1:  # è‡³å°‘æœ‰æ ‡é¢˜è¡Œå’Œä¸€è¡Œæ•°æ®
                            tables.append({
                                "page": page_num,
                                "table_index": i,
                                "data": table_data,
                                "bbox": tab.bbox,  # è¡¨æ ¼è¾¹ç•Œæ¡†
                                "method": "pymupdf"
                            })
                    except Exception as e:
                        logger.warning(f"PyMuPDFè¡¨æ ¼æå–å¤±è´¥: {e}")
            
            # æ–¹æ³•2: åŸºäºæ–‡æœ¬ä½ç½®çš„è¡¨æ ¼æ£€æµ‹ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
            if not tables:
                text_tables = self._detect_tables_by_text_position(page, page_num)
                tables.extend(text_tables)
            
            logger.info(f"é¡µé¢ {page_num + 1} æ£€æµ‹åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
            return tables
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼æå–å¤±è´¥: {e}")
            return []
    
    def _detect_tables_by_text_position(self, page, page_num: int) -> List[Dict]:
        """åŸºäºæ–‡æœ¬ä½ç½®æ£€æµ‹è¡¨æ ¼"""
        tables = []
        
        try:
            # è·å–è¯¦ç»†çš„æ–‡æœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½ç½®
            text_dict = page.get_text("dict")
            
            # åˆ†ææ–‡æœ¬å—ï¼Œå¯»æ‰¾è¡¨æ ¼æ¨¡å¼
            potential_table_blocks = []
            
            for block in text_dict["blocks"]:
                if "lines" in block:
                    lines = block["lines"]
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šåˆ—å¯¹é½çš„æ–‡æœ¬ï¼ˆè¡¨æ ¼ç‰¹å¾ï¼‰
                    line_positions = []
                    for line in lines:
                        if "spans" in line:
                            x_positions = []
                            for span in line["spans"]:
                                x_positions.append(span["bbox"][0])  # xåæ ‡
                            
                            if len(x_positions) > 1:  # å¤šåˆ—
                                line_positions.append(x_positions)
                    
                    # å¦‚æœæœ‰å¤šè¡Œéƒ½æœ‰ç›¸ä¼¼çš„åˆ—ä½ç½®ï¼Œå¯èƒ½æ˜¯è¡¨æ ¼
                    if len(line_positions) >= 3:  # è‡³å°‘3è¡Œ
                        is_table = self._is_aligned_table(line_positions)
                        if is_table:
                            table_data = self._extract_table_from_block(block)
                            if table_data:
                                potential_table_blocks.append({
                                    "page": page_num,
                                    "table_index": len(tables),
                                    "data": table_data,
                                    "bbox": block["bbox"],
                                    "method": "text_position"
                                })
            
            tables.extend(potential_table_blocks)
            return tables
            
        except Exception as e:
            logger.error(f"åŸºäºä½ç½®çš„è¡¨æ ¼æ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def _is_aligned_table(self, line_positions: List[List[float]], tolerance: float = 10.0) -> bool:
        """æ£€æŸ¥æ–‡æœ¬è¡Œæ˜¯å¦å‘ˆè¡¨æ ¼å¯¹é½"""
        if len(line_positions) < 3:
            return False
        
        # æ£€æŸ¥åˆ—ä½ç½®çš„ä¸€è‡´æ€§
        first_line_cols = len(line_positions[0])
        
        # æ£€æŸ¥æ¯è¡Œçš„åˆ—æ•°æ˜¯å¦ç›¸ä¼¼
        consistent_cols = 0
        for positions in line_positions:
            if abs(len(positions) - first_line_cols) <= 1:  # å…è®¸1åˆ—çš„å·®å¼‚
                consistent_cols += 1
        
        # å¦‚æœå¤§éƒ¨åˆ†è¡Œçš„åˆ—æ•°ä¸€è‡´ï¼Œè®¤ä¸ºæ˜¯è¡¨æ ¼
        return consistent_cols / len(line_positions) >= 0.7
    
    def _extract_table_from_block(self, block: Dict) -> List[List[str]]:
        """ä»æ–‡æœ¬å—ä¸­æå–è¡¨æ ¼æ•°æ®"""
        table_data = []
        
        try:
            lines = block.get("lines", [])
            
            for line in lines:
                row_data = []
                spans = line.get("spans", [])
                
                # æŒ‰xåæ ‡æ’åºspan
                spans_sorted = sorted(spans, key=lambda s: s["bbox"][0])
                
                for span in spans_sorted:
                    text = span.get("text", "").strip()
                    if text:
                        row_data.append(text)
                
                if row_data:
                    table_data.append(row_data)
            
            return table_data if len(table_data) >= 2 else []
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼æ•°æ®æå–å¤±è´¥: {e}")
            return []
    
    def _create_text_chunks(self, text: str, file_id: str, page_num: int) -> List[Dict]:
        """åˆ›å»ºæ–‡æœ¬å—"""
        chunk_size = config_loader.get_app_config()["graph_rag"]["chunk_size"]
        chunk_overlap = config_loader.get_app_config()["graph_rag"]["chunk_overlap"]
        
        chunks = []
        start = 0
        chunk_index = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk_text = text[start:end]
            
            if chunk_text.strip():
                chunk_id = f"{file_id}_page_{page_num}_chunk_{chunk_index}"
                chunks.append({
                    "chunk_id": chunk_id,
                    "file_id": file_id,
                    "content": chunk_text.strip(),
                    "content_type": "text",
                    "page": page_num,
                    "metadata": {
                        "page": page_num,
                        "chunk_index": chunk_index,
                        "start_pos": start,
                        "end_pos": end
                    }
                })
                chunk_index += 1
            
            start = end - chunk_overlap
        
        return chunks
    
    def _process_images(self, images: List[Dict], file_id: str, page_num: int) -> List[Dict]:
        """å¤„ç†å›¾åƒï¼Œæå–æè¿°ä¿¡æ¯"""
        chunks = []
        
        for i, image in enumerate(images):
            try:
                # ä¿å­˜ä¸´æ—¶å›¾åƒæ–‡ä»¶
                temp_image_path = f"temp_image_{file_id}_{page_num}_{i}.png"
                with open(temp_image_path, "wb") as f:
                    f.write(image["data"])
                
                # ä½¿ç”¨OCRæå–æ–‡æœ¬
                ocr_results = model_manager.extract_text_from_image(temp_image_path)
                
                # ç”Ÿæˆå›¾åƒæè¿°ï¼ˆè¿™é‡Œéœ€è¦ä½¿ç”¨å›¾åƒåˆ†ææ¨¡å‹ï¼‰
                image_description = self._generate_image_description(temp_image_path)
                
                # åˆ›å»ºå›¾åƒå—
                chunk_id = f"{file_id}_page_{page_num}_image_{i}"
                content = f"å›¾åƒæè¿°: {image_description}\n"
                
                if ocr_results:
                    ocr_text = " ".join([result["text"] for result in ocr_results])
                    content += f"å›¾åƒä¸­çš„æ–‡å­—: {ocr_text}"
                
                chunks.append({
                    "chunk_id": chunk_id,
                    "file_id": file_id,
                    "content": content,
                    "content_type": "image",
                    "page": page_num,
                    "metadata": {
                        "page": page_num,
                        "image_index": i,
                        "width": image["width"],
                        "height": image["height"],
                        "ocr_results": ocr_results
                    }
                })
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                os.remove(temp_image_path)
                
            except Exception as e:
                logger.error(f"å¤„ç†å›¾åƒå¤±è´¥: {e}")
        
        return chunks
    
    def _process_tables(self, tables: List[Dict], file_id: str, page_num: int) -> List[Dict]:
        """å¤„ç†è¡¨æ ¼æ•°æ®"""
        chunks = []
        
        for i, table in enumerate(tables):
            try:
                # å¤„ç†è¡¨æ ¼å†…å®¹ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„è¡¨æ ¼æ•°æ®ç»“æ„å®ç°ï¼‰
                table_content = self._format_table_content(table)
                
                chunk_id = f"{file_id}_page_{page_num}_table_{i}"
                chunks.append({
                    "chunk_id": chunk_id,
                    "file_id": file_id,
                    "content": table_content,
                    "content_type": "table",
                    "page": page_num,
                    "metadata": {
                        "page": page_num,
                        "table_index": i,
                        "table_data": table
                    }
                })
                
            except Exception as e:
                logger.error(f"å¤„ç†è¡¨æ ¼å¤±è´¥: {e}")
        
        return chunks
    
    def _generate_image_description(self, image_path: str) -> str:
        """ç”Ÿæˆå›¾åƒæè¿°"""
        try:
            # ä½¿ç”¨DeepSeekå¤šæ¨¡æ€èƒ½åŠ›æˆ–OCRç»“æœåˆ†æå›¾åƒå†…å®¹
            # é¦–å…ˆå°è¯•è·å–å›¾åƒçš„åŸºæœ¬ä¿¡æ¯
            with Image.open(image_path) as img:
                width, height = img.size
                mode = img.mode
                format_name = img.format
            
            # æ„å»ºå›¾åƒåˆ†ææç¤º
            basic_info = f"å›¾åƒå°ºå¯¸: {width}x{height}, é¢œè‰²æ¨¡å¼: {mode}, æ ¼å¼: {format_name}"
            
            # å°è¯•ä½¿ç”¨OCRæå–æ–‡æœ¬ï¼ˆå¦‚æœå›¾åƒåŒ…å«æ–‡å­—ï¼‰
            ocr_text = ""
            try:
                ocr_results = model_manager.extract_text_from_image(image_path)
                if ocr_results:
                    ocr_texts = [result.get("text", "") for result in ocr_results]
                    ocr_text = " ".join(ocr_texts).strip()
            except Exception as e:
                logger.warning(f"OCRå¤„ç†å¤±è´¥: {e}")
            
            # åŸºäºOCRç»“æœå’Œå›¾åƒä¿¡æ¯ç”Ÿæˆæè¿°
            description_parts = []
            description_parts.append(basic_info)
            
            if ocr_text:
                description_parts.append(f"å›¾åƒä¸­åŒ…å«æ–‡å­—å†…å®¹: {ocr_text[:200]}")  # é™åˆ¶é•¿åº¦
                
                # åŸºäºæ–‡å­—å†…å®¹åˆ¤æ–­å›¾åƒç±»å‹
                if any(keyword in ocr_text.lower() for keyword in ['chart', 'å›¾è¡¨', 'æ•°æ®', 'ç™¾åˆ†æ¯”', '%']):
                    description_parts.append("å›¾åƒç±»å‹: å¯èƒ½æ˜¯æ•°æ®å›¾è¡¨æˆ–ç»Ÿè®¡å›¾")
                elif any(keyword in ocr_text.lower() for keyword in ['title', 'æ ‡é¢˜', 'ç« èŠ‚']):
                    description_parts.append("å›¾åƒç±»å‹: å¯èƒ½æ˜¯æ–‡æ¡£æ ‡é¢˜æˆ–ç« èŠ‚é¡µé¢")
                elif len(ocr_text.split()) > 20:
                    description_parts.append("å›¾åƒç±»å‹: åŒ…å«å¤§é‡æ–‡å­—çš„æ–‡æ¡£å›¾åƒ")
                else:
                    description_parts.append("å›¾åƒç±»å‹: åŒ…å«å°‘é‡æ–‡å­—çš„å›¾åƒ")
            else:
                # åŸºäºå›¾åƒå°ºå¯¸å’Œæ¯”ä¾‹è¿›è¡Œç®€å•åˆ¤æ–­
                aspect_ratio = width / height if height > 0 else 1
                if 0.8 <= aspect_ratio <= 1.2:
                    description_parts.append("å›¾åƒç±»å‹: æ¥è¿‘æ­£æ–¹å½¢ï¼Œå¯èƒ½æ˜¯å›¾æ ‡ã€logoæˆ–ç¤ºæ„å›¾")
                elif aspect_ratio > 2:
                    description_parts.append("å›¾åƒç±»å‹: å®½å¹…å›¾åƒï¼Œå¯èƒ½æ˜¯æ¨ªå‘å›¾è¡¨ã€æ—¶é—´çº¿æˆ–æµç¨‹å›¾")
                elif aspect_ratio < 0.5:
                    description_parts.append("å›¾åƒç±»å‹: ç«–å¹…å›¾åƒï¼Œå¯èƒ½æ˜¯ç«–å‘åˆ—è¡¨æˆ–çºµå‘å›¾è¡¨")
                else:
                    description_parts.append("å›¾åƒç±»å‹: å¸¸è§„æ¯”ä¾‹å›¾åƒï¼Œå¯èƒ½æ˜¯ç…§ç‰‡ã€æ’å›¾æˆ–æ··åˆå†…å®¹")
            
            # å°è¯•ä½¿ç”¨LLMè¿›è¡Œæ›´è¯¦ç»†çš„åˆ†æï¼ˆå¦‚æœé…ç½®äº†ç›¸åº”æç¤ºè¯ï¼‰
            try:
                enhanced_description = self._analyze_image_with_llm(description_parts, ocr_text)
                if enhanced_description:
                    description_parts.append(f"æ™ºèƒ½åˆ†æ: {enhanced_description}")
            except Exception as e:
                logger.warning(f"LLMå›¾åƒåˆ†æå¤±è´¥: {e}")
            
            return "\n".join(description_parts)
            
        except Exception as e:
            logger.error(f"å›¾åƒæè¿°ç”Ÿæˆå¤±è´¥: {e}")
            return f"å›¾åƒæè¿°ç”Ÿæˆå¤±è´¥: æ— æ³•åˆ†æå›¾åƒ {image_path}"
    
    def _analyze_image_with_llm(self, image_info: List[str], ocr_text: str) -> str:
        """ä½¿ç”¨LLMåˆ†æå›¾åƒå†…å®¹"""
        try:
            # æ„å»ºåˆ†ææç¤º
            image_context = "\n".join(image_info)
            
            if "image_analysis" in self.prompt_config and "image_description" in self.prompt_config["image_analysis"]:
                prompt_template = self.prompt_config["image_analysis"]["image_description"]
                
                # è‡ªå®šä¹‰æç¤ºå†…å®¹
                analysis_prompt = f"""
åŸºäºä»¥ä¸‹å›¾åƒä¿¡æ¯ï¼Œè¯·æä¾›ä¸€ä¸ªè¯¦ç»†çš„å›¾åƒæè¿°å’Œåˆ†æï¼š

å›¾åƒåŸºæœ¬ä¿¡æ¯ï¼š
{image_context}

OCRæå–çš„æ–‡å­—å†…å®¹ï¼š
{ocr_text if ocr_text else "æ— æ–‡å­—å†…å®¹"}

è¯·åˆ†æï¼š
1. å›¾åƒçš„ä¸»è¦å†…å®¹å’Œç”¨é€”
2. å¦‚æœæ˜¯å›¾è¡¨ï¼Œæè¿°æ•°æ®ç±»å‹å’Œè¶‹åŠ¿
3. å¦‚æœæ˜¯æ–‡æ¡£ï¼Œæè¿°å¸ƒå±€å’Œç»“æ„
4. å›¾åƒåœ¨æ–‡æ¡£ä¸­å¯èƒ½çš„ä½œç”¨å’Œæ„ä¹‰

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒç®€æ´ä½†åŒ…å«å…³é”®ä¿¡æ¯ã€‚
"""
                
                response = self._call_llm(analysis_prompt)
                return response.strip() if response else ""
            
        except Exception as e:
            logger.error(f"LLMå›¾åƒåˆ†æå¤±è´¥: {e}")
            
        return ""
    
    def _format_table_content(self, table: Dict) -> str:
        """æ ¼å¼åŒ–è¡¨æ ¼å†…å®¹"""
        try:
            table_data = table.get("data", [])
            if not table_data:
                return "ç©ºè¡¨æ ¼"
            
            # æ„å»ºè¡¨æ ¼æ–‡æœ¬è¡¨ç¤º
            formatted_content = []
            
            # æ·»åŠ è¡¨æ ¼å…ƒä¿¡æ¯
            formatted_content.append(f"è¡¨æ ¼ä½ç½®: ç¬¬{table.get('page', 0) + 1}é¡µ")
            formatted_content.append(f"æ£€æµ‹æ–¹æ³•: {table.get('method', 'unknown')}")
            formatted_content.append(f"è¡¨æ ¼å¤§å°: {len(table_data)}è¡Œ x {len(table_data[0]) if table_data else 0}åˆ—")
            formatted_content.append("")
            
            # æ ¼å¼åŒ–è¡¨æ ¼æ•°æ®
            if len(table_data) > 0:
                # è¡¨å¤´
                headers = table_data[0]
                formatted_content.append("è¡¨å¤´: " + " | ".join(str(cell) for cell in headers))
                formatted_content.append("-" * 50)
                
                # æ•°æ®è¡Œ
                for i, row in enumerate(table_data[1:], 1):
                    if i <= 10:  # åªæ˜¾ç¤ºå‰10è¡Œæ•°æ®ï¼Œé¿å…å†…å®¹è¿‡é•¿
                        row_text = " | ".join(str(cell) for cell in row)
                        formatted_content.append(f"ç¬¬{i}è¡Œ: {row_text}")
                    elif i == 11:
                        formatted_content.append(f"... (è¿˜æœ‰{len(table_data) - 11}è¡Œæ•°æ®)")
                        break
            
            # ä½¿ç”¨LLMç”Ÿæˆè¡¨æ ¼æ‘˜è¦
            table_text = "\n".join(formatted_content)
            summary = self._generate_table_summary(table_text)
            
            final_content = []
            final_content.append("=== è¡¨æ ¼æ‘˜è¦ ===")
            final_content.append(summary)
            final_content.append("")
            final_content.append("=== è¡¨æ ¼è¯¦ç»†å†…å®¹ ===")
            final_content.extend(formatted_content)
            
            return "\n".join(final_content)
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼å†…å®¹æ ¼å¼åŒ–å¤±è´¥: {e}")
            return f"è¡¨æ ¼æ ¼å¼åŒ–å¤±è´¥: {str(e)}"
    
    def _generate_table_summary(self, table_content: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆè¡¨æ ¼æ‘˜è¦"""
        try:
            prompt_template = self.prompt_config["table_analysis"]["table_summary"]
            prompt = prompt_template.format(table_content=table_content)
            
            response = self._call_llm(prompt)
            return response.strip() if response else "æ— æ³•ç”Ÿæˆè¡¨æ ¼æ‘˜è¦"
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return "è¡¨æ ¼æ‘˜è¦ç”Ÿæˆå¤±è´¥"
    
    def _generate_embeddings(self, chunks: List[Dict]) -> None:
        """ä¸ºæ–‡æœ¬å—ç”ŸæˆåµŒå…¥å‘é‡"""
        texts = [chunk["content"] for chunk in chunks]
        embeddings = model_manager.get_embedding(texts)
        
        for chunk, embedding in zip(chunks, embeddings):
            chunk["embedding"] = embedding
    
    def _extract_entities_and_relations(self, chunks: List[Dict]) -> tuple:
        """æå–å®ä½“å’Œå…³ç³»"""
        entities = []
        relations = []
        
        try:
            # åˆå¹¶æ–‡æœ¬å†…å®¹è¿›è¡Œæ‰¹é‡å¤„ç†
            text_chunks = [chunk for chunk in chunks if chunk["content_type"] == "text"]
            
            # æ‰¹é‡å¤„ç†æ–‡æœ¬å—ä»¥æé«˜æ•ˆç‡
            batch_size = 5
            for i in range(0, len(text_chunks), batch_size):
                batch = text_chunks[i:i+batch_size]
                combined_text = "\n\n".join([chunk["content"] for chunk in batch])
                
                # æå–å®ä½“
                batch_entities = self._extract_entities(combined_text)
                
                # æå–å…³ç³»
                if batch_entities:
                    batch_relations = self._extract_relations(combined_text, batch_entities)
                    relations.extend(batch_relations)
                
                # ä¸ºå®ä½“æ·»åŠ chunkä¿¡æ¯
                for entity in batch_entities:
                    entity["chunks"] = [chunk["chunk_id"] for chunk in batch]
                    entity["file_id"] = batch[0]["file_id"] if batch else None
                
                entities.extend(batch_entities)
                
            logger.info(f"æå–åˆ° {len(entities)} ä¸ªå®ä½“ï¼Œ{len(relations)} ä¸ªå…³ç³»")
            return entities, relations
            
        except Exception as e:
            logger.error(f"å®ä½“å…³ç³»æå–å¤±è´¥: {e}")
            return [], []
    
    def _save_to_vector_db(self, chunks: List[Dict]) -> None:
        """ä¿å­˜åˆ°å‘é‡æ•°æ®åº“"""
        try:
            # ç¡®ä¿Milvusè¿æ¥å’Œé›†åˆåˆå§‹åŒ–
            if not milvus_manager.collection:
                logger.info("Milvusé›†åˆæœªåˆå§‹åŒ–ï¼Œé‡æ–°è¿æ¥...")
                milvus_manager.connect()
            
            vector_data = []
            
            for chunk in chunks:
                vector_data.append({
                    "file_id": chunk["file_id"],
                    "chunk_id": chunk["chunk_id"],
                    "content": chunk["content"],
                    "embedding": chunk["embedding"],
                    "metadata": json.dumps(chunk["metadata"])
                })
            
            if vector_data:
                milvus_manager.insert_vectors(vector_data)
                logger.info(f"æˆåŠŸä¿å­˜ {len(vector_data)} ä¸ªå‘é‡åˆ°Milvus")
                
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            raise
    
    def _save_to_graph_db(self, entities: List[Dict], relations: List[Dict], file_id: str) -> None:
        """ä¿å­˜åˆ°å›¾æ•°æ®åº“"""
        # åˆ›å»ºå®ä½“èŠ‚ç‚¹
        for entity in entities:
            entity["file_id"] = file_id
            neo4j_manager.create_entity(entity["type"], entity)
        
        # åˆ›å»ºå…³ç³»
        for relation in relations:
            neo4j_manager.create_relationship(
                relation["subject"],
                relation["object"],
                relation["predicate"],
                {"confidence": relation.get("confidence", 1.0), "file_id": file_id}
            )
    
    def _extract_entities(self, text: str) -> List[Dict]:
        """ä½¿ç”¨LLMæå–å®ä½“"""
        try:
            prompt_template = self.prompt_config["document_parsing"]["entity_extraction"]
            prompt = prompt_template.format(text=text)
            
            response = self._call_llm(prompt)
            
            # è§£æJSONå“åº”
            try:
                # æ¸…ç†å“åº”å­—ç¬¦ä¸²ï¼Œç§»é™¤å¯èƒ½çš„å‰ç¼€å’Œåç¼€
                cleaned_response = response.strip()
                if cleaned_response.startswith('```json'):
                    cleaned_response = cleaned_response[7:]
                if cleaned_response.endswith('```'):
                    cleaned_response = cleaned_response[:-3]
                cleaned_response = cleaned_response.strip()
                
                result = json.loads(cleaned_response)
                entities = result.get("entities", [])
                
                # æ ‡å‡†åŒ–å®ä½“æ ¼å¼
                standardized_entities = []
                for entity in entities:
                    standardized_entities.append({
                        "name": entity.get("name", ""),
                        "type": entity.get("type", "UNKNOWN"),
                        "position": entity.get("position", ""),
                        "confidence": 0.8  # é»˜è®¤ç½®ä¿¡åº¦
                    })
                
                return standardized_entities
                
            except json.JSONDecodeError as e:
                logger.warning(f"LLMè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆJSONæ ¼å¼: {str(e)[:100]}ï¼Œå°è¯•è§£ææ–‡æœ¬")
                logger.debug(f"åŸå§‹å“åº”: {response[:200]}...")
                return self._parse_entities_from_text(response)
                
        except Exception as e:
            logger.error(f"å®ä½“æå–å¤±è´¥: {e}")
            return []
    
    def _extract_relations(self, text: str, entities: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨LLMæå–å…³ç³»"""
        try:
            entities_str = "\n".join([f"- {entity['name']} ({entity['type']})" for entity in entities])
            prompt_template = self.prompt_config["document_parsing"]["relation_extraction"]
            prompt = prompt_template.format(text=text, entities=entities_str)
            
            response = self._call_llm(prompt)
            
            # è§£æJSONå“åº”
            try:
                # æ¸…ç†å“åº”å­—ç¬¦ä¸²
                cleaned_response = response.strip()
                if cleaned_response.startswith('```json'):
                    cleaned_response = cleaned_response[7:]
                if cleaned_response.endswith('```'):
                    cleaned_response = cleaned_response[:-3]
                cleaned_response = cleaned_response.strip()
                
                result = json.loads(cleaned_response)
                relations = result.get("relations", [])
                
                # æ ‡å‡†åŒ–å…³ç³»æ ¼å¼
                standardized_relations = []
                for relation in relations:
                    standardized_relations.append({
                        "subject": relation.get("subject", ""),
                        "predicate": relation.get("predicate", "RELATED_TO"),
                        "object": relation.get("object", ""),
                        "confidence": relation.get("confidence", 0.8)
                    })
                
                return standardized_relations
                
            except json.JSONDecodeError as e:
                logger.warning(f"LLMè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆJSONæ ¼å¼: {str(e)[:100]}ï¼Œå°è¯•è§£ææ–‡æœ¬")
                logger.debug(f"åŸå§‹å“åº”: {response[:200]}...")
                return self._parse_relations_from_text(response)
                
        except Exception as e:
            logger.error(f"å…³ç³»æå–å¤±è´¥: {e}")
            return []
    
    def _parse_entities_from_text(self, text: str) -> List[Dict]:
        """ä»æ–‡æœ¬ä¸­è§£æå®ä½“ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        entities = []
        lines = text.strip().split('\n')
        
        for line in lines:
            if '(' in line and ')' in line:
                match = re.match(r'.*?([^(]+)\s*\(([^)]+)\)', line)
                if match:
                    name = match.group(1).strip()
                    entity_type = match.group(2).strip()
                    if name and entity_type:
                        entities.append({
                            "name": name,
                            "type": entity_type,
                            "position": "",
                            "confidence": 0.7
                        })
        
        return entities
    
    def _parse_relations_from_text(self, text: str) -> List[Dict]:
        """ä»æ–‡æœ¬ä¸­è§£æå…³ç³»ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        relations = []
        lines = text.strip().split('\n')
        
        for line in lines:
            # æŸ¥æ‰¾å½¢å¦‚ "A å…³ç³» B" çš„æ¨¡å¼
            if '->' in line or 'â†’' in line or ' ä¸ ' in line:
                parts = re.split(r'[-â†’]|ä¸', line)
                if len(parts) >= 2:
                    subject = parts[0].strip()
                    obj = parts[-1].strip()
                    predicate = "RELATED_TO"
                    
                    if subject and obj:
                        relations.append({
                            "subject": subject,
                            "predicate": predicate,
                            "object": obj,
                            "confidence": 0.6
                        })
        
        return relations
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨DeepSeek LLM"""
        try:
            llm_config = self.model_config["llm"]
            
            headers = {
                "Authorization": f"Bearer {llm_config['api_key']}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": llm_config["model_name"],
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": llm_config["max_tokens"],
                "temperature": llm_config["temperature"]
            }
            
            response = requests.post(
                f"{llm_config['api_url']}/chat/completions",
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                logger.error(f"LLM APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return "{}"
                
        except Exception as e:
            logger.error(f"è°ƒç”¨LLMå¤±è´¥: {e}")
            return "{}"

# å…¨å±€æ–‡ä»¶æœåŠ¡å®ä¾‹
file_service = FileService() 