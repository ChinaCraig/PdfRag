"""
æ–‡ä»¶ç®¡ç†æœåŠ¡ - é‡æ„ç‰ˆ
è´Ÿè´£PDFæ–‡ä»¶çš„ä¸Šä¼ ã€åˆ é™¤ã€é‡å‘½åä»¥åŠGraphRAGå†…å®¹è§£æ
ç®€åŒ–å¤„ç†æµç¨‹ï¼Œæé«˜å¯é æ€§
"""
import os
import uuid
import logging
import json
import threading
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
import fitz  # PyMuPDF
from PIL import Image
import io
import requests
import re

from utils.config_loader import config_loader
from utils.database import mysql_manager, milvus_manager, neo4j_manager
from utils.model_manager import model_manager

logger = logging.getLogger(__name__)

class FileService:
    """æ–‡ä»¶ç®¡ç†æœåŠ¡ç±» - é‡æ„ç‰ˆ"""
    
    def __init__(self):
        self.config = config_loader.get_app_config()
        self.model_config = config_loader.get_model_config()
        self.prompt_config = config_loader.get_prompt_config()
        self.upload_dir = self.config["upload"]["upload_dir"]
        self.allowed_extensions = set(self.config["upload"]["allowed_extensions"])
        self.max_file_size = self.config["upload"]["max_file_size"] * 1024 * 1024  # MB to bytes
        self.processing_status = {}  # æ–‡ä»¶å¤„ç†çŠ¶æ€
        
        # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
        os.makedirs(self.upload_dir, exist_ok=True)
        
        logger.info("æ–‡ä»¶æœåŠ¡åˆå§‹åŒ–å®Œæˆ - GraphRAGé‡æ„ç‰ˆ")
    
    def upload_file(self, file, filename: str, original_filename: str = None) -> Dict[str, Any]:
        """
        ä¸Šä¼ PDFæ–‡ä»¶
        
        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            filename: å¤„ç†åçš„æ–‡ä»¶åï¼ˆç”¨äºç±»å‹æ£€æŸ¥ï¼‰
            original_filename: åŸå§‹æ–‡ä»¶åï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
            
        Returns:
            ä¸Šä¼ ç»“æœ
        """
        try:
            # ä½¿ç”¨åŸå§‹æ–‡ä»¶åæˆ–å¤„ç†åçš„æ–‡ä»¶åè¿›è¡Œç±»å‹æ£€æŸ¥
            check_filename = original_filename or filename
            display_filename = original_filename or filename
            
            logger.info(f"ğŸ“ å¼€å§‹ä¸Šä¼ æ–‡ä»¶: {display_filename}")
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            if not self._allowed_file(check_filename):
                logger.warning(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {check_filename}")
                return {
                    "success": False,
                    "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œä»…æ”¯æŒ: {', '.join(self.allowed_extensions)}"
                }
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
            file_size = file.tell()
            file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
            
            if file_size > self.max_file_size:
                return {
                    "success": False,
                    "message": f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ ({self.max_file_size // 1024 // 1024}MB)"
                }
            
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶IDå’Œä¿å­˜è·¯å¾„
            file_id = str(uuid.uuid4())
            
            # ç”Ÿæˆå®‰å…¨çš„å­˜å‚¨æ–‡ä»¶å
            import time
            timestamp = str(int(time.time()))
            safe_filename = f"{file_id}_{timestamp}.pdf"
            file_path = os.path.join(self.upload_dir, safe_filename)
            
            logger.info(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶: {file_path}")
            
            # ä¿å­˜æ–‡ä»¶
            file.save(file_path)
            
            # ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“
            file_info = {
                "file_id": file_id,
                "original_filename": display_filename,
                "filename": safe_filename,
                "file_path": file_path,
                "file_size": file_size,
                "upload_time": datetime.now(),
                "status": "uploaded",
                "processing_progress": 0
            }
            
            logger.info(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“...")
            self._save_file_info(file_info)
            logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {display_filename}")
            
            # å¼‚æ­¥å¼€å§‹GraphRAGå¤„ç†
            logger.info(f"ğŸš€ å¯åŠ¨GraphRAGå¤„ç†çº¿ç¨‹...")
            self._start_graphrag_processing(file_id, file_path)
            
            return {
                "success": True,
                "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ŒGraphRAGå¤„ç†å·²å¼€å§‹",
                "file_id": file_id,
                "filename": display_filename
            }
            
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "message": f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
            }
    
    def _start_graphrag_processing(self, file_id: str, file_path: str) -> None:
        """å¯åŠ¨GraphRAGå¤„ç†çº¿ç¨‹"""
        try:
            thread = threading.Thread(
                target=self._process_graphrag,
                args=(file_id, file_path),
                name=f"GraphRAG-{file_id[:8]}",
                daemon=True
            )
            thread.start()
            logger.info(f"âœ… GraphRAGå¤„ç†çº¿ç¨‹å·²å¯åŠ¨: {file_id}")
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨GraphRAGå¤„ç†çº¿ç¨‹å¤±è´¥: {e}")
            self._update_file_status(file_id, "failed", 0, f"å¯åŠ¨å¤„ç†å¤±è´¥: {str(e)}")
    
    def _process_graphrag(self, file_id: str, file_path: str) -> None:
        """
        GraphRAGå¤„ç†ä¸»æµç¨‹
        
        æµç¨‹ï¼š
        1. PDFè§£æå’Œå†…å®¹æå–
        2. ç”ŸæˆåµŒå…¥å‘é‡
        3. å®ä½“å…³ç³»æå–
        4. ä¿å­˜åˆ°å‘é‡æ•°æ®åº“å’Œå›¾æ•°æ®åº“
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹GraphRAGå¤„ç†: {file_id}")
            
            # æ›´æ–°çŠ¶æ€
            self._update_file_status(file_id, "processing", 5, "å¼€å§‹å¤„ç†...")
            
            # ç¬¬ä¸€æ­¥ï¼šPDFå†…å®¹æå–
            logger.info(f"ğŸ“– æ­¥éª¤1: PDFå†…å®¹æå–")
            chunks = self._extract_pdf_content(file_id, file_path)
            self._update_file_status(file_id, "processing", 40, f"å†…å®¹æå–å®Œæˆï¼Œå…±{len(chunks)}ä¸ªå—")
            
            if not chunks:
                raise ValueError("PDFå†…å®¹æå–å¤±è´¥ï¼Œæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆå†…å®¹")
            
            # ç¬¬äºŒæ­¥ï¼šç”ŸæˆåµŒå…¥å‘é‡
            logger.info(f"ğŸ”¤ æ­¥éª¤2: ç”ŸæˆåµŒå…¥å‘é‡")
            self._generate_embeddings_for_chunks(chunks)
            self._update_file_status(file_id, "processing", 60, "åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆ")
            
            # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜åˆ°å‘é‡æ•°æ®åº“
            logger.info(f"ğŸ’¾ æ­¥éª¤3: ä¿å­˜åˆ°å‘é‡æ•°æ®åº“")
            self._save_chunks_to_vector_db(chunks)
            self._update_file_status(file_id, "processing", 75, "å‘é‡æ•°æ®ä¿å­˜å®Œæˆ")
            
            # ç¬¬å››æ­¥ï¼šå®ä½“å…³ç³»æå–
            logger.info(f"ğŸ§  æ­¥éª¤4: å®ä½“å…³ç³»æå–")
            entities, relations = self._extract_knowledge_graph(chunks)
            self._update_file_status(file_id, "processing", 90, f"çŸ¥è¯†å›¾è°±æå–å®Œæˆï¼š{len(entities)}ä¸ªå®ä½“ï¼Œ{len(relations)}ä¸ªå…³ç³»")
            
            # ç¬¬äº”æ­¥ï¼šä¿å­˜åˆ°å›¾æ•°æ®åº“
            logger.info(f"ğŸ•¸ï¸ æ­¥éª¤5: ä¿å­˜åˆ°å›¾æ•°æ®åº“")
            self._save_knowledge_graph(entities, relations, file_id)
            
            # æ›´æ–°æœ€ç»ˆçŠ¶æ€
            self._update_file_status(file_id, "completed", 100, "GraphRAGå¤„ç†å®Œæˆ")
            logger.info(f"âœ… GraphRAGå¤„ç†å®Œæˆ: {file_id}")
            
        except Exception as e:
            logger.error(f"âŒ GraphRAGå¤„ç†å¤±è´¥: {file_id}, é”™è¯¯: {e}", exc_info=True)
            self._update_file_status(file_id, "failed", 0, f"å¤„ç†å¤±è´¥: {str(e)}")
    
    def _extract_pdf_content(self, file_id: str, file_path: str) -> List[Dict[str, Any]]:
        """
        æå–PDFå†…å®¹ï¼šæ–‡å­—ã€è¡¨æ ¼ã€å›¾ç‰‡ã€å›¾è¡¨
        
        Returns:
            å†…å®¹å—åˆ—è¡¨
        """
        all_chunks = []
        
        try:
            # æ‰“å¼€PDFæ–‡æ¡£
            doc = fitz.open(file_path)
            total_pages = len(doc)
            logger.info(f"ğŸ“„ PDFå…±{total_pages}é¡µ")
            
            # é€é¡µå¤„ç†
            for page_num in range(total_pages):
                logger.info(f"ğŸ“– å¤„ç†ç¬¬{page_num + 1}/{total_pages}é¡µ")
                page = doc[page_num]
                
                # æå–æ–‡æœ¬
                text_chunks = self._extract_text_from_page(page, file_id, page_num)
                all_chunks.extend(text_chunks)
                
                # æå–å›¾åƒ
                image_chunks = self._extract_images_from_page(page, file_id, page_num)
                all_chunks.extend(image_chunks)
                
                # æå–è¡¨æ ¼
                table_chunks = self._extract_tables_from_page(page, file_id, page_num)
                all_chunks.extend(table_chunks)
                
                # æ›´æ–°è¿›åº¦
                progress = 5 + int((page_num + 1) / total_pages * 35)
                self._update_file_status(file_id, "processing", progress, f"å·²å¤„ç†{page_num + 1}/{total_pages}é¡µ")
            
            doc.close()
            logger.info(f"âœ… PDFå†…å®¹æå–å®Œæˆï¼Œå…±{len(all_chunks)}ä¸ªå†…å®¹å—")
            return all_chunks
            
        except Exception as e:
            logger.error(f"âŒ PDFå†…å®¹æå–å¤±è´¥: {e}")
            raise
    
    def _extract_text_from_page(self, page, file_id: str, page_num: int) -> List[Dict[str, Any]]:
        """ä»é¡µé¢æå–æ–‡æœ¬å—"""
        chunks = []
        
        try:
            # è·å–é¡µé¢æ–‡æœ¬
            text = page.get_text()
            if not text.strip():
                return chunks
            
            # åˆ†å—è®¾ç½®
            chunk_size = self.config.get("graph_rag", {}).get("chunk_size", 1000)
            chunk_overlap = self.config.get("graph_rag", {}).get("chunk_overlap", 200)
            
            # åˆ†å‰²æ–‡æœ¬
            start = 0
            chunk_index = 0
            
            while start < len(text):
                end = start + chunk_size
                chunk_text = text[start:end].strip()
                
                if chunk_text:
                    chunk_id = f"{file_id}_page_{page_num}_text_{chunk_index}"
                    chunks.append({
                        "chunk_id": chunk_id,
                        "file_id": file_id,
                        "content": chunk_text,
                        "content_type": "text",
                        "page_number": page_num,
                        "chunk_index": chunk_index,
                        "start_position": start,
                        "end_position": min(end, len(text)),
                        "metadata": {
                            "page": page_num,
                            "type": "text",
                            "length": len(chunk_text)
                        }
                    })
                    chunk_index += 1
                
                start = end - chunk_overlap
                if start >= len(text):
                    break
            
            logger.debug(f"ğŸ“ ç¬¬{page_num + 1}é¡µæå–{len(chunks)}ä¸ªæ–‡æœ¬å—")
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬æå–å¤±è´¥: {e}")
            return []
    
    def _extract_images_from_page(self, page, file_id: str, page_num: int) -> List[Dict[str, Any]]:
        """ä»é¡µé¢æå–å›¾åƒ"""
        chunks = []
        
        try:
            image_list = page.get_images()
            logger.debug(f"ğŸ“· ç¬¬{page_num + 1}é¡µå‘ç°{len(image_list)}ä¸ªå›¾åƒ")
            
            # å¦‚æœå›¾åƒå¤ªå¤šï¼Œé™åˆ¶å¤„ç†æ•°é‡ä»¥é¿å…å¡ä½
            max_images_per_page = 5
            if len(image_list) > max_images_per_page:
                logger.warning(f"âš ï¸ ç¬¬{page_num + 1}é¡µå›¾åƒè¿‡å¤š({len(image_list)}ä¸ª)ï¼Œä»…å¤„ç†å‰{max_images_per_page}ä¸ª")
                image_list = image_list[:max_images_per_page]
            
            for img_index, img in enumerate(image_list):
                try:
                    logger.debug(f"ğŸ“· å¤„ç†ç¬¬{page_num + 1}é¡µç¬¬{img_index + 1}ä¸ªå›¾åƒ")
                    
                    # æå–å›¾åƒæ•°æ®
                    xref = img[0]
                    pix = fitz.Pixmap(page.parent, xref)
                    
                    if pix.n - pix.alpha < 4:  # ç¡®ä¿ä¸æ˜¯CMYK
                        # å›¾åƒæè¿°ï¼ˆä½¿ç”¨OCRï¼Œå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
                        description = self._analyze_image_simple(pix, file_id, page_num, img_index)
                        
                        chunk_id = f"{file_id}_page_{page_num}_image_{img_index}"
                        chunks.append({
                            "chunk_id": chunk_id,
                            "file_id": file_id,
                            "content": description,
                            "content_type": "image",
                            "page_number": page_num,
                            "chunk_index": img_index,
                            "metadata": {
                                "page": page_num,
                                "type": "image",
                                "width": pix.width,
                                "height": pix.height,
                                "format": img[8] if len(img) > 8 else "unknown"
                            }
                        })
                        logger.debug(f"âœ… ç¬¬{page_num + 1}é¡µç¬¬{img_index + 1}ä¸ªå›¾åƒå¤„ç†å®Œæˆ")
                    
                    pix = None  # é‡Šæ”¾å†…å­˜
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ç¬¬{page_num + 1}é¡µç¬¬{img_index + 1}ä¸ªå›¾åƒå¤„ç†å¤±è´¥: {e}")
                    continue
            
            logger.debug(f"ğŸ“· ç¬¬{page_num + 1}é¡µæå–{len(chunks)}ä¸ªå›¾åƒå—")
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ å›¾åƒæå–å¤±è´¥: {e}")
            return []
    
    def _extract_tables_from_page(self, page, file_id: str, page_num: int) -> List[Dict[str, Any]]:
        """ä»é¡µé¢æå–è¡¨æ ¼"""
        chunks = []
        
        try:
            # ä½¿ç”¨PyMuPDFçš„è¡¨æ ¼æ£€æµ‹
            table_finder = page.find_tables()
            
            # å°†TableFinderè½¬æ¢ä¸ºåˆ—è¡¨
            tables = list(table_finder)
            
            for table_index, table in enumerate(tables):
                try:
                    # æå–è¡¨æ ¼æ•°æ®
                    table_data = table.extract()
                    if table_data and len(table_data) > 1:  # è‡³å°‘æœ‰æ ‡é¢˜è¡Œå’Œæ•°æ®è¡Œ
                        
                        # æ ¼å¼åŒ–è¡¨æ ¼å†…å®¹
                        table_content = self._format_table_simple(table_data, page_num, table_index)
                        
                        chunk_id = f"{file_id}_page_{page_num}_table_{table_index}"
                        chunks.append({
                            "chunk_id": chunk_id,
                            "file_id": file_id,
                            "content": table_content,
                            "content_type": "table",
                            "page_number": page_num,
                            "chunk_index": table_index,
                            "metadata": {
                                "page": page_num,
                                "type": "table",
                                "table_index": table_index,
                                "rows": len(table_data),
                                "columns": len(table_data[0]) if table_data else 0
                            }
                        })
                
                except Exception as e:
                    logger.warning(f"âš ï¸ å¤„ç†è¡¨æ ¼å¤±è´¥: {e}")
                    continue
            
            logger.debug(f"ğŸ“Š ç¬¬{page_num + 1}é¡µæå–{len(chunks)}ä¸ªè¡¨æ ¼")
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ è¡¨æ ¼æå–å¤±è´¥: {e}")
            return []
    
    def _analyze_image_simple(self, pix, file_id: str, page_num: int, img_index: int) -> str:
        """ç®€å•çš„å›¾åƒåˆ†æï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰"""
        try:
            # åŸºæœ¬ä¿¡æ¯
            width, height = pix.width, pix.height
            
            # è·³è¿‡å¤ªå°çš„å›¾åƒ
            if width < 50 or height < 50:
                return f"å›¾åƒ(ç¬¬{page_num + 1}é¡µ)ï¼šå°ºå¯¸{width}x{height}ï¼Œå›¾åƒè¿‡å°è·³è¿‡OCR"
            
            # è·³è¿‡å¤ªå¤§çš„å›¾åƒä»¥é¿å…OCRå¡ä½ - é’ˆå¯¹è£…ä¿®åˆåŒç­‰æ–‡æ¡£ä¼˜åŒ–
            dev_config = config_loader.get_app_config().get("development", {})
            max_image_size = dev_config.get("max_image_size", 2000000)
            if width * height > max_image_size:
                return f"å›¾åƒ(ç¬¬{page_num + 1}é¡µ)ï¼šå°ºå¯¸{width}x{height}ï¼Œå›¾åƒè¿‡å¤§è·³è¿‡OCRï¼ˆé™åˆ¶ï¼š{max_image_size}åƒç´ ï¼‰"
            
            # æ„å»ºåŸºç¡€æè¿°
            description = f"å›¾åƒ(ç¬¬{page_num + 1}é¡µ)ï¼šå°ºå¯¸{width}x{height}"
            
            # å¼€å‘æ¨¡å¼ä¸‹æš‚æ—¶è·³è¿‡OCRä»¥é¿å…å¡ä½
            dev_config = config_loader.get_app_config().get("development", {})
            skip_ocr = dev_config.get("skip_image_ocr", False)
            
            if skip_ocr:
                logger.debug(f"ğŸ”§ å¼€å‘æ¨¡å¼ï¼šè·³è¿‡å›¾åƒOCRå¤„ç†")
                return description + "ï¼Œå¼€å‘æ¨¡å¼è·³è¿‡OCR"
            
            # ä¿å­˜ä¸´æ—¶å›¾åƒè¿›è¡ŒOCRï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
            temp_path = f"temp_img_{file_id}_{page_num}_{img_index}.png"
            
            try:
                logger.debug(f"ğŸ’¾ ä¿å­˜ä¸´æ—¶å›¾åƒ: {temp_path}")
                pix.save(temp_path)
                
                # ç›´æ¥è°ƒç”¨OCRï¼Œé¿å…å¤šçº¿ç¨‹é—®é¢˜
                ocr_text = ""
                try:
                    logger.debug(f"ğŸ” å¼€å§‹OCRå¤„ç†: {temp_path}")
                    ocr_results = model_manager.extract_text_from_image(temp_path)
                    
                    if ocr_results:
                        ocr_text = " ".join([result.get("text", "") for result in ocr_results if result.get("text")])
                        logger.debug(f"âœ… OCRå®Œæˆï¼Œæå–æ–‡å­—: {len(ocr_text)}å­—ç¬¦")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ OCRå¤„ç†å¤±è´¥: {e}")
                    ocr_text = ""
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_path):
                    os.remove(temp_path)
                    logger.debug(f"ğŸ—‘ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {temp_path}")
                
                # æ„å»ºæœ€ç»ˆæè¿°
                if ocr_text:
                    description += f"ï¼ŒåŒ…å«æ–‡å­—ï¼š{ocr_text[:200]}"
                else:
                    description += "ï¼Œæ— æ–‡å­—å†…å®¹"
                
                return description
                
            except Exception as e:
                # ç¡®ä¿åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_path):
                    os.remove(temp_path)
                logger.warning(f"âš ï¸ å›¾åƒå¤„ç†å¤±è´¥: {e}")
                return description + "ï¼Œå¤„ç†å¤±è´¥"
            
        except Exception as e:
            logger.error(f"âŒ å›¾åƒåˆ†æå¤±è´¥: {e}")
            return f"å›¾åƒ(ç¬¬{page_num + 1}é¡µ)ï¼šåˆ†æå¤±è´¥"
    
    def _format_table_simple(self, table_data: List[List[str]], page_num: int, table_index: int) -> str:
        """ç®€å•çš„è¡¨æ ¼æ ¼å¼åŒ–"""
        try:
            content_lines = [f"è¡¨æ ¼(ç¬¬{page_num + 1}é¡µï¼Œè¡¨{table_index + 1})ï¼š"]
            
            # æ·»åŠ è¡¨å¤´
            if table_data:
                headers = table_data[0]
                content_lines.append("è¡¨å¤´ï¼š" + " | ".join(str(cell) for cell in headers))
                
                # æ·»åŠ æ•°æ®è¡Œï¼ˆæœ€å¤š10è¡Œï¼‰
                data_rows = table_data[1:min(11, len(table_data))]
                for i, row in enumerate(data_rows, 1):
                    row_text = " | ".join(str(cell) for cell in row)
                    content_lines.append(f"ç¬¬{i}è¡Œï¼š{row_text}")
                
                if len(table_data) > 11:
                    content_lines.append(f"... (å…±{len(table_data) - 1}è¡Œæ•°æ®)")
            
            return "\n".join(content_lines)
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼æ ¼å¼åŒ–å¤±è´¥: {e}")
            return f"è¡¨æ ¼(ç¬¬{page_num + 1}é¡µ)ï¼šæ ¼å¼åŒ–å¤±è´¥"
    
    def _generate_embeddings_for_chunks(self, chunks: List[Dict[str, Any]]) -> None:
        """ä¸ºå†…å®¹å—ç”ŸæˆåµŒå…¥å‘é‡"""
        try:
            # æå–æ–‡æœ¬å†…å®¹
            texts = [chunk["content"] for chunk in chunks]
            logger.info(f"ğŸ”¤ å¼€å§‹ç”Ÿæˆ{len(texts)}ä¸ªå†…å®¹å—çš„åµŒå…¥å‘é‡...")
            
            # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
            embeddings = model_manager.get_embedding(texts)
            
            # åˆ†é…ç»™å„ä¸ªå—
            for chunk, embedding in zip(chunks, embeddings):
                chunk["embedding"] = embedding
            
            logger.info(f"âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œå…±{len(embeddings)}ä¸ª768ç»´å‘é‡")
            
        except Exception as e:
            logger.error(f"âŒ åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def _save_chunks_to_vector_db(self, chunks: List[Dict[str, Any]]) -> None:
        """ä¿å­˜å†…å®¹å—åˆ°å‘é‡æ•°æ®åº“"""
        try:
            # ç¡®ä¿Milvusè¿æ¥
            if not milvus_manager.collection:
                logger.info("åˆå§‹åŒ–Milvusè¿æ¥...")
                milvus_manager.connect()
            
            # å‡†å¤‡å‘é‡æ•°æ®
            vector_data = []
            for chunk in chunks:
                vector_data.append({
                    "file_id": chunk["file_id"],
                    "chunk_id": chunk["chunk_id"],
                    "content": chunk["content"],
                    "embedding": chunk["embedding"],
                    "metadata": json.dumps(chunk["metadata"])
                })
            
            # æ’å…¥æ•°æ®
            milvus_manager.insert_vectors(vector_data)
            logger.info(f"âœ… æˆåŠŸä¿å­˜{len(vector_data)}ä¸ªå‘é‡åˆ°Milvus")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å‘é‡æ•°æ®å¤±è´¥: {e}")
            raise
    
    def _extract_knowledge_graph(self, chunks: List[Dict[str, Any]]) -> tuple:
        """æå–çŸ¥è¯†å›¾è°±ï¼šå®ä½“å’Œå…³ç³»"""
        entities = []
        relations = []
        
        try:
            # åªå¤„ç†æ–‡æœ¬å—
            text_chunks = [chunk for chunk in chunks if chunk["content_type"] == "text"]
            
            # åˆå¹¶æ–‡æœ¬å†…å®¹è¿›è¡Œæ‰¹é‡å¤„ç†
            batch_size = 3  # æ¯æ¬¡å¤„ç†3ä¸ªæ–‡æœ¬å—
            
            for i in range(0, len(text_chunks), batch_size):
                batch = text_chunks[i:i + batch_size]
                combined_text = "\n\n".join([chunk["content"] for chunk in batch])
                
                # æå–å®ä½“
                batch_entities = self._extract_entities_simple(combined_text)
                
                # æå–å…³ç³»
                batch_relations = self._extract_relations_simple(combined_text, batch_entities)
                
                # ä¸ºå®ä½“æ·»åŠ æ¥æºä¿¡æ¯
                for entity in batch_entities:
                    entity["source_chunks"] = [chunk["chunk_id"] for chunk in batch]
                    entity["file_id"] = batch[0]["file_id"] if batch else None
                
                # ä¸ºå…³ç³»æ·»åŠ æ¥æºä¿¡æ¯
                for relation in batch_relations:
                    relation["source_chunks"] = [chunk["chunk_id"] for chunk in batch]
                    relation["file_id"] = batch[0]["file_id"] if batch else None
                
                entities.extend(batch_entities)
                relations.extend(batch_relations)
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±æå–å®Œæˆï¼š{len(entities)}ä¸ªå®ä½“ï¼Œ{len(relations)}ä¸ªå…³ç³»")
            return entities, relations
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±æå–å¤±è´¥: {e}")
            return [], []
    
    def _extract_entities_simple(self, text: str) -> List[Dict[str, Any]]:
        """ç®€åŒ–çš„å®ä½“æå–"""
        response = ""  # åˆå§‹åŒ–responseå˜é‡é¿å…UnboundLocalError
        
        try:
            if "entity_extraction" not in self.prompt_config.get("document_parsing", {}):
                logger.warning("å®ä½“æå–æç¤ºè¯æœªé…ç½®ï¼Œè·³è¿‡å®ä½“æå–")
                return []
            
            prompt_template = self.prompt_config["document_parsing"]["entity_extraction"]
            prompt = prompt_template.format(text=text[:2000])  # é™åˆ¶æ–‡æœ¬é•¿åº¦
            
            logger.debug(f"ğŸ“ å®ä½“æå–æç¤ºè¯: {prompt[:200]}...")
            response = self._call_llm_simple(prompt)
            logger.debug(f"ğŸ¤– LLMåŸå§‹å“åº”: {repr(response[:300])}")
            
            # è§£æJSONå“åº”ï¼ˆå¢å¼ºå®¹é”™æ€§ï¼‰
            try:
                # æ¸…ç†å“åº”æ–‡æœ¬ - æ›´å¼ºçš„æ¸…ç†é€»è¾‘
                cleaned_response = response.strip()
                
                # ç§»é™¤markdownä»£ç å—æ ‡è®°
                if cleaned_response.startswith('```json'):
                    cleaned_response = cleaned_response[7:]
                elif cleaned_response.startswith('```'):
                    cleaned_response = cleaned_response[3:]
                    
                if cleaned_response.endswith('```'):
                    cleaned_response = cleaned_response[:-3]
                
                # ç§»é™¤é¢å¤–çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼ï¼Œä½†ä¿æŒJSONç»“æ„
                cleaned_response = cleaned_response.strip()
                
                # å°è¯•æ‰¾åˆ°JSONå¯¹è±¡çš„å¼€å§‹å’Œç»“æŸ
                start_idx = cleaned_response.find('{')
                end_idx = cleaned_response.rfind('}')
                
                if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                    json_str = cleaned_response[start_idx:end_idx+1]
                    logger.debug(f"ğŸ” æå–çš„JSON: {json_str[:100]}...")
                    result = json.loads(json_str)
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONå¯¹è±¡ï¼Œå°è¯•ç›´æ¥è§£æ
                    logger.warning(f"æœªæ‰¾åˆ°å®Œæ•´JSONå¯¹è±¡ï¼Œå°è¯•ç›´æ¥è§£æ: {cleaned_response[:200]}")
                    result = json.loads(cleaned_response)
                
                # ç¡®ä¿resultæ˜¯å­—å…¸ç±»å‹
                if not isinstance(result, dict):
                    logger.warning(f"LLMè¿”å›çš„ä¸æ˜¯å­—å…¸æ ¼å¼ï¼Œè€Œæ˜¯: {type(result).__name__}")
                    return []
                
                # å¢å¼ºçš„é”®åæ£€æµ‹å’Œæ¸…ç†
                entities = []
                for key in result.keys():
                    # æ¸…ç†é”®åä¸­çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼
                    clean_key = key.strip().replace('\n', '').replace('\r', '')
                    if clean_key == "entities" or "entities" in clean_key:
                        entities = result[key]
                        break
                
                if not entities:
                    logger.warning(f"æœªæ‰¾åˆ°entitieså­—æ®µï¼Œå¯ç”¨å­—æ®µ: {list(result.keys())}")
                    return []
                
                # æ ‡å‡†åŒ–å®ä½“æ ¼å¼
                standardized_entities = []
                for entity in entities:
                    if isinstance(entity, dict) and entity.get("name"):
                        standardized_entities.append({
                            "entity_id": str(uuid.uuid4()),
                            "name": entity.get("name", "").strip(),
                            "type": entity.get("type", "UNKNOWN").strip(),
                            "confidence": 0.8
                        })
                
                logger.info(f"âœ… å®ä½“æå–æˆåŠŸ: {len(standardized_entities)}ä¸ªå®ä½“")
                return standardized_entities
                
            except json.JSONDecodeError as e:
                logger.warning(f"LLMè¿”å›çš„JSONè§£æå¤±è´¥: {e}")
                logger.debug(f"åŸå§‹å“åº”: {repr(response[:500])}")
                logger.info("å°è¯•ä½¿ç”¨æ–‡æœ¬è§£æä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
                return self._parse_entities_from_text(response)
                
        except Exception as e:
            logger.error(f"å®ä½“æå–å¤±è´¥: {e}")
            logger.debug(f"å®ä½“æå–å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            logger.debug(f"åŸå§‹LLMå“åº”: {repr(response[:500])}")
            return []
    
    def _extract_relations_simple(self, text: str, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç®€åŒ–çš„å…³ç³»æå–"""
        response = ""  # åˆå§‹åŒ–responseå˜é‡é¿å…UnboundLocalError
        
        try:
            if not entities or "relation_extraction" not in self.prompt_config.get("document_parsing", {}):
                logger.info("è·³è¿‡å…³ç³»æå–ï¼šæ²¡æœ‰å®ä½“æˆ–æç¤ºè¯æœªé…ç½®")
                return []
            
            entities_str = "\n".join([f"- {entity['name']} ({entity['type']})" for entity in entities[:10]])  # é™åˆ¶å®ä½“æ•°é‡
            prompt_template = self.prompt_config["document_parsing"]["relation_extraction"]
            prompt = prompt_template.format(text=text[:2000], entities=entities_str)
            
            logger.debug(f"ğŸ“ å…³ç³»æå–æç¤ºè¯: {prompt[:200]}...")
            response = self._call_llm_simple(prompt)
            logger.debug(f"ğŸ¤– å…³ç³»æå–LLMåŸå§‹å“åº”: {repr(response[:300])}")
            
            # è§£æJSONå“åº”ï¼ˆå¢å¼ºå®¹é”™æ€§ï¼‰
            try:
                # æ¸…ç†å“åº”æ–‡æœ¬
                cleaned_response = response.strip()
                
                # ç§»é™¤markdownä»£ç å—æ ‡è®°
                if cleaned_response.startswith('```json'):
                    cleaned_response = cleaned_response[7:]
                elif cleaned_response.startswith('```'):
                    cleaned_response = cleaned_response[3:]
                    
                if cleaned_response.endswith('```'):
                    cleaned_response = cleaned_response[:-3]
                
                # ç§»é™¤é¢å¤–çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼ï¼Œä½†ä¿æŒJSONç»“æ„
                cleaned_response = cleaned_response.strip()
                
                # å°è¯•æ‰¾åˆ°JSONå¯¹è±¡çš„å¼€å§‹å’Œç»“æŸ
                start_idx = cleaned_response.find('{')
                end_idx = cleaned_response.rfind('}')
                
                if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                    json_str = cleaned_response[start_idx:end_idx+1]
                    logger.debug(f"ğŸ” å…³ç³»æå–JSON: {json_str[:100]}...")
                    result = json.loads(json_str)
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONå¯¹è±¡ï¼Œå°è¯•ç›´æ¥è§£æ
                    logger.warning(f"å…³ç³»æå–æœªæ‰¾åˆ°å®Œæ•´JSONå¯¹è±¡ï¼Œå°è¯•ç›´æ¥è§£æ: {cleaned_response[:200]}")
                    result = json.loads(cleaned_response)
                
                # ç¡®ä¿resultæ˜¯å­—å…¸ç±»å‹
                if not isinstance(result, dict):
                    logger.warning(f"å…³ç³»æå–LLMè¿”å›çš„ä¸æ˜¯å­—å…¸æ ¼å¼ï¼Œè€Œæ˜¯: {type(result).__name__}")
                    return []
                
                # å¢å¼ºçš„é”®åæ£€æµ‹å’Œæ¸…ç†
                relations = []
                for key in result.keys():
                    # æ¸…ç†é”®åä¸­çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼
                    clean_key = key.strip().replace('\n', '').replace('\r', '')
                    if clean_key == "relations" or "relations" in clean_key:
                        relations = result[key]
                        break
                
                if not relations:
                    logger.warning(f"æœªæ‰¾åˆ°relationså­—æ®µï¼Œå¯ç”¨å­—æ®µ: {list(result.keys())}")
                    return []
                
                # æ ‡å‡†åŒ–å…³ç³»æ ¼å¼
                standardized_relations = []
                for relation in relations:
                    if isinstance(relation, dict) and relation.get("subject") and relation.get("object"):
                        standardized_relations.append({
                            "relationship_id": str(uuid.uuid4()),
                            "subject": relation.get("subject", "").strip(),
                            "predicate": relation.get("predicate", "RELATED_TO").strip(),
                            "object": relation.get("object", "").strip(),
                            "confidence": relation.get("confidence", 0.8)
                        })
                
                logger.info(f"âœ… å…³ç³»æå–æˆåŠŸ: {len(standardized_relations)}ä¸ªå…³ç³»")
                return standardized_relations
                
            except json.JSONDecodeError as e:
                logger.warning(f"å…³ç³»æå–JSONè§£æå¤±è´¥: {e}")
                logger.debug(f"åŸå§‹å“åº”: {repr(response[:500])}")
                return []
                
        except Exception as e:
            logger.error(f"å…³ç³»æå–å¤±è´¥: {e}")
            logger.debug(f"å…³ç³»æå–å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            logger.debug(f"åŸå§‹LLMå“åº”: {repr(response[:500])}")
            return []
    
    def _save_knowledge_graph(self, entities: List[Dict[str, Any]], relations: List[Dict[str, Any]], file_id: str) -> None:
        """ä¿å­˜çŸ¥è¯†å›¾è°±åˆ°Neo4j"""
        try:
            # åˆ›å»ºå®ä½“èŠ‚ç‚¹
            for entity in entities:
                entity_data = {
                    "entity_id": entity["entity_id"],
                    "name": entity["name"],
                    "type": entity["type"],
                    "file_id": file_id,
                    "confidence": entity["confidence"]
                }
                neo4j_manager.create_entity(entity["type"], entity_data)
            
            # åˆ›å»ºå…³ç³»
            for relation in relations:
                # ç®€åŒ–çš„å…³ç³»åˆ›å»ºï¼Œç›´æ¥æ ¹æ®åç§°åŒ¹é…
                subject_entity = {"name": relation["subject"]}
                object_entity = {"name": relation["object"]}
                relation_props = {
                    "confidence": relation["confidence"],
                    "file_id": file_id
                }
                
                neo4j_manager.create_relationship(
                    subject_entity,
                    object_entity,
                    relation["predicate"],
                    relation_props
                )
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±ä¿å­˜å®Œæˆï¼š{len(entities)}ä¸ªå®ä½“ï¼Œ{len(relations)}ä¸ªå…³ç³»")
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±ä¿å­˜å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸å…¶ä»–å¤„ç†ç»§ç»­
    
    def _call_llm_simple(self, prompt: str) -> str:
        """ç®€åŒ–çš„LLMè°ƒç”¨"""
        try:
            if not hasattr(self, 'model_config') or not self.model_config:
                logger.error("æ¨¡å‹é…ç½®æœªåŠ è½½")
                return '{"entities": [], "relations": []}'
            
            llm_config = self.model_config.get("llm", {})
            if not llm_config:
                logger.error("LLMé…ç½®æœªæ‰¾åˆ°")
                return '{"entities": [], "relations": []}'
            
            # æ£€æŸ¥å¿…è¦çš„é…ç½®é¡¹
            required_keys = ["api_key", "api_url", "model_name"]
            for key in required_keys:
                if not llm_config.get(key):
                    logger.error(f"LLMé…ç½®ç¼ºå°‘å¿…è¦é¡¹: {key}")
                    return '{"entities": [], "relations": []}'
            
            headers = {
                "Authorization": f"Bearer {llm_config['api_key']}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": llm_config["model_name"],
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": min(llm_config.get("max_tokens", 2048), 2048),
                "temperature": llm_config.get("temperature", 0.7)
            }
            
            logger.debug(f"ğŸŒ è°ƒç”¨LLM API: {llm_config['api_url']}")
            response = requests.post(
                f"{llm_config['api_url']}/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                if not content:
                    logger.warning("LLMè¿”å›ç©ºå†…å®¹")
                    return '{"entities": [], "relations": []}'
                return content
            else:
                logger.error(f"LLM APIè°ƒç”¨å¤±è´¥: HTTP {response.status_code}, å“åº”: {response.text[:200]}")
                return '{"entities": [], "relations": []}'
                
        except requests.exceptions.Timeout:
            logger.error("LLM APIè°ƒç”¨è¶…æ—¶")
            return '{"entities": [], "relations": []}'
        except requests.exceptions.RequestException as e:
            logger.error(f"LLM APIè¯·æ±‚å¼‚å¸¸: {e}")
            return '{"entities": [], "relations": []}'
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¼‚å¸¸: {e}")
            return '{"entities": [], "relations": []}'
    
    def _parse_entities_from_text(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­è§£æå®ä½“ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        entities = []
        try:
            lines = text.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # å°è¯•å¤šç§è§£ææ¨¡å¼
                patterns = [
                    r'.*?([^(]+)\s*\(([^)]+)\)',  # å®ä½“å(ç±»å‹)
                    r'.*?name["\'\s]*:["\'\s]*([^,"\'}]+).*?type["\'\s]*:["\'\s]*([^,"\'}]+)',  # JSONæ ¼å¼çš„nameå’Œtype
                    r'.*?([A-Za-z\u4e00-\u9fff]+).*?([A-Z_]{2,})',  # ä¸­è‹±æ–‡å®ä½“åå’Œå¤§å†™ç±»å‹
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, line, re.IGNORECASE)
                    if match:
                        name = match.group(1).strip().strip('"\'')
                        entity_type = match.group(2).strip().strip('"\'')
                        
                        if name and entity_type and len(name) > 1:
                            entities.append({
                                "entity_id": str(uuid.uuid4()),
                                "name": name,
                                "type": entity_type,
                                "confidence": 0.6  # æ–‡æœ¬è§£æçš„ç½®ä¿¡åº¦è¾ƒä½
                            })
                            break
            
            # å»é‡
            seen_names = set()
            unique_entities = []
            for entity in entities:
                if entity["name"] not in seen_names:
                    seen_names.add(entity["name"])
                    unique_entities.append(entity)
            
            logger.info(f"ğŸ“ æ–‡æœ¬è§£æå®ä½“å¤‡ç”¨æ–¹æ¡ˆæå–åˆ° {len(unique_entities)} ä¸ªå®ä½“")
            return unique_entities[:20]  # é™åˆ¶æ•°é‡
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬è§£æå®ä½“å¤±è´¥: {e}")
            return []
    
    def _update_file_status(self, file_id: str, status: str, progress: int, message: str) -> None:
        """æ›´æ–°æ–‡ä»¶å¤„ç†çŠ¶æ€"""
        try:
            # æ›´æ–°å†…å­˜çŠ¶æ€
            self.processing_status[file_id] = {
                "status": status,
                "progress": progress,
                "message": message
            }
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            mysql_manager.execute_update(
                "UPDATE files SET status = %s, processing_progress = %s WHERE file_id = %s",
                (status, progress, file_id)
            )
            
            logger.info(f"ğŸ“Š {file_id}: {status} - {progress}% - {message}")
            
        except Exception as e:
            logger.warning(f"æ›´æ–°æ–‡ä»¶çŠ¶æ€å¤±è´¥: {e}")
    
    def delete_file(self, file_id: str) -> Dict[str, Any]:
        """
        åˆ é™¤æ–‡ä»¶
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            åˆ é™¤ç»“æœ
        """
        try:
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_info = self._get_file_info(file_id)
            if not file_info:
                return {
                    "success": False,
                    "message": "æ–‡ä»¶ä¸å­˜åœ¨"
                }
            
            # åˆ é™¤ç‰©ç†æ–‡ä»¶
            if os.path.exists(file_info["file_path"]):
                os.remove(file_info["file_path"])
            
            # ä»æ•°æ®åº“åˆ é™¤æ–‡ä»¶ä¿¡æ¯
            self._delete_file_info(file_id)
            
            # åˆ é™¤å‘é‡æ•°æ®
            self._delete_vector_data(file_id)
            
            # åˆ é™¤å›¾æ•°æ®
            self._delete_graph_data(file_id)
            
            return {
                "success": True,
                "message": "æ–‡ä»¶åˆ é™¤æˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶åˆ é™¤å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ–‡ä»¶åˆ é™¤å¤±è´¥: {str(e)}"
            }
    
    def rename_file(self, file_id: str, new_filename: str) -> Dict[str, Any]:
        """
        é‡å‘½åæ–‡ä»¶
        
        Args:
            file_id: æ–‡ä»¶ID
            new_filename: æ–°æ–‡ä»¶å
            
        Returns:
            é‡å‘½åç»“æœ
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            file_info = self._get_file_info(file_id)
            if not file_info:
                return {
                    "success": False,
                    "message": "æ–‡ä»¶ä¸å­˜åœ¨"
                }
            
            # æ›´æ–°æ–‡ä»¶å
            mysql_manager.execute_update(
                "UPDATE files SET original_filename = %s WHERE file_id = %s",
                (new_filename, file_id)
            )
            
            return {
                "success": True,
                "message": "æ–‡ä»¶é‡å‘½åæˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶é‡å‘½åå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ–‡ä»¶é‡å‘½åå¤±è´¥: {str(e)}"
            }
    
    def get_file_list(self) -> List[Dict[str, Any]]:
        """
        è·å–æ–‡ä»¶åˆ—è¡¨
        
        Returns:
            æ–‡ä»¶åˆ—è¡¨
        """
        try:
            files = mysql_manager.execute_query(
                "SELECT file_id, original_filename, file_size, upload_time, status, processing_progress FROM files ORDER BY upload_time DESC"
            )
            
            # å¤„ç†æ–‡ä»¶çŠ¶æ€å’Œè¿›åº¦
            for file in files:
                file_id = file["file_id"]
                if file_id in self.processing_status:
                    file["processing_progress"] = self.processing_status[file_id]["progress"]
                    file["status"] = self.processing_status[file_id]["status"]
            
            return files
            
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_processing_status(self, file_id: str) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶å¤„ç†çŠ¶æ€
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            å¤„ç†çŠ¶æ€ä¿¡æ¯
        """
        if file_id in self.processing_status:
            return self.processing_status[file_id]
        
        # ä»æ•°æ®åº“è·å–çŠ¶æ€
        file_info = self._get_file_info(file_id)
        if file_info:
            return {
                "status": file_info["status"],
                "progress": file_info["processing_progress"],
                "message": "å¤„ç†ä¸­..." if file_info["status"] == "processing" else "å¤„ç†å®Œæˆ"
            }
        
        return {
            "status": "not_found",
            "progress": 0,
            "message": "æ–‡ä»¶ä¸å­˜åœ¨"
        }
    
    def get_file_detailed_info(self, file_id: str) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
        """
        try:
            # è·å–åŸºæœ¬æ–‡ä»¶ä¿¡æ¯
            file_info = self._get_file_info(file_id)
            if not file_info:
                return {
                    "success": False,
                    "message": "æ–‡ä»¶ä¸å­˜åœ¨"
                }
            
            # è·å–å¤„ç†çŠ¶æ€
            status_info = self.get_processing_status(file_id)
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self._get_file_statistics(file_id)
            
            return {
                "success": True,
                "file_info": {
                    "file_id": file_info["file_id"],
                    "original_filename": file_info["original_filename"],
                    "file_size": file_info["file_size"],
                    "upload_time": file_info["upload_time"],
                    "status": file_info["status"],
                    "processing_progress": file_info["processing_progress"],
                    "file_path": file_info["file_path"]
                },
                "processing_info": status_info,
                "statistics": stats
            }
            
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"è·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}"
            }
    
    def _get_file_statistics(self, file_id: str) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            file_id: æ–‡ä»¶ID
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # è·å–å‘é‡æ•°æ®ç»Ÿè®¡
            vector_count = 0
            try:
                if milvus_manager.collection and milvus_manager.has_data():
                    # æŸ¥è¯¢å‘é‡æ•°æ®æ•°é‡
                    vector_result = milvus_manager.collection.query(
                        expr=f"file_id == '{file_id}'",
                        output_fields=["chunk_id"]
                    )
                    vector_count = len(vector_result) if vector_result else 0
            except Exception as e:
                logger.warning(f"è·å–å‘é‡æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
            
            # è·å–å›¾æ•°æ®ç»Ÿè®¡
            entity_count = 0
            relation_count = 0
            try:
                entity_result = neo4j_manager.execute_query(
                    "MATCH (n) WHERE n.file_id = $file_id RETURN count(n) as count",
                    {"file_id": file_id}
                )
                entity_count = entity_result[0]["count"] if entity_result else 0
                
                relation_result = neo4j_manager.execute_query(
                    "MATCH ()-[r]->() WHERE r.file_id = $file_id RETURN count(r) as count",
                    {"file_id": file_id}
                )
                relation_count = relation_result[0]["count"] if relation_result else 0
            except Exception as e:
                logger.warning(f"è·å–å›¾æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
            
            return {
                "chunks_count": vector_count,
                "entities_count": entity_count,
                "relations_count": relation_count
            }
            
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "chunks_count": 0,
                "entities_count": 0,
                "relations_count": 0
            }
    
    def _allowed_file(self, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦å…è®¸"""
        return '.' in filename and \
               '.' + filename.rsplit('.', 1)[1].lower() in self.allowed_extensions
    
    def _save_file_info(self, file_info: Dict[str, Any]) -> None:
        """ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            mysql_manager.execute_update(
                """
                INSERT INTO files (file_id, original_filename, filename, file_path, file_size, upload_time, status, processing_progress)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """,
                (
                    file_info["file_id"],
                    file_info["original_filename"],
                    file_info["filename"],
                    file_info["file_path"],
                    file_info["file_size"],
                    file_info["upload_time"],
                    file_info["status"],
                    file_info["processing_progress"]
                )
            )
            logger.info(f"âœ… æ–‡ä»¶ä¿¡æ¯ä¿å­˜æˆåŠŸ: {file_info['file_id']}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")
            raise
    
    def _get_file_info(self, file_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        results = mysql_manager.execute_query(
            "SELECT * FROM files WHERE file_id = %s",
            (file_id,)
        )
        return results[0] if results else None
    
    def _delete_file_info(self, file_id: str) -> None:
        """ä»æ•°æ®åº“åˆ é™¤æ–‡ä»¶ä¿¡æ¯"""
        mysql_manager.execute_update(
            "DELETE FROM files WHERE file_id = %s",
            (file_id,)
        )
    
    def _delete_vector_data(self, file_id: str) -> None:
        """åˆ é™¤å‘é‡æ•°æ®"""
        try:
            if milvus_manager.collection and milvus_manager.has_data():
                # ä½¿ç”¨Milvusçš„deleteæ¥å£åˆ é™¤æŒ‡å®šfile_idçš„å‘é‡
                expr = f"file_id == '{file_id}'"
                milvus_manager.collection.delete(expr)
                milvus_manager.collection.flush()
                logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶ {file_id} çš„å‘é‡æ•°æ®")
            else:
                logger.info(f"Milvusé›†åˆä¸ºç©ºæˆ–æœªåˆå§‹åŒ–ï¼Œè·³è¿‡åˆ é™¤æ–‡ä»¶ {file_id} çš„å‘é‡æ•°æ®")
        except Exception as e:
            logger.warning(f"åˆ é™¤æ–‡ä»¶ {file_id} çš„å‘é‡æ•°æ®å¤±è´¥: {e}")
    
    def _delete_graph_data(self, file_id: str) -> None:
        """åˆ é™¤å›¾æ•°æ®"""
        try:
            # åˆ é™¤å®ä½“èŠ‚ç‚¹å’Œå…³ç³»
            neo4j_manager.execute_query(
                "MATCH (n {file_id: $file_id}) DETACH DELETE n",
                {"file_id": file_id}
            )
            # åˆ é™¤åªæœ‰file_idå±æ€§çš„å…³ç³»
            neo4j_manager.execute_query(
                "MATCH ()-[r {file_id: $file_id}]-() DELETE r",
                {"file_id": file_id}
            )
            logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶ {file_id} çš„å›¾æ•°æ®")
        except Exception as e:
            logger.warning(f"åˆ é™¤æ–‡ä»¶ {file_id} çš„å›¾æ•°æ®å¤±è´¥: {e}")
    
    def _allowed_file(self, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦å…è®¸"""
        return '.' in filename and \
               '.' + filename.rsplit('.', 1)[1].lower() in self.allowed_extensions
    
    def add_ocr_support(self) -> None:
        """ä¸ºå›¾åƒåˆ†ææ·»åŠ OCRæ”¯æŒ"""
        # è¿™ä¸ªæ–¹æ³•å°†åœ¨åç»­ç‰ˆæœ¬ä¸­å®ç°å›¾åƒOCRåŠŸèƒ½
        pass

# å…¨å±€æ–‡ä»¶æœåŠ¡å®ä¾‹
file_service = FileService() 
